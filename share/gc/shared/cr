accessBarrierSupport.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
accessBarrierSupport.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
accessBarrierSupport.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
adaptiveSizePolicy.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
adaptiveSizePolicy.cpp://      _throughput_goal = 1 - ( 1 / (1 + gc_cost_ratio))
adaptiveSizePolicy.cpp:    _throughput_goal(1.0 - double(1.0 / (1.0 + (double) gc_cost_ratio))),
adaptiveSizePolicy.cpp:  _minor_pause_young_estimator->update(eden_size_in_mbytes,
adaptiveSizePolicy.cpp:    _avg_minor_pause->sample(minor_pause_in_seconds);
adaptiveSizePolicy.cpp:    // Cost of collection (unit-less)
adaptiveSizePolicy.cpp:      _avg_minor_gc_cost->sample(collection_cost);
adaptiveSizePolicy.cpp:      _avg_minor_interval->sample(interval_in_seconds);
adaptiveSizePolicy.cpp:      (_avg_minor_gc_cost->count() >= AdaptiveSizePolicyReadyThreshold);
adaptiveSizePolicy.cpp:                        collection_cost, _avg_minor_gc_cost->average());
adaptiveSizePolicy.cpp:    assert(collection_cost >= 0.0, "Expected to be non-negative");
adaptiveSizePolicy.cpp:    _minor_collection_estimator->update(eden_size_in_mbytes, collection_cost);
adaptiveSizePolicy.cpp://      average-interval-between-major-gc * AdaptiveSizeMajorGCDecayTimeScale /
adaptiveSizePolicy.cpp://        time-since-last-major-gc
adaptiveSizePolicy.cpp:// if the average-interval-between-major-gc * AdaptiveSizeMajorGCDecayTimeScale
adaptiveSizePolicy.cpp:// is less than time-since-last-major-gc.
adaptiveSizePolicy.cpp:      // Decay using the time-since-last-major-gc
adaptiveSizePolicy.cpp:      _max_eden_size - live_in_eden : 0;
adaptiveSizePolicy.cpp:    const size_t free_in_old_gen = (size_t)(_max_old_gen_size - _avg_old_live);
adaptiveSizePolicy.cpp:    size_t promo_limit = (size_t)(_max_old_gen_size - _avg_old_live);
adaptiveSizePolicy.cpp:                                                       avg_eden_live()->average(),
adaptiveSizePolicy.cpp:                                                       avg_old_live()->average());
adaptiveSizePolicy.cpp:                      100.0 * avg_minor_gc_cost()->average(), young_gen_action);
adaptiveSizePolicy.cpp:                      100.0 * avg_major_gc_cost()->average(), tenured_gen_action);
adaptiveSizePolicy.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
adaptiveSizePolicy.hpp:    decrease_old_gen_for_throughput_true = -7,
adaptiveSizePolicy.hpp:    decrease_young_gen_for_througput_true = -6,
adaptiveSizePolicy.hpp:    increase_old_gen_for_min_pauses_true = -5,
adaptiveSizePolicy.hpp:    decrease_old_gen_for_min_pauses_true = -4,
adaptiveSizePolicy.hpp:    decrease_young_gen_for_maj_pauses_true = -3,
adaptiveSizePolicy.hpp:    increase_young_gen_for_min_pauses_true = -2,
adaptiveSizePolicy.hpp:    increase_old_gen_for_maj_pauses_true = -1,
adaptiveSizePolicy.hpp:  // These variables represent linear least-squares fits of
adaptiveSizePolicy.hpp:    return MAX2(0.0F, _avg_major_gc_cost->average());
adaptiveSizePolicy.hpp:    return MAX2(0.0F, _avg_minor_gc_cost->average());
adaptiveSizePolicy.hpp:  // cost stays non-negative.
adaptiveSizePolicy.hpp:    assert(result >= 0.0, "Both minor and major costs are non-negative");
adaptiveSizePolicy.hpp:    return _avg_major_interval->average();
adaptiveSizePolicy.hpp:    double result = 1.0 - decaying_gc_cost();
adaptiveSizePolicy.hpp:    double result = 1.0 - gc_cost();
adaptiveSizePolicy.hpp:    return _minor_pause_young_estimator->slope();
adaptiveSizePolicy.hpp:  float minor_collection_slope() { return _minor_collection_estimator->slope();}
adaptiveSizePolicy.hpp:  float major_collection_slope() { return _major_collection_estimator->slope();}
adaptiveSizePolicy.hpp:    return _minor_pause_old_estimator->slope();
adaptiveSizePolicy.hpp:  // Check the conditions for an out-of-memory due to excessive GC time.
ageTable.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
ageTable.cpp:    sizes[i]+= subTable->sizes[i];
ageTable.cpp:        log_trace(gc, age)("- age %3u: " SIZE_FORMAT_W(10) " bytes, " SIZE_FORMAT_W(10) " total",
ageTable.cpp:        _perf_sizes[age]->set_value(wordSize * oopSize);
ageTable.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
ageTable.hpp:// Age table for adaptive feedback-mediated tenuring (scavenging)
ageTable.hpp:  // (as opposed to gc-thread-local)
ageTable.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
ageTable.inline.hpp:  add(p->age(), oop_size);
ageTableTracer.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
ageTableTracer.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
allocTracer.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
allocTracer.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
barrierSetAssembler.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
barrierSetConfig.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
barrierSetConfig.hpp:// To enable runtime-resolution of GC barriers on primitives, please
barrierSetConfig.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
barrierSet.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
barrierSet.cpp:  assert(Thread::current()->is_Java_thread(),
barrierSet.cpp:  assert(!JavaThread::current()->on_thread_list(),
barrierSet.cpp:  _barrier_set->on_thread_create(Thread::current());
barrierSet.cpp:  BarrierSetAssembler* bs_assembler = bs->barrier_set_assembler();
barrierSet.cpp:  bs_assembler->barrier_stubs_init();
barrierSet.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
barrierSet.hpp:  // - T must have a corresponding Name entry.
barrierSet.hpp:  // - GetName<T> must be specialized to return the corresponding Name
barrierSet.hpp:  // - If T is a base class, the constructor must have a FakeRtti
barrierSet.hpp:  // - If T is a concrete class, the constructor must create a
barrierSet.hpp:  // The allocation is safe to use iff it returns true. If not, the slow-path allocation
barrierSet.hpp:  // These perform BarrierSet-related initialization/cleanup before the thread
barrierSet.hpp:    // Off-heap oop accesses. These accessors get resolved when
barrierSet.hpp:  assert(bs->is_a(BarrierSet::GetName<T>::value), "wrong type of barrier set");
barrierSet.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
barrierSet.inline.hpp:  Klass* const dst_klass = objArrayOop(dst_obj)->element_klass();
barrierSetNMethod.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
barrierSetNMethod.cpp:  if (nm->method()->is_method_handle_intrinsic()) {
barrierSetNMethod.cpp:  if (!nm->is_native_method() && !nm->is_compiled_by_c2() && !nm->is_compiled_by_c1()) {
barrierSetNMethod.cpp:  nmethod* nm = cb->as_nmethod();
barrierSetNMethod.cpp:  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();
barrierSetNMethod.cpp:  if (!bs_nm->is_armed(nm)) {
barrierSetNMethod.cpp:  assert(!nm->is_osr_method(), "Should not reach here");
barrierSetNMethod.cpp:  bool may_enter = bs_nm->nmethod_entry_barrier(nm);
barrierSetNMethod.cpp:    bs_nm->deoptimize(nm, return_address_ptr);
barrierSetNMethod.cpp:  // a deoptimized method will always hit the barrier and come to the same conclusion - deoptimize
barrierSetNMethod.cpp:  assert(nm->is_osr_method(), "Should not reach here");
barrierSetNMethod.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
blockOffsetTable.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
blockOffsetTable.cpp:    delta = ReservedSpace::page_align_size_up(new_size - old_size);
blockOffsetTable.cpp:    delta = ReservedSpace::page_align_size_down(old_size - new_size);
blockOffsetTable.cpp:    assert(_vs.high() == high - delta, "invalid expansion");
blockOffsetTable.cpp:    _array->set_offset_array(0, 0);  // set first card to 0
blockOffsetTable.cpp:// a right-open interval: [start, end)
blockOffsetTable.cpp:  //     | +- 1st        |                         |
blockOffsetTable.cpp:  //    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+     +-+-+-+-+-+-+-+-+-+-+-
blockOffsetTable.cpp:  //    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+     +-+-+-+-+-+-+-+-+-+-+-
blockOffsetTable.cpp:  //      x - offset value of offset card
blockOffsetTable.cpp:  //    1st - start of first logarithmic region
blockOffsetTable.cpp:  //    2nd - start of second logarithmic region
blockOffsetTable.cpp:  //    3rd - start of third logarithmic region
blockOffsetTable.cpp:  //          back slip = 2**(3*(0x81 - N_words)) = 2**3) = 8
blockOffsetTable.cpp:  size_t start_card = _array->index_for(start);
blockOffsetTable.cpp:  size_t end_card = _array->index_for(end-1);
blockOffsetTable.cpp:  assert(start ==_array->address_for_index(start_card), "Precondition");
blockOffsetTable.cpp:  assert(end ==_array->address_for_index(end_card)+BOTConstants::N_words, "Precondition");
blockOffsetTable.cpp:  assert(start_card > _array->index_for(_bottom), "Cannot be first card");
blockOffsetTable.cpp:  assert(_array->offset_array(start_card-1) <= BOTConstants::N_words,
blockOffsetTable.cpp:    // -1 so that the the card with the actual offset is counted.  Another -1
blockOffsetTable.cpp:    size_t reach = start_card - 1 + (BOTConstants::power_to_cards_back(i+1) - 1);
blockOffsetTable.cpp:      _array->set_offset_array(start_card_for_region, end_card, offset, reducing);
blockOffsetTable.cpp:    _array->set_offset_array(start_card_for_region, reach, offset, reducing);
blockOffsetTable.cpp:// The card-interval [start_card, end_card] is a closed interval; this
blockOffsetTable.cpp:// is an expensive check -- use with care and only under protection of
blockOffsetTable.cpp:  guarantee(_array->offset_array(start_card) == BOTConstants::N_words, "Wrong value in second card");
blockOffsetTable.cpp:    u_char entry = _array->offset_array(c);
blockOffsetTable.cpp:    if (c - start_card > BOTConstants::power_to_cards_back(1)) {
blockOffsetTable.cpp:    size_t landing_card = c - backskip;
blockOffsetTable.cpp:    guarantee(landing_card >= (start_card - 1), "Inv");
blockOffsetTable.cpp:      guarantee(_array->offset_array(landing_card) <= entry, "Monotonicity");
blockOffsetTable.cpp:      guarantee(landing_card == (start_card - 1), "Tautology");
blockOffsetTable.cpp:      guarantee(_array->offset_array(landing_card) <= BOTConstants::N_words, "Offset value");
blockOffsetTable.cpp:// Action_mark - update the BOT for the block [blk_start, blk_end).
blockOffsetTable.cpp:// Action_single - udpate the BOT for an allocation.
blockOffsetTable.cpp:// Action_verify - BOT verification.
blockOffsetTable.cpp:  assert(_sp->is_in_reserved(blk_start),
blockOffsetTable.cpp:  assert(_sp->is_in_reserved(blk_end-1),
blockOffsetTable.cpp:  uintptr_t end_ui = (uintptr_t)(blk_end - 1);
blockOffsetTable.cpp:    size_t    start_index = _array->index_for(blk_start);
blockOffsetTable.cpp:    size_t    end_index   = _array->index_for(blk_end - 1);
blockOffsetTable.cpp:    HeapWord* boundary    = _array->address_for_index(start_index);
blockOffsetTable.cpp:          _array->set_offset_array(start_index, boundary, blk_start, reducing);
blockOffsetTable.cpp:        _array->set_offset_array(start_index, boundary, blk_start, reducing);
blockOffsetTable.cpp:          HeapWord* rem_st = _array->address_for_index(start_index) + BOTConstants::N_words;
blockOffsetTable.cpp:          HeapWord* rem_end = _array->address_for_index(end_index) + BOTConstants::N_words;
blockOffsetTable.cpp:        _array->check_offset_array(start_index, boundary, blk_start);
blockOffsetTable.cpp:// information; Right-open interval: [blk_start, blk_end)
blockOffsetTable.cpp:    HeapWord* p = _array->address_for_index(next_index) + 1;
blockOffsetTable.cpp:           _sp->is_free_block(start) ||
blockOffsetTable.cpp:  assert(_array->offset_array(0) == 0, "objects can't cross covered areas");
blockOffsetTable.cpp:  size_t index = _array->index_for(addr);
blockOffsetTable.cpp:  index = MIN2(index, _next_offset_index-1);
blockOffsetTable.cpp:  HeapWord* q = _array->address_for_index(index);
blockOffsetTable.cpp:  uint offset = _array->offset_array(index);    // Extend u_char to uint.
blockOffsetTable.cpp:    q -= (BOTConstants::N_words * n_cards_back);
blockOffsetTable.cpp:    assert(q >= _sp->bottom(), "Went below bottom!");
blockOffsetTable.cpp:    index -= n_cards_back;
blockOffsetTable.cpp:    offset = _array->offset_array(index);
blockOffsetTable.cpp:    assert(q >= _sp->bottom(), "Went below bottom!");
blockOffsetTable.cpp:    q -= BOTConstants::N_words;
blockOffsetTable.cpp:    index--;
blockOffsetTable.cpp:    offset = _array->offset_array(index);
blockOffsetTable.cpp:  q -= offset;
blockOffsetTable.cpp:    n += _sp->block_size(n);
blockOffsetTable.cpp://      +-------+-------+-------+-------+-------+
blockOffsetTable.cpp://      | i-1   |   i   | i+1   | i+2   | i+3   |
blockOffsetTable.cpp://      +-------+-------+-------+-------+-------+
blockOffsetTable.cpp://         block-start
blockOffsetTable.cpp:  assert(_sp->is_in_reserved(blk_start),
blockOffsetTable.cpp:  assert(_sp->is_in_reserved(blk_end-1),
blockOffsetTable.cpp:         _array->_reserved.start() + _next_offset_index*BOTConstants::N_words,
blockOffsetTable.cpp:  _array->set_offset_array(_next_offset_index,
blockOffsetTable.cpp:  size_t end_index   = _array->index_for(blk_end - 1);
blockOffsetTable.cpp:    HeapWord* rem_st  = _array->address_for_index(_next_offset_index + 1);
blockOffsetTable.cpp:    HeapWord* rem_end = _array->address_for_index(end_index) +  BOTConstants::N_words;
blockOffsetTable.cpp:  _next_offset_threshold = _array->address_for_index(end_index) + BOTConstants::N_words;
blockOffsetTable.cpp:  size_t start_index = _array->index_for(blk_start);
blockOffsetTable.cpp:  HeapWord* boundary    = _array->address_for_index(start_index);
blockOffsetTable.cpp:  assert((_array->offset_array(orig_next_offset_index) == 0 &&
blockOffsetTable.cpp:          (_array->offset_array(orig_next_offset_index) > 0 &&
blockOffsetTable.cpp:         _array->offset_array(orig_next_offset_index) <= BOTConstants::N_words),
blockOffsetTable.cpp:    assert(_array->offset_array(j) > 0 &&
blockOffsetTable.cpp:           _array->offset_array(j) <= (u_char) (BOTConstants::N_words+BOTConstants::N_powers-1),
blockOffsetTable.cpp:  _next_offset_index = _array->index_for(_bottom);
blockOffsetTable.cpp:    _array->address_for_index(_next_offset_index);
blockOffsetTable.cpp:  size_t bottom_index = _array->index_for(_bottom);
blockOffsetTable.cpp:  _array->set_offset_array(bottom_index, 0);
blockOffsetTable.cpp:  return _next_offset_index == 0 ? 0 : _next_offset_index - 1;
blockOffsetTable.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
blockOffsetTable.hpp:// systems using card-table-based write barriers, the efficiency of this
blockOffsetTable.hpp://   - BlockOffsetArray (abstract)
blockOffsetTable.hpp://     - BlockOffsetArrayContigSpace
blockOffsetTable.hpp:  static const uint LogN_words = LogN - LogHeapWordSize;
blockOffsetTable.hpp:  // entries "e" of at least N_words mean "go back by Base^(e-N_words)."
blockOffsetTable.hpp:    return power_to_cards_back(entry - N_words);
blockOffsetTable.hpp:    return power_to_words_back(entry - N_words);
blockOffsetTable.hpp:// divides the covered region into "N"-word subregions (where
blockOffsetTable.hpp:// for example, the garbage-first generation.)
blockOffsetTable.hpp:  // An assertion-checking helper method for the set_offset_array() methods below.
blockOffsetTable.hpp:    assert(index_for(right - 1) < _vs.committed_size(),
blockOffsetTable.hpp:    size_t num_cards = right - left + 1;
blockOffsetTable.hpp:  // An assertion-checking helper method for the set_remainder*() methods below.
blockOffsetTable.hpp:  void check_reducing_assertion(bool reducing) { _array->check_reducing_assertion(reducing); }
blockOffsetTable.hpp:  // is right-open. The last parameter, reducing, indicates whether the
blockOffsetTable.hpp:      assert(_array->is_card_boundary(_end),
blockOffsetTable.hpp:      assert(_array->is_card_boundary(new_end),
blockOffsetTable.hpp:      _array->set_offset_array(_end, new_end, BOTConstants::N_words);
blockOffsetTable.hpp:    assert(_array != NULL, "_array should be non-NULL");
blockOffsetTable.hpp:    _array->set_init_to_zero(val);
blockOffsetTable.hpp:// region can be more efficiently tracked (than for a non-contiguous space).
blockOffsetTable.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
blockOffsetTable.inline.hpp:           Thread::current()->is_VM_thread() ||
blockOffsetTable.inline.hpp:           Thread::current()->is_ConcurrentGC_thread() ||
blockOffsetTable.inline.hpp:           ((!Thread::current()->is_ConcurrentGC_thread()) &&
blockOffsetTable.inline.hpp:            ParGCRareEvent_lock->owned_by_self()), "Crack");
c1/barrierSetC1.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c1/barrierSetC1.hpp:      return _item->result();
c1/modRefBarrierSetC1.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c1/modRefBarrierSetC1.cpp:#define __ gen->lir(__FILE__, __LINE__)->
c1/modRefBarrierSetC1.cpp:#define __ gen->lir()->
c1/modRefBarrierSetC1.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c1/cardTableBarrierSetC1.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c1/barrierSetC1.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c1/barrierSetC1.cpp:#define __ gen->lir(__FILE__, __LINE__)->
c1/barrierSetC1.cpp:#define __ gen->lir()->
c1/barrierSetC1.cpp:    addr_opr = LIR_OprFact::address(gen->emit_array_address(base.result(), offset, access.type()));
c1/barrierSetC1.cpp:    // generate_address to try to be smart about emitting the -1.
c1/barrierSetC1.cpp:    addr_opr = LIR_OprFact::address(gen->generate_address(base.result(), offset, 0, 0, access.type()));
c1/barrierSetC1.cpp:    LIR_Opr resolved_addr = gen->new_pointer_register();
c1/barrierSetC1.cpp:    value = gen->mask_boolean(access.base().opr(), value, access.access_emit_info());
c1/barrierSetC1.cpp:    gen->volatile_field_store(value, access.resolved_addr()->as_address_ptr(), access.access_emit_info());
c1/barrierSetC1.cpp:    __ store(value, access.resolved_addr()->as_address_ptr(), access.access_emit_info(), patch_code);
c1/barrierSetC1.cpp:    __ move_wide(access.resolved_addr()->as_address_ptr(), result);
c1/barrierSetC1.cpp:    gen->volatile_field_load(access.resolved_addr()->as_address_ptr(), result, access.access_emit_info());
c1/barrierSetC1.cpp:    __ load(access.resolved_addr()->as_address_ptr(), result, access.access_emit_info(), patch_code);
c1/barrierSetC1.cpp:    __ branch(lir_cond_equal, equalZeroLabel->label());
c1/barrierSetC1.cpp:    __ branch_destination(equalZeroLabel->label());
c1/barrierSetC1.cpp:  return gen->atomic_cmpxchg(access.type(), access.resolved_addr(), cmp_value, new_value);
c1/barrierSetC1.cpp:  return gen->atomic_xchg(access.type(), access.resolved_addr(), value);
c1/barrierSetC1.cpp:  return gen->atomic_add(access.type(), access.resolved_addr(), value);
c1/barrierSetC1.cpp:  //     if (klass(src)->reference_type() != REF_NONE) {
c1/barrierSetC1.cpp:  if (offset->is_constant()) {
c1/barrierSetC1.cpp:    LIR_Const* constant = offset->as_constant_ptr();
c1/barrierSetC1.cpp:    jlong off_con = (constant->type() == T_INT ?
c1/barrierSetC1.cpp:                     (jlong)constant->as_jint() :
c1/barrierSetC1.cpp:                     constant->as_jlong());
c1/barrierSetC1.cpp:      // The constant offset is the same as referent_offset -
c1/barrierSetC1.cpp:  if (gen_pre_barrier && base.type()->is_array()) {
c1/barrierSetC1.cpp:      if (src_con->is_null_object()) {
c1/barrierSetC1.cpp:        // The constant src object is null - We can skip
c1/barrierSetC1.cpp:        // Non-null constant source object. We still have to generate
c1/barrierSetC1.cpp:        // the slow stub - but we don't need to generate the runtime
c1/barrierSetC1.cpp:    // a sub-class of Reference?
c1/barrierSetC1.cpp:    ciType* type = base.value()->declared_type();
c1/barrierSetC1.cpp:    if ((type != NULL) && type->is_loaded()) {
c1/barrierSetC1.cpp:      if (type->is_subtype_of(gen->compilation()->env()->Reference_klass())) {
c1/barrierSetC1.cpp:      } else if (type->is_klass() &&
c1/barrierSetC1.cpp:                 !gen->compilation()->env()->Object_klass()->is_subtype_of(type->as_klass())) {
c1/barrierSetC1.cpp:    LIR_Opr base_reg = gen->new_register(T_OBJECT);
c1/barrierSetC1.cpp:      // if (offset != referent_offset) -> continue
c1/barrierSetC1.cpp:      // a reg-reg compare.
c1/barrierSetC1.cpp:      if (offset->type() == T_INT) {
c1/barrierSetC1.cpp:        assert(offset->type() == T_LONG, "what else?");
c1/barrierSetC1.cpp:        referent_off = gen->new_register(T_LONG);
c1/barrierSetC1.cpp:      __ branch(lir_cond_notEqual, cont->label());
c1/barrierSetC1.cpp:      // if (source == null) -> continue
c1/barrierSetC1.cpp:      __ branch(lir_cond_equal, cont->label());
c1/barrierSetC1.cpp:    LIR_Opr src_klass = gen->new_register(T_METADATA);
c1/barrierSetC1.cpp:      // if (src->_klass->_reference_type == REF_NONE) -> continue
c1/barrierSetC1.cpp:      LIR_Opr reference_type = gen->new_register(T_INT);
c1/barrierSetC1.cpp:      __ branch(lir_cond_equal, cont->label());
c1/cardTableBarrierSetC1.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c1/cardTableBarrierSetC1.cpp:#define __ gen->lir(__FILE__, __LINE__)->
c1/cardTableBarrierSetC1.cpp:#define __ gen->lir()->
c1/cardTableBarrierSetC1.cpp:  CardTable* ct = ctbs->card_table();
c1/cardTableBarrierSetC1.cpp:  LIR_Const* card_table_base = new LIR_Const(ct->byte_map_base());
c1/cardTableBarrierSetC1.cpp:  if (addr->is_address()) {
c1/cardTableBarrierSetC1.cpp:    LIR_Address* address = addr->as_address_ptr();
c1/cardTableBarrierSetC1.cpp:    LIR_Opr ptr = gen->new_pointer_register();
c1/cardTableBarrierSetC1.cpp:    if (!address->index()->is_valid() && address->disp() == 0) {
c1/cardTableBarrierSetC1.cpp:      __ move(address->base(), ptr);
c1/cardTableBarrierSetC1.cpp:      assert(address->disp() != max_jint, "lea doesn't support patched addresses!");
c1/cardTableBarrierSetC1.cpp:  assert(addr->is_register(), "must be a register at this point");
c1/cardTableBarrierSetC1.cpp:  gen->CardTableBarrierSet_post_barrier_helper(addr, card_table_base);
c1/cardTableBarrierSetC1.cpp:  LIR_Opr tmp = gen->new_pointer_register();
c1/cardTableBarrierSetC1.cpp:  if (gen->can_inline_as_constant(card_table_base)) {
c1/cardTableBarrierSetC1.cpp:    card_addr = new LIR_Address(tmp, card_table_base->as_jint(), T_BYTE);
c1/cardTableBarrierSetC1.cpp:    card_addr = new LIR_Address(tmp, gen->load_constant(card_table_base), T_BYTE);
c1/cardTableBarrierSetC1.cpp:    LIR_Opr cur_value = gen->new_register(T_INT);
c1/cardTableBarrierSetC1.cpp:    if (ct->scanned_concurrently()) {
c1/cardTableBarrierSetC1.cpp:    __ branch(lir_cond_equal, L_already_dirty->label());
c1/cardTableBarrierSetC1.cpp:    __ branch_destination(L_already_dirty->label());
c1/cardTableBarrierSetC1.cpp:    if (ct->scanned_concurrently()) {
c2/cardTableBarrierSetC2.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c2/cardTableBarrierSetC2.cpp:     return kit->makecon(TypeRawPtr::make((address)card_table_base));
c2/cardTableBarrierSetC2.cpp:     return kit->null();
c2/cardTableBarrierSetC2.cpp:// Insert a write-barrier store.  This is to let generational GC work; we have
c2/cardTableBarrierSetC2.cpp:// to flag all oop-stores before the next GC point.
c2/cardTableBarrierSetC2.cpp:  CardTable* ct = ctbs->card_table();
c2/cardTableBarrierSetC2.cpp:  // oop updates flagged via card-marks.
c2/cardTableBarrierSetC2.cpp:  if (val != NULL && val->is_Con()) {
c2/cardTableBarrierSetC2.cpp:    const Type* t = val->bottom_type();
c2/cardTableBarrierSetC2.cpp:      && obj == kit->just_allocated_object(kit->control())) {
c2/cardTableBarrierSetC2.cpp:    // We can skip marks on a freshly-allocated object in Eden.
c2/cardTableBarrierSetC2.cpp:    // upon a slow-path allocation, so as to make this card-mark
c2/cardTableBarrierSetC2.cpp:    // All card marks for a (non-array) instance are in one place:
c2/cardTableBarrierSetC2.cpp:  // Get the alias_index for raw card-mark memory
c2/cardTableBarrierSetC2.cpp:    if (ct->scanned_concurrently()) {
c2/cardTableBarrierSetC2.cpp:      kit->insert_mem_bar(Op_MemBarVolatile, oop_store);
c2/cardTableBarrierSetC2.cpp:  if(!ct->scanned_concurrently()) {
c2/cardTableBarrierSetC2.cpp:  kit->final_sync(ideal);
c2/cardTableBarrierSetC2.cpp:  // If necessary, emit some card marks afterwards.  (Non-arrays only.)
c2/cardTableBarrierSetC2.cpp:    post_barrier(kit, kit->control(),
c2/cardTableBarrierSetC2.cpp:                 kit->memory(raw_adr_type),
c2/cardTableBarrierSetC2.cpp:  return ModRefBarrierSetC2::is_gc_barrier_node(node) || node->Opcode() == Op_StoreCM;
c2/cardTableBarrierSetC2.cpp:  assert(node->Opcode() == Op_CastP2X, "ConvP2XNode required");
c2/cardTableBarrierSetC2.cpp:  Node *shift = node->unique_out();
c2/cardTableBarrierSetC2.cpp:  Node *addp = shift->unique_out();
c2/cardTableBarrierSetC2.cpp:  for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {
c2/cardTableBarrierSetC2.cpp:    Node *mem = addp->last_out(j);
c2/cardTableBarrierSetC2.cpp:    if (UseCondCardMark && mem->is_Load()) {
c2/cardTableBarrierSetC2.cpp:      assert(mem->Opcode() == Op_LoadB, "unexpected code shape");
c2/cardTableBarrierSetC2.cpp:      macro->replace_node(mem, macro->intcon(0));
c2/cardTableBarrierSetC2.cpp:    assert(mem->is_Store(), "store required");
c2/cardTableBarrierSetC2.cpp:    macro->replace_node(mem, mem->in(MemNode::Memory));
c2/cardTableBarrierSetC2.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c2/barrierSetC2.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c2/barrierSetC2.hpp:  virtual void set_memory() {} // no-op for normal accesses, but not for atomic accesses.
c2/barrierSetC2.hpp:// This is the top-level class for the backend of the Access API in C2.
c2/barrierSetC2.hpp:// The top-level class is responsible for performing raw accesses. The
c2/barrierSetC2.hpp:  // This is the entry-point for the backend to perform accesses through the Access API.
c2/barrierSetC2.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c2/barrierSetC2.cpp:// By default this is a no-op.
c2/barrierSetC2.cpp:  return _kit->barrier_set_state();
c2/barrierSetC2.cpp:PhaseGVN& C2ParseAccess::gvn() const { return _kit->gvn(); }
c2/barrierSetC2.cpp:    if (is_mixed || !is_unordered || (mismatched && !_addr.type()->isa_aryptr())) {
c2/barrierSetC2.cpp:      Node* new_val = kit->dstore_rounding(val.node());
c2/barrierSetC2.cpp:    store = kit->store_to_memory(kit->control(), access.addr().node(), val.node(), access.type(),
c2/barrierSetC2.cpp:    int alias = gvn.C->get_alias_index(adr_type);
c2/barrierSetC2.cpp:    Node* mem = mm->memory_at(alias);
c2/barrierSetC2.cpp:      st->set_unaligned_access();
c2/barrierSetC2.cpp:      st->set_mismatched_access();
c2/barrierSetC2.cpp:      mm->set_memory_at(alias, st);
c2/barrierSetC2.cpp:    Node* control = control_dependent ? kit->control() : NULL;
c2/barrierSetC2.cpp:      Node* mem = kit->immutable_memory();
c2/barrierSetC2.cpp:      load = LoadNode::make(kit->gvn(), control, mem, adr,
c2/barrierSetC2.cpp:      load = kit->gvn().transform(load);
c2/barrierSetC2.cpp:      load = kit->make_load(control, adr, val_type, access.type(), adr_type, mo,
c2/barrierSetC2.cpp:    Node* mem = mm->memory_at(gvn.C->get_alias_index(adr_type));
c2/barrierSetC2.cpp:      // Memory-model-wise, a LoadStore acts like a little synchronized
c2/barrierSetC2.cpp:        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);
c2/barrierSetC2.cpp:          _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);
c2/barrierSetC2.cpp:          _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);
c2/barrierSetC2.cpp:        _leading_membar = kit->insert_mem_bar(Op_MemBarRelease);
c2/barrierSetC2.cpp:        _leading_membar = kit->insert_mem_bar(Op_MemBarVolatile);
c2/barrierSetC2.cpp:      kit->insert_mem_bar(Op_MemBarCPUOrder);
c2/barrierSetC2.cpp:      kit->insert_mem_bar(Op_MemBarCPUOrder);
c2/barrierSetC2.cpp:        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);
c2/barrierSetC2.cpp:          MemBarNode::set_load_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());
c2/barrierSetC2.cpp:        Node* mb = kit->insert_mem_bar(Op_MemBarVolatile, n); // Use fat membar
c2/barrierSetC2.cpp:          MemBarNode::set_store_pair(_leading_membar->as_MemBar(), mb->as_MemBar());
c2/barrierSetC2.cpp:        Node* mb = kit->insert_mem_bar(Op_MemBarAcquire, n);
c2/barrierSetC2.cpp:        mb->as_MemBar()->set_trailing_load();
c2/barrierSetC2.cpp:    // Non-volatile fields also need releasing stores if they hold an
c2/barrierSetC2.cpp:    if (!needs_cpu_membar() && adr_type->isa_instptr()) {
c2/barrierSetC2.cpp:      assert(adr_type->meet(TypePtr::NULL_PTR) != adr_type->remove_speculative(), "should be not null");
c2/barrierSetC2.cpp:        int s = Klass::layout_helper_size_in_bytes(adr_type->isa_instptr()->klass()->layout_helper());
c2/barrierSetC2.cpp://--------------------------- atomic operations---------------------------------
c2/barrierSetC2.cpp:  Node* proj = kit->gvn().transform(new SCMemProjNode(load_store));
c2/barrierSetC2.cpp:  kit->set_memory(proj, access.alias_idx());
c2/barrierSetC2.cpp:  Node *mem = _kit->memory(_alias_idx);
c2/barrierSetC2.cpp:    if (adr->bottom_type()->is_ptr_to_narrowoop()) {
c2/barrierSetC2.cpp:      Node *newval_enc = kit->gvn().transform(new EncodePNode(new_val, new_val->bottom_type()->make_narrowoop()));
c2/barrierSetC2.cpp:      Node *oldval_enc = kit->gvn().transform(new EncodePNode(expected_val, expected_val->bottom_type()->make_narrowoop()));
c2/barrierSetC2.cpp:      load_store = new CompareAndExchangeNNode(kit->control(), mem, adr, newval_enc, oldval_enc, adr_type, value_type->make_narrowoop(), mo);
c2/barrierSetC2.cpp:      load_store = new CompareAndExchangePNode(kit->control(), mem, adr, new_val, expected_val, adr_type, value_type->is_oopptr(), mo);
c2/barrierSetC2.cpp:        load_store = new CompareAndExchangeBNode(kit->control(), mem, adr, new_val, expected_val, adr_type, mo);
c2/barrierSetC2.cpp:        load_store = new CompareAndExchangeSNode(kit->control(), mem, adr, new_val, expected_val, adr_type, mo);
c2/barrierSetC2.cpp:        load_store = new CompareAndExchangeINode(kit->control(), mem, adr, new_val, expected_val, adr_type, mo);
c2/barrierSetC2.cpp:        load_store = new CompareAndExchangeLNode(kit->control(), mem, adr, new_val, expected_val, adr_type, mo);
c2/barrierSetC2.cpp:  load_store->as_LoadStore()->set_barrier_data(access.barrier_data());
c2/barrierSetC2.cpp:  load_store = kit->gvn().transform(load_store);
c2/barrierSetC2.cpp:  if (access.is_oop() && adr->bottom_type()->is_ptr_to_narrowoop()) {
c2/barrierSetC2.cpp:    return kit->gvn().transform(new DecodeNNode(load_store, load_store->get_ptr_type()));
c2/barrierSetC2.cpp:    if (adr->bottom_type()->is_ptr_to_narrowoop()) {
c2/barrierSetC2.cpp:      Node *newval_enc = kit->gvn().transform(new EncodePNode(new_val, new_val->bottom_type()->make_narrowoop()));
c2/barrierSetC2.cpp:      Node *oldval_enc = kit->gvn().transform(new EncodePNode(expected_val, expected_val->bottom_type()->make_narrowoop()));
c2/barrierSetC2.cpp:        load_store = new WeakCompareAndSwapNNode(kit->control(), mem, adr, newval_enc, oldval_enc, mo);
c2/barrierSetC2.cpp:        load_store = new CompareAndSwapNNode(kit->control(), mem, adr, newval_enc, oldval_enc, mo);
c2/barrierSetC2.cpp:        load_store = new WeakCompareAndSwapPNode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:        load_store = new CompareAndSwapPNode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:          load_store = new WeakCompareAndSwapBNode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:          load_store = new CompareAndSwapBNode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:          load_store = new WeakCompareAndSwapSNode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:          load_store = new CompareAndSwapSNode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:          load_store = new WeakCompareAndSwapINode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:          load_store = new CompareAndSwapINode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:          load_store = new WeakCompareAndSwapLNode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:          load_store = new CompareAndSwapLNode(kit->control(), mem, adr, new_val, expected_val, mo);
c2/barrierSetC2.cpp:  load_store->as_LoadStore()->set_barrier_data(access.barrier_data());
c2/barrierSetC2.cpp:  load_store = kit->gvn().transform(load_store);
c2/barrierSetC2.cpp:    if (adr->bottom_type()->is_ptr_to_narrowoop()) {
c2/barrierSetC2.cpp:      Node *newval_enc = kit->gvn().transform(new EncodePNode(new_val, new_val->bottom_type()->make_narrowoop()));
c2/barrierSetC2.cpp:      load_store = kit->gvn().transform(new GetAndSetNNode(kit->control(), mem, adr, newval_enc, adr_type, value_type->make_narrowoop()));
c2/barrierSetC2.cpp:      load_store = new GetAndSetPNode(kit->control(), mem, adr, new_val, adr_type, value_type->is_oopptr());
c2/barrierSetC2.cpp:        load_store = new GetAndSetBNode(kit->control(), mem, adr, new_val, adr_type);
c2/barrierSetC2.cpp:        load_store = new GetAndSetSNode(kit->control(), mem, adr, new_val, adr_type);
c2/barrierSetC2.cpp:        load_store = new GetAndSetINode(kit->control(), mem, adr, new_val, adr_type);
c2/barrierSetC2.cpp:        load_store = new GetAndSetLNode(kit->control(), mem, adr, new_val, adr_type);
c2/barrierSetC2.cpp:  load_store->as_LoadStore()->set_barrier_data(access.barrier_data());
c2/barrierSetC2.cpp:  load_store = kit->gvn().transform(load_store);
c2/barrierSetC2.cpp:  if (access.is_oop() && adr->bottom_type()->is_ptr_to_narrowoop()) {
c2/barrierSetC2.cpp:    return kit->gvn().transform(new DecodeNNode(load_store, load_store->get_ptr_type()));
c2/barrierSetC2.cpp:      load_store = new GetAndAddBNode(kit->control(), mem, adr, new_val, adr_type);
c2/barrierSetC2.cpp:      load_store = new GetAndAddSNode(kit->control(), mem, adr, new_val, adr_type);
c2/barrierSetC2.cpp:      load_store = new GetAndAddINode(kit->control(), mem, adr, new_val, adr_type);
c2/barrierSetC2.cpp:      load_store = new GetAndAddLNode(kit->control(), mem, adr, new_val, adr_type);
c2/barrierSetC2.cpp:  load_store->as_LoadStore()->set_barrier_data(access.barrier_data());
c2/barrierSetC2.cpp:  load_store = kit->gvn().transform(load_store);
c2/barrierSetC2.cpp:  // 8  - 32-bit VM
c2/barrierSetC2.cpp:  // 12 - 64-bit VM, compressed klass
c2/barrierSetC2.cpp:  // 16 - 64-bit VM, normal klass
c2/barrierSetC2.cpp:  Node* offset = kit->MakeConX(base_off);
c2/barrierSetC2.cpp:  payload_size = kit->gvn().transform(new SubXNode(payload_size, offset));
c2/barrierSetC2.cpp:  payload_size = kit->gvn().transform(new URShiftXNode(payload_size, kit->intcon(LogBytesPerLong)));
c2/barrierSetC2.cpp:    ac->set_clone_array();
c2/barrierSetC2.cpp:    ac->set_clone_inst();
c2/barrierSetC2.cpp:  Node* n = kit->gvn().transform(ac);
c2/barrierSetC2.cpp:    ac->_adr_type = TypeRawPtr::BOTTOM;
c2/barrierSetC2.cpp:    kit->set_predefined_output_for_runtime_call(ac, ac->in(TypeFunc::Memory), raw_adr_type);
c2/barrierSetC2.cpp:    kit->set_all_memory(n);
c2/barrierSetC2.cpp:  macro->set_eden_pointers(eden_top_adr, eden_end_adr);
c2/barrierSetC2.cpp:  Node *eden_end = macro->make_load(ctrl, mem, eden_end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);
c2/barrierSetC2.cpp:  // We need a Region for the loop-back contended case.
c2/barrierSetC2.cpp:    // Now handle the passing-too-big test.  We fall into the contended
c2/barrierSetC2.cpp:    // loop-back merge point.
c2/barrierSetC2.cpp:    contended_region    ->init_req(fall_in_path, toobig_false);
c2/barrierSetC2.cpp:    contended_phi_rawmem->init_req(fall_in_path, mem);
c2/barrierSetC2.cpp:    macro->transform_later(contended_region);
c2/barrierSetC2.cpp:    macro->transform_later(contended_phi_rawmem);
c2/barrierSetC2.cpp:  // Load(-locked) the heap top.
c2/barrierSetC2.cpp:  macro->transform_later(old_eden_top);
c2/barrierSetC2.cpp:  Node *new_eden_top = new AddPNode(macro->top(), old_eden_top, size_in_bytes);
c2/barrierSetC2.cpp:  macro->transform_later(new_eden_top);
c2/barrierSetC2.cpp:  macro->transform_later(needgc_cmp);
c2/barrierSetC2.cpp:  macro->transform_later(needgc_bol);
c2/barrierSetC2.cpp:  macro->transform_later(needgc_iff);
c2/barrierSetC2.cpp:  // Plug the failing-heap-space-need-gc test into the slow-path region
c2/barrierSetC2.cpp:  macro->transform_later(needgc_true);
c2/barrierSetC2.cpp:  // No need for a GC.  Setup for the Store-Conditional
c2/barrierSetC2.cpp:  macro->transform_later(needgc_false);
c2/barrierSetC2.cpp:  i_o = macro->prefetch_allocation(i_o, needgc_false, contended_phi_rawmem,
c2/barrierSetC2.cpp:  // Store (-conditional) the modified eden top back down.
c2/barrierSetC2.cpp:    macro->transform_later(store_eden_top);
c2/barrierSetC2.cpp:    macro->transform_later(store_eden_top);
c2/barrierSetC2.cpp:    macro->transform_later(contention_check);
c2/barrierSetC2.cpp:    macro->transform_later(store_eden_top);
c2/barrierSetC2.cpp:    macro->transform_later(contention_iff);
c2/barrierSetC2.cpp:    macro->transform_later(contention_true);
c2/barrierSetC2.cpp:    contended_region->init_req(contended_loopback_path, contention_true);
c2/barrierSetC2.cpp:    contended_phi_rawmem->init_req(contended_loopback_path, store_eden_top);
c2/barrierSetC2.cpp:    // Fast-path succeeded with no contention!
c2/barrierSetC2.cpp:    macro->transform_later(contention_false);
c2/barrierSetC2.cpp:    macro->transform_later(thread);
c2/barrierSetC2.cpp:    Node* alloc_bytes_adr = macro->basic_plus_adr(macro->top()/*not oop*/, thread,
c2/barrierSetC2.cpp:    Node* alloc_bytes = macro->make_load(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,
c2/barrierSetC2.cpp:    macro->transform_later(alloc_size);
c2/barrierSetC2.cpp:    macro->transform_later(new_alloc_bytes);
c2/barrierSetC2.cpp:    fast_oop_rawmem = macro->make_store(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,
c2/barrierSetC2.cpp:#define XTOP LP64_ONLY(COMMA phase->top())
c2/barrierSetC2.cpp:  Node* ctrl = ac->in(TypeFunc::Control);
c2/barrierSetC2.cpp:  Node* mem = ac->in(TypeFunc::Memory);
c2/barrierSetC2.cpp:  Node* src = ac->in(ArrayCopyNode::Src);
c2/barrierSetC2.cpp:  Node* src_offset = ac->in(ArrayCopyNode::SrcPos);
c2/barrierSetC2.cpp:  Node* dest = ac->in(ArrayCopyNode::Dest);
c2/barrierSetC2.cpp:  Node* dest_offset = ac->in(ArrayCopyNode::DestPos);
c2/barrierSetC2.cpp:  Node* length = ac->in(ArrayCopyNode::Length);
c2/barrierSetC2.cpp:  Node* payload_src = phase->basic_plus_adr(src, src_offset);
c2/barrierSetC2.cpp:  Node* payload_dst = phase->basic_plus_adr(dest, dest_offset);
c2/barrierSetC2.cpp:  address     copyfunc_addr = phase->basictype2arraycopy(T_LONG, NULL, NULL, true, copyfunc_name, true);
c2/barrierSetC2.cpp:  Node* call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);
c2/barrierSetC2.cpp:  phase->transform_later(call);
c2/barrierSetC2.cpp:  phase->igvn().replace_node(ac, call);
c2/modRefBarrierSetC2.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
c2/modRefBarrierSetC2.cpp:  uint adr_idx = kit->C->get_alias_index(adr_type);
c2/modRefBarrierSetC2.cpp:  pre_barrier(kit, true /* do_load */, kit->control(), access.base(), adr, adr_idx, val.node(),
c2/modRefBarrierSetC2.cpp:  post_barrier(kit, kit->control(), access.raw_access(), access.base(), adr, adr_idx, val.node(),
c2/modRefBarrierSetC2.cpp:              kit->control(), NULL, NULL, max_juint, NULL, NULL,
c2/modRefBarrierSetC2.cpp:  post_barrier(kit, kit->control(), access.raw_access(), access.base(),
c2/modRefBarrierSetC2.cpp:              kit->control(), NULL, NULL, max_juint, NULL, NULL,
c2/modRefBarrierSetC2.cpp:    kit->sync_kit(ideal);
c2/modRefBarrierSetC2.cpp:  kit->final_sync(ideal);
c2/modRefBarrierSetC2.cpp:              kit->control(), NULL, NULL, max_juint, NULL, NULL,
c2/modRefBarrierSetC2.cpp:  post_barrier(kit, kit->control(), access.raw_access(), access.base(), access.addr().node(),
c2/modRefBarrierSetC2.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardGeneration.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardGeneration.cpp:  _rs->resize_covered_region(committed_mr);
cardGeneration.cpp:  guarantee(_rs->is_aligned(reserved_mr.start()), "generation must be card aligned");
cardGeneration.cpp:  if (reserved_mr.end() != GenCollectedHeap::heap()->reserved_region().end()) {
cardGeneration.cpp:    guarantee(_rs->is_aligned(reserved_mr.end()), "generation must be card aligned");
cardGeneration.cpp:    MemRegion mr(space()->bottom(), new_word_size);
cardGeneration.cpp:    GenCollectedHeap::heap()->rem_set()->resize_covered_region(mr);
cardGeneration.cpp:    _bts->resize(new_word_size);
cardGeneration.cpp:      MemRegion mangle_region(space()->end(),
cardGeneration.cpp:    // Expand space -- also expands space's BOT
cardGeneration.cpp:    space()->set_end((HeapWord*)_virtual_space.high());
cardGeneration.cpp:    size_t old_mem_size = new_mem_size - bytes;
cardGeneration.cpp:  space()->set_end((HeapWord*) _virtual_space.high());
cardGeneration.cpp:  size_t new_word_size = heap_word_size(space()->capacity());
cardGeneration.cpp:  _bts->resize(new_word_size);
cardGeneration.cpp:  MemRegion mr(space()->bottom(), new_word_size);
cardGeneration.cpp:  GenCollectedHeap::heap()->rem_set()->resize_covered_region(mr);
cardGeneration.cpp:  _rs->clear(reserved());
cardGeneration.cpp:  _rs->invalidate(used_region());
cardGeneration.cpp:  // We don't have floating point command-line arguments
cardGeneration.cpp:  const double maximum_used_percentage = 1.0 - minimum_free_percentage;
cardGeneration.cpp:    size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;
cardGeneration.cpp:  size_t max_shrink_bytes = capacity_after_gc - minimum_desired_capacity;
cardGeneration.cpp:    const double minimum_used_percentage = 1.0 - maximum_free_percentage;
cardGeneration.cpp:      shrink_bytes = capacity_after_gc - maximum_desired_capacity;
cardGeneration.cpp:    size_t expansion_for_promotion = capacity_after_gc - _capacity_at_prologue;
cardGeneration.cpp:  blk->do_space(space());
cardGeneration.cpp:  // Apply "cl->do_oop" to (the address of) (exactly) all the ref fields in
cardGeneration.cpp:  _rs->younger_refs_in_space_iterate(space(), gen_boundary, blk, n_threads);
cardGeneration.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardGeneration.hpp:// and uses a card-size block-offset array to implement block_start.
cardGeneration.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardGeneration.inline.hpp:  return space()->capacity();
cardGeneration.inline.hpp:  return space()->used();
cardGeneration.inline.hpp:  return space()->free();
cardGeneration.inline.hpp:  return space()->used_region();
cardGeneration.inline.hpp:  return space()->is_in(p);
cardTableBarrierSetAssembler.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardTableBarrierSet.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardTableBarrierSet.cpp:  _card_table->dirty_MemRegion(mr);
cardTableBarrierSet.cpp:  _card_table->invalidate(mr);
cardTableBarrierSet.cpp:  _card_table->print_on(st);
cardTableBarrierSet.cpp:// compiled code may elide card-marks for initializing stores
cardTableBarrierSet.cpp:// to a newly allocated object along the fast-path. We
cardTableBarrierSet.cpp:// compensate for such elided card-marks as follows:
cardTableBarrierSet.cpp:// (a) Generational, non-concurrent collectors, such as
cardTableBarrierSet.cpp://     need the card-mark if and only if the region is
cardTableBarrierSet.cpp://     in the old gen, and do not care if the card-mark
cardTableBarrierSet.cpp://     so long as the card-mark is completed before the next
cardTableBarrierSet.cpp://     G1 concurrent marking is in progress an SATB (pre-write-)barrier
cardTableBarrierSet.cpp://     is used to remember the pre-value of any store. Initializing
cardTableBarrierSet.cpp://     compensating for the missing pre-barrier here. Turning now
cardTableBarrierSet.cpp://     to the post-barrier, we note that G1 needs a RS update barrier
cardTableBarrierSet.cpp://     that this barrier need only be applied to a non-young write,
cardTableBarrierSet.cpp://     must strictly follow the oop-store.
cardTableBarrierSet.cpp:  // If a previous card-mark was deferred, flush it now.
cardTableBarrierSet.cpp:  if (new_obj->is_typeArray() || _card_table->is_in_young(new_obj)) {
cardTableBarrierSet.cpp:    // Arrays of non-references don't need a post-barrier.
cardTableBarrierSet.cpp:    assert(thread->deferred_card_mark().is_empty(), "Error");
cardTableBarrierSet.cpp:    MemRegion mr(cast_from_oop<HeapWord*>(new_obj), new_obj->size());
cardTableBarrierSet.cpp:      thread->set_deferred_card_mark(mr);
cardTableBarrierSet.cpp:  MemRegion deferred = thread->deferred_card_mark();
cardTableBarrierSet.cpp:      assert(!_card_table->is_in_young(old_obj),
cardTableBarrierSet.cpp:      assert(deferred.word_size() == (size_t)(old_obj->size()),
cardTableBarrierSet.cpp:    thread->set_deferred_card_mark(MemRegion());
cardTableBarrierSet.cpp:  assert(thread->deferred_card_mark().is_empty(), "invariant");
cardTableBarrierSet.cpp:  assert(thread->deferred_card_mark().is_empty(), "Should be empty");
cardTableBarrierSet.cpp:  // card-table (or other remembered set structure) before GC starts
cardTableBarrierSet.cpp:  // processing the card-table (or other remembered set).
cardTableBarrierSet.cpp:  if (thread->is_Java_thread()) { // Only relevant for Java threads.
cardTableBarrierSet.cpp: return _card_table->scanned_concurrently();
cardTableBarrierSet.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardTableBarrierSet.hpp:  // either precise or imprecise. We make non-virtual inline variants of
cardTableBarrierSet.hpp:  // If a compiler is eliding store barriers for TLAB-allocated objects,
cardTableBarrierSet.hpp:  // we will be informed of a slow-path allocation by a call
cardTableBarrierSet.hpp:  // initialization of the object itself, and no post-store-barriers will
cardTableBarrierSet.hpp:  // barrier until the next slow-path allocation or gc-related safepoint.)
cardTableBarrierSet.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardTableBarrierSet.inline.hpp:  volatile CardValue* byte = _card_table->byte_for(field);
cardTableBarrierSet.inline.hpp:  if (_card_table->scanned_concurrently()) {
cardTable.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardTable.cpp:  assert(_guard_index == cards_required(_whole_heap.word_size()) - 1,
cardTable.cpp:  assert((uintptr_t(_whole_heap.start())  & (card_size - 1))  == 0, "heap must start at card boundary");
cardTable.cpp:  assert((uintptr_t(_whole_heap.end()) & (card_size - 1))  == 0, "heap must end at card boundary");
cardTable.cpp:  _guard_index = cards_required(_whole_heap.word_size()) - 1;
cardTable.cpp:  _last_valid_index = _guard_index - 1;
cardTable.cpp:  _byte_map_base = _byte_map - (uintptr_t(low_bound) >> card_shift);
cardTable.cpp:  assert(byte_for(high_bound-1) <= &_byte_map[_last_valid_index], "Checking end of map");
cardTable.cpp:  for (int j = _cur_covered_regions; j > i; j--) {
cardTable.cpp:    _covered[j] = _covered[j-1];
cardTable.cpp:    _committed[j] = _committed[j-1];
cardTable.cpp:  return -1;
cardTable.cpp:    // that the would-be end of the new committed region would have
cardTable.cpp:      if (_committed[cr].contains(new_end - 1)) {
cardTable.cpp:    cur = byte_after(mr.start() - 1);
cardTable.cpp:  memset(first, dirty_card, last-first);
cardTable.cpp:          cl->do_MemRegion(cur_cards);
cardTable.cpp:  st->print_cr("Card table byte_map: [" INTPTR_FORMAT "," INTPTR_FORMAT "] _byte_map_base: " INTPTR_FORMAT,
cardTable.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardTable.hpp:  // The committed regions correspond one-to-one to the covered regions.
cardTable.hpp:  // They represent the card-table memory that has been committed to service
cardTable.hpp:  // one covered region corresponds to a larger region because of page-size
cardTable.hpp:  // actually extend onto the card-table space for the next covered region.
cardTable.hpp:    clean_card                  = (CardValue)-1,
cardTable.hpp:  static const intptr_t clean_card_row = (intptr_t)(-1);
cardTable.hpp:  // Provide read-only access to the card table array.
cardTable.hpp:    return byte_for(p) - _byte_map;
cardTable.hpp:  // *** Card-table-RemSet-specific things.
cardTable.hpp:  // val_equals -> it will check that all cards covered by mr equal val
cardTable.hpp:  // !val_equals -> it will check that all cards covered by mr do not equal val
cardTableRS.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardTableRS.cpp:    if (cld->has_accumulated_modified_oops()) {
cardTableRS.cpp:    if (cld->has_accumulated_modified_oops()) {
cardTableRS.cpp:      cld->clear_accumulated_modified_oops();
cardTableRS.cpp:        || _ct->is_prev_youngergen_card_val(entry_val)) {
cardTableRS.cpp:      *entry = _ct->cur_youngergen_card_val();
cardTableRS.cpp:      assert(entry_val == _ct->cur_youngergen_card_val(),
cardTableRS.cpp:  return (((intptr_t)entry) & (BytesPerWord-1)) == 0;
cardTableRS.cpp:  assert(_ct->is_aligned(mr.start()), "mr.start() should be card aligned");
cardTableRS.cpp:  CardValue* cur_entry = _ct->byte_for(mr.last());
cardTableRS.cpp:  const CardValue* limit = _ct->byte_for(mr.start());
cardTableRS.cpp:    HeapWord* cur_hw = _ct->addr_for(cur_entry);
cardTableRS.cpp:      // We hit a "clean" card; process any non-empty
cardTableRS.cpp:        _dirty_card_closure->do_MemRegion(mrd);
cardTableRS.cpp:      // fast forward through potential continuous whole-word range of clean cards beginning at a word-boundary
cardTableRS.cpp:        CardValue* cur_row = cur_entry - BytesPerWord;
cardTableRS.cpp:          cur_row -= BytesPerWord;
cardTableRS.cpp:        cur_hw = _ct->addr_for(cur_entry);
cardTableRS.cpp:    // will point off of the left end of the card-table
cardTableRS.cpp:    cur_entry--;
cardTableRS.cpp:  // been left with a dirty window, co-initial with "mr",
cardTableRS.cpp:    _dirty_card_closure->do_MemRegion(mrd);
cardTableRS.cpp:  const MemRegion urasm = sp->used_region_at_save_marks();
cardTableRS.cpp:  MemRegion ur    = sp->used_region();
cardTableRS.cpp:  MemRegion urasm = sp->used_region_at_save_marks();
cardTableRS.cpp:  assert(GenCollectedHeap::heap()->is_old_gen(old_gen),
cardTableRS.cpp:  clear(old_gen->prev_used_region());
cardTableRS.cpp:  assert(GenCollectedHeap::heap()->is_old_gen(old_gen),
cardTableRS.cpp:  MemRegion used_mr = old_gen->used_region();
cardTableRS.cpp:  MemRegion to_be_cleared_mr = old_gen->prev_used_region().minus(used_mr);
cardTableRS.cpp:  virtual void do_space(Space* s) { _ct->verify_space(s, _boundary); }
cardTableRS.cpp:    if (GenCollectedHeap::heap()->is_young_gen(gen)) {
cardTableRS.cpp:    VerifyCTSpaceClosure blk(_ct, gen->reserved().start());
cardTableRS.cpp:    gen->space_iterate(&blk, true);
cardTableRS.cpp:  // We don't need to do young-gen spaces.
cardTableRS.cpp:  if (s->end() <= gen_boundary) return;
cardTableRS.cpp:  MemRegion used = s->used_region();
cardTableRS.cpp:      // young-to-old field, that would mark the previous card.
cardTableRS.cpp:      HeapWord* boundary_block = s->block_start(boundary);
cardTableRS.cpp:        if (s->block_is_obj(boundary_block) && s->obj_is_alive(boundary_block)) {
cardTableRS.cpp:          if (!boundary_obj->is_objArray() &&
cardTableRS.cpp:              !boundary_obj->is_typeArray()) {
cardTableRS.cpp:              begin = boundary_block + s->block_size(boundary_block);
cardTableRS.cpp:        for (HeapWord* cur = start_block; cur < end; cur += s->block_size(cur)) {
cardTableRS.cpp:          if (s->block_is_obj(cur) && s->obj_is_alive(cur)) {
cardTableRS.cpp:            oop(cur)->oop_iterate(&verify_blk, mr);
cardTableRS.cpp:      //     card can be re-allocated either due to direct allocation
cardTableRS.cpp:      //     before-gc verification will fail the above assert.
cardTableRS.cpp:      //     re-allocates that part of the heap.
cardTableRS.cpp:      // after re-allocation, it would be dirty, not "stale". Thus,
cardTableRS.cpp:      //   There are two sub-cases to consider:
cardTableRS.cpp:      //     it with its stale value -- because the promotions didn't
cardTableRS.cpp:      // redundant work in a subsequent (low priority) re-design of
cardTableRS.cpp:      // the card-scanning code, if only to simplify the underlying
cardTableRS.cpp:  GenCollectedHeap::heap()->generation_iterate(&blk, false);
cardTableRS.cpp:  // max_gens is really GenCollectedHeap::heap()->gen_policy()->number_of_generations()
cardTableRS.cpp:    _last_LNC_resizing_collection[i] = -1;
cardTableRS.cpp:      // This is the single-threaded version used by DefNew.
cardTableRS.cpp:      DirtyCardToOopClosure* dcto_cl = sp->new_dcto_cl(cl, precision(), gen_boundary, parallel);
cardTableRS.cpp:  return GenCollectedHeap::heap()->is_in_young(obj);
cardTableRS.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
cardTableRS.hpp:  // Iterate over the portion of the card-table which covers the given
cardTableRS.hpp:  // region mr in the given space and apply cl to any dirty sub-regions
cardTableRS.hpp:  // covered region.  Each entry of these arrays is the lowest non-clean
collectedHeap.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
collectedHeap.cpp:  st->print_cr("GC heap %s", m.is_before ? "before" : "after");
collectedHeap.cpp:  st->print_raw(m);
collectedHeap.cpp:                 heap->total_collections(),
collectedHeap.cpp:                 heap->total_full_collections());
collectedHeap.cpp:  heap->print_on(&st);
collectedHeap.cpp:  return capacity() - used();
collectedHeap.cpp:    _gc_heap_log->log_heap_before(this);
collectedHeap.cpp:    _gc_heap_log->log_heap_after(this);
collectedHeap.cpp:  st->print_cr("Heap:");
collectedHeap.cpp:  st->cr();
collectedHeap.cpp:    bs->print_on(st);
collectedHeap.cpp:  gc_tracer->report_gc_heap_summary(when, heap_summary);
collectedHeap.cpp:  gc_tracer->report_metaspace_summary(when, metaspace_summary);
collectedHeap.cpp:  if (is_in(object->klass_or_null())) {
collectedHeap.cpp:  assert(thread->is_VM_thread(), "Precondition#1");
collectedHeap.cpp:  assert(Heap_lock->is_locked(), "Precondition#2");
collectedHeap.cpp:  assert(!Heap_lock->owned_by_self(), "Should not be holding the Heap_lock");
collectedHeap.cpp:    MetaWord* result = loader_data->metaspace_non_null()->allocate(word_size, mdtype);
collectedHeap.cpp:      result = loader_data->metaspace_non_null()->expand_and_allocate(word_size, mdtype);
collectedHeap.cpp:      if (!jthr->in_critical()) {
collectedHeap.cpp:      gc_count      = Universe::heap()->total_collections();
collectedHeap.cpp:      full_gc_count = Universe::heap()->total_full_collections();
collectedHeap.cpp:      assert(*ju_addr == badHeapWordVal, "Found non badHeapWordValue in pre-allocation check");
collectedHeap.cpp:                        words - filler_array_hdr_size(), 0XDEAFBABE);
collectedHeap.cpp:  const size_t payload_size = words - filler_array_hdr_size();
collectedHeap.cpp:    const size_t cur = (words - max) >= min ? max : max - min;
collectedHeap.cpp:    words -= cur;
collectedHeap.cpp:  guarantee(false, "thread-local allocation buffers not supported");
collectedHeap.cpp:         "Should only be called at a safepoint or at start-up");
collectedHeap.cpp:    BarrierSet::barrier_set()->make_parsable(thread);
collectedHeap.cpp:        thread->tlab().retire(&stats);
collectedHeap.cpp:        thread->tlab().make_parsable();
collectedHeap.cpp:      thread->tlab().resize();
collectedHeap.cpp:  return (os::javaTimeNanos() - _last_whole_heap_examined_time_ns) / NANOSECS_PER_MILLISEC;
collectedHeap.cpp:    const size_t elapsed_gcs = gc_num - _promotion_failure_alot_gc_number;
collectedHeap.cpp:      // Test for unsigned arithmetic wrap-around.
collectedHeap.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
collectedHeap.hpp:  // This timestamp must be monotonically non-decreasing to avoid
collectedHeap.hpp:  // time-warp warnings.
collectedHeap.hpp:    assert(kind == heap->kind(), "Heap kind %u should be %u",
collectedHeap.hpp:           static_cast<uint>(heap->kind()), static_cast<uint>(kind));
collectedHeap.hpp:       _perf_gc_lastcause->set_value(GCCause::to_string(_gc_lastcause));
collectedHeap.hpp:       _perf_gc_cause->set_value(GCCause::to_string(v));
collectedHeap.hpp:  // fill_with_objects() can fill arbitrary-sized regions of the heap using
collectedHeap.hpp:  // Some heaps may offer a contiguous region for shared non-blocking
collectedHeap.hpp:  // certain allocation-related activities. Calling this function before
collectedHeap.hpp:  // super::ensure_parsability so that the non-generational
collectedHeap.hpp:  // Section on thread-local allocation buffers (TLABs)
collectedHeap.hpp:  // If the heap supports thread-local allocation buffers, it should override
collectedHeap.hpp:  // Returns "true" iff the heap supports thread-local allocation buffers.
collectedHeap.hpp:  // The amount of space available for thread-local allocation buffers.
collectedHeap.hpp:  // The amount of used space for thread-local allocation buffers for the given thread.
collectedHeap.hpp:  // for thread-local allocation buffers without triggering any
collectedHeap.hpp:    guarantee(false, "thread-local allocation buffers not supported");
collectedHeap.hpp:  // Returns "true" iff there is a stop-world GC in progress.  (I assume
collectedHeap.hpp:  // collector -- dld).
collectedHeap.hpp:  // concurrent marking) for an intermittent non-GC safepoint.
collectedHeap.hpp:  // promotion failure.  The no-argument version uses
collectedHeap.hpp:  // this->_promotion_failure_alot_count as the counter.
collectedHeap.hpp:    _previous_cause = _heap->gc_cause();
collectedHeap.hpp:    _heap->set_gc_cause(cause);
collectedHeap.hpp:    _heap->set_gc_cause(_previous_cause);
collectedHeap.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
collectorCounters.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
collectorCounters.cpp:    PerfTraceTimedEvent(c->time_counter(), c->invocation_counter()),
collectorCounters.cpp:     _c->last_entry_counter()->set_value(os::elapsed_counter());
collectorCounters.cpp:    _c->last_exit_counter()->set_value(os::elapsed_counter());
collectorCounters.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
concurrentGCBreakpoints.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
concurrentGCBreakpoints.cpp:// (2) Active run_to() running     non-NULL      false          false
concurrentGCBreakpoints.cpp:  assert(Thread::current()->is_Java_thread(), "precondition")
concurrentGCBreakpoints.cpp:    Universe::heap()->collect(GCCause::_wb_breakpoint);
concurrentGCBreakpoints.cpp:  assert(Thread::current()->is_ConcurrentGC_thread(), "precondition");
concurrentGCBreakpoints.cpp:  // Ignore non-matching request state.
concurrentGCBreakpoints.cpp:  monitor()->notify_all();
concurrentGCBreakpoints.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
concurrentGCThread.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
concurrentGCThread.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
copyFailedInfo.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcArguments.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcArguments.cpp:    // Turn off gc-overhead-limit-exceeded checks
gcArguments.cpp:    // Keeping the heap 100% free is hard ;-) so limit it to 99%.
gcArguments.cpp:  // User inputs from -Xmx and -Xms must be aligned
gcArguments.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcBehaviours.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcBehaviours.cpp:    if (!_cl->do_object_b(obj)) {
gcBehaviours.cpp:  if (cm->is_nmethod()) {
gcBehaviours.cpp:    static_cast<nmethod*>(cm)->oops_do(&cl, true /* allow_dead */);
gcBehaviours.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcCause.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcCause.cpp:      return "Full GC for -Xshare:dump";
gcCause.cpp:      return "ILLEGAL VALUE - last gc cause - ILLEGAL VALUE";
gcCause.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcCause.hpp:// and implementation-private "causes".
gcConfig.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcConfig.cpp:                                  "Option -XX:+" #option " not supported" : \
gcConfig.cpp:                                  "Option -XX:-" #option " not supported"); \
gcConfig.cpp:    if (gc->_flag) {
gcConfig.cpp:    if (gc->_flag) {
gcConfig.cpp:      if (gc->_name == selected || selected == CollectedHeap::None) {
gcConfig.cpp:        selected = gc->_name;
gcConfig.cpp:    if (gc->_flag) {
gcConfig.cpp:      return &gc->_arguments;
gcConfig.cpp:    if (gc->_name == name && gc->_arguments.is_supported()) {
gcConfig.cpp:    if (gc->_name == name && gc->_flag) {
gcConfig.cpp:      if (gc->_flag) {
gcConfig.cpp:        return gc->_hs_err_name;
gcConfig.cpp:    if (gc->_name == name) {
gcConfig.cpp:      return gc->_hs_err_name;
gcConfig.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcConfiguration.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcConfiguration.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gc_globals.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gc_globals.hpp:          "Use the Garbage-First garbage collector")                        \
gc_globals.hpp:          "Use the Epsilon (no-op) garbage collector")                      \
gc_globals.hpp:          "Percentage (0-100) used to weight the current sample when "      \
gc_globals.hpp:          "Force all freshly committed pages to be pre-touched")            \
gc_globals.hpp:          "Per-thread chunk size for parallel memory pre-touch.")           \
gc_globals.hpp:  /* where does the range max value of (max_jint - 1) come from? */         \
gc_globals.hpp:          range(1, (max_jint - 1))                                          \
gc_globals.hpp:          range(1, (max_jint - 1))                                          \
gc_globals.hpp:          "reference-based(0) or referent-based(1)")                        \
gc_globals.hpp:  product_pd(bool, UseTLAB, "Use thread-local object allocation")           \
gc_globals.hpp:          "Never act like a server-class machine")                          \
gc_globals.hpp:          "Always act like a server-class machine")                         \
gc_globals.hpp:          "Optimize heap options for long-running memory intensive apps")   \
gc_globals.hpp:  product(int, ActiveProcessorCount, -1,                                    \
gc_globals.hpp:          "Use adaptive young-old sizing policies at minor collections")    \
gc_globals.hpp:          "Use adaptive young-old sizing policies at major collections")    \
gc_globals.hpp:  develop(intx, PSAdaptiveSizePolicyResizeVirtualSpaceAlot, -1,             \
gc_globals.hpp:          range(-1, 1)                                                      \
gc_globals.hpp:  product(uintx, MaxGCPauseMillis, max_uintx - 1,                           \
gc_globals.hpp:          range(1, max_uintx - 1)                                           \
gc_globals.hpp:  product(intx, PrefetchCopyIntervalInBytes, -1,                            \
gc_globals.hpp:          range(-1, max_jint)                                               \
gc_globals.hpp:  product(intx, PrefetchScanIntervalInBytes, -1,                            \
gc_globals.hpp:          range(-1, max_jint)                                               \
gc_globals.hpp:  product(intx, PrefetchFieldsAhead, -1,                                    \
gc_globals.hpp:          range(-1, max_jint)                                               \
gc_globals.hpp:          "Memory sub-systems to verify when Verify*GC flag(s) "            \
gc_globals.hpp:          "are enabled. One or more sub-systems can be specified "          \
gc_globals.hpp:          "in a comma separated string. Sub-systems are: "                  \
gc_globals.hpp:          "Deprecated, use -Xlog:gc instead.")                              \
gc_globals.hpp:          "Deprecated, use -Xlog:gc* instead.")                             \
gc_globals.hpp:          "If non-zero, assert that GC threads yield within this "          \
gc_globals.hpp:          range(1, max_uintx-2)                                             \
gc_globals.hpp:          range(0, max_uintx-1)                                             \
gc_globals.hpp:          "non-daemon thread (in bytes)")                                   \
gc_globals.hpp:          "Percentage (0-100) of the old gen allowed as dead wood. "        \
gcHeapSummary.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcHeapSummary.hpp:  size_t committed_size() const { return (uintptr_t)_committed_end - (uintptr_t)_start;  }
gcHeapSummary.hpp:  size_t reserved_size() const { return (uintptr_t)_reserved_end - (uintptr_t)_start; }
gcHeapSummary.hpp:  size_t size() const { return (uintptr_t)_end - (uintptr_t)_start; }
gcHeapSummary.hpp:     visitor->visit(this);
gcHeapSummary.hpp:     visitor->visit(this);
gcHeapSummary.hpp:     visitor->visit(this);
gcId.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcId.cpp:  assert(Thread::current()->is_Named_thread(), "This thread must be NamedThread");
gcId.cpp:  const uint gc_id = currentNamedthread()->gc_id();
gcId.cpp:  return Thread::current()->is_Named_thread() ? currentNamedthread()->gc_id() : undefined();
gcId.cpp:GCIdMark::GCIdMark() : _previous_gc_id(currentNamedthread()->gc_id()) {
gcId.cpp:  currentNamedthread()->set_gc_id(GCId::create());
gcId.cpp:GCIdMark::GCIdMark(uint gc_id) : _previous_gc_id(currentNamedthread()->gc_id()) {
gcId.cpp:  currentNamedthread()->set_gc_id(gc_id);
gcId.cpp:  currentNamedthread()->set_gc_id(_previous_gc_id);
gcId.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcId.hpp:  static const uint UNDEFINED = (uint)-1;
gcInitLogger.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcInitLogger.cpp:  log_info_p(gc, init)("Pre-touch: %s", AlwaysPreTouch ? "Enabled" : "Disabled");
gcInitLogger.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcLocker.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcLocker.cpp:      if (thr->in_critical()) {
gcLocker.cpp:        if (thr->in_critical()) {
gcLocker.cpp:          log_error(gc, verify)(INTPTR_FORMAT " in_critical %d", p2i(thr), thr->in_critical());
gcLocker.cpp:    log.debug("%s Thread \"%s\" %d locked.", msg, Thread::current()->name(), _jni_lock_count);
gcLocker.cpp:  assert(!JavaThread::current()->in_critical(), "Would deadlock");
gcLocker.cpp:  assert(!thread->in_critical(), "shouldn't currently be in a critical region");
gcLocker.cpp:  thread->enter_critical();
gcLocker.cpp:  assert(thread->in_last_critical(), "should be exiting critical region");
gcLocker.cpp:  _jni_lock_count--;
gcLocker.cpp:  thread->exit_critical();
gcLocker.cpp:    _total_collections = Universe::heap()->total_collections();
gcLocker.cpp:      Universe::heap()->collect(GCCause::_gc_locker);
gcLocker.cpp:    JNICritical_lock->notify_all();
gcLocker.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcLocker.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcLocker.inline.hpp:  if (!thread->in_critical()) {
gcLocker.inline.hpp:  thread->enter_critical();
gcLocker.inline.hpp:  if (thread->in_last_critical()) {
gcLocker.inline.hpp:  thread->exit_critical();
gcLogPrecious.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcLogPrecious.cpp:  _temp->reset();
gcLogPrecious.cpp:  _temp->vprint(format, args);
gcLogPrecious.cpp:  _lines->print_cr(" %s", _temp->base());
gcLogPrecious.cpp:  log.print("%s", _temp->base());
gcLogPrecious.cpp:    DEBUG_ONLY(debug_message = strdup(_temp->base()));
gcLogPrecious.cpp:    if (_lines->size() > 0) {
gcLogPrecious.cpp:      st->print_cr("GC Precious Log:");
gcLogPrecious.cpp:      st->print_cr("%s", _lines->base());
gcLogPrecious.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcName.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcOverheadChecker.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcOverheadChecker.cpp:    if (time_overhead->is_exceeded() && space_overhead->is_exceeded()) {
gcOverheadChecker.cpp:          // All conditions have been met for throwing an out-of-memory
gcOverheadChecker.cpp:          // throw an out-of-memory before all SoftRef's have been
gcOverheadChecker.cpp:            soft_ref_policy->set_should_clear_all_soft_refs(true);
gcOverheadChecker.cpp:      // cause an out-of-memory.  Diagnostic message indicating
gcOverheadChecker.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcOverheadChecker.hpp:    return _gc_overhead_limit_count >= (GCOverheadLimitThreshold - 1);
gcOverheadChecker.hpp:  // Check the conditions for an out-of-memory due to excessive GC time.
gcPolicyCounters.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcPolicyCounters.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcStats.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcStats.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcThreadLocalData.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcThreadLocalData.hpp:// Thread local data area for GC-specific information. Each GC
gcThreadLocalData.hpp:// area. It is represented as a 64-bit aligned opaque blob to
gcThreadLocalData.hpp:// GC-specific type describing the structure of the data. GCs
gcTimer.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcTimer.cpp:  _next_phase_level--;
gcTimer.cpp:  assert(level < count(), "Out-of-bounds");
gcTimer.cpp:  int index = _active_phases.phase_index(level - 1);
gcTimer.cpp:  GCPhase phase = _phases->at(index);
gcTimer.cpp:  _phases->clear();
gcTimer.cpp:  assert(_phases->length() <= 1000, "Too many recored phases?");
gcTimer.cpp:  int index = _phases->append(phase);
gcTimer.cpp:  assert(level == 0, "Must be a top-level phase");
gcTimer.cpp:  assert(level > 0, "Must be a sub-phase");
gcTimer.cpp:  if ((phase->type() == GCPhase::PausePhaseType) && (phase->level() == 0)) {
gcTimer.cpp:    const Tickspan pause = phase->end() - phase->start();
gcTimer.cpp:  GCPhase* phase = _phases->adr_at(phase_index);
gcTimer.cpp:  phase->set_end(time);
gcTimer.cpp:  return _phases->length();
gcTimer.cpp:  assert(index < _phases->length(), "Out of bounds");
gcTimer.cpp:  return _phases->adr_at(index);
gcTimer.cpp:  return _next < _time_partitions->num_phases();
gcTimer.cpp:  return _time_partitions->phase_at(_next++);
gcTimer.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcTimer.hpp:    visitor->visit(this);
gcTrace.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcTrace.cpp:  _shared_gc_info.set_sum_of_pauses(time_partitions->sum_of_pauses());
gcTrace.cpp:  _shared_gc_info.set_longest_pause(time_partitions->longest_pause());
gcTrace.cpp:    double percentage_of_heap = ((double) entry->words()) / _total_size_in_words;
gcTrace.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcTrace.hpp:  static const uint UNSET_TENURING_THRESHOLD = (uint) -1;
gcTraceSend.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcTraceSend.cpp:    const VirtualSpaceSummary& heap_space = heap_summary->heap();
gcTraceSend.cpp:      e.set_heapUsed(heap_summary->used());
gcTraceSend.cpp:      e.set_edenUsedSize(g1_heap_summary->edenUsed());
gcTraceSend.cpp:      e.set_edenTotalSize(g1_heap_summary->edenCapacity());
gcTraceSend.cpp:      e.set_survivorUsedSize(g1_heap_summary->survivorUsed());
gcTraceSend.cpp:      e.set_numberOfRegions(g1_heap_summary->numberOfRegions());
gcTraceSend.cpp:    const VirtualSpaceSummary& old_summary = ps_heap_summary->old();
gcTraceSend.cpp:    const SpaceSummary& old_space = ps_heap_summary->old_space();
gcTraceSend.cpp:    const VirtualSpaceSummary& young_summary = ps_heap_summary->young();
gcTraceSend.cpp:    const SpaceSummary& eden_space = ps_heap_summary->eden();
gcTraceSend.cpp:    const SpaceSummary& from_space = ps_heap_summary->from();
gcTraceSend.cpp:    const SpaceSummary& to_space = ps_heap_summary->to();
gcTraceSend.cpp:      e.set_oldSpace(to_struct(ps_heap_summary->old()));
gcTraceSend.cpp:      e.set_oldObjectSpace(to_struct(ps_heap_summary->old_space()));
gcTraceSend.cpp:      e.set_youngSpace(to_struct(ps_heap_summary->young()));
gcTraceSend.cpp:      e.set_edenSpace(to_struct(ps_heap_summary->eden()));
gcTraceSend.cpp:      e.set_fromSpace(to_struct(ps_heap_summary->from()));
gcTraceSend.cpp:      e.set_toSpace(to_struct(ps_heap_summary->to()));
gcTraceSend.cpp:    assert(phase->level() < PhasesStack::PHASE_LEVELS, "Need more event types for PausePhase");
gcTraceSend.cpp:    switch (phase->level()) {
gcTraceSend.cpp:    assert(phase->level() < 2, "There is only two levels for ConcurrentPhase");
gcTraceSend.cpp:    switch (phase->level()) {
gcTraceSend.cpp:      event.set_name(phase->name());
gcTraceSend.cpp:      event.set_starttime(phase->start());
gcTraceSend.cpp:      event.set_endtime(phase->end());
gcTraceSend.cpp:    if (phase->type() == GCPhase::PausePhaseType) {
gcTraceSend.cpp:      assert(phase->type() == GCPhase::ConcurrentPhaseType, "Should be ConcurrentPhaseType");
gcTraceSend.cpp:    phase->accept(&phase_reporter);
gcTraceTime.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcTraceTime.cpp:    _heap_usage_before = Universe::heap()->used();
gcTraceTime.cpp:  double duration_in_ms = TimeHelper::counter_to_millis(end.value() - _start.value());
gcTraceTime.cpp:    size_t used_m = heap->used() / M;
gcTraceTime.cpp:    size_t capacity_m = heap->capacity() / M;
gcTraceTime.cpp:    out.print(" " SIZE_FORMAT "M->" SIZE_FORMAT "M("  SIZE_FORMAT "M)", used_before_m, used_m, capacity_m);
gcTraceTime.cpp:                        user_time - _starting_user_time,
gcTraceTime.cpp:                        system_time - _starting_system_time,
gcTraceTime.cpp:                        real_time - _starting_real_time);
gcTraceTime.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcTraceTime.hpp:  // An arbitrary number of callbacks - extend if needed
gcTraceTime.hpp:// GCTraceTime is used to register a sub-phase. The super-phase
gcTraceTime.hpp:// when the GCTraceTimer is used to report the top-level
gcTraceTime.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcTraceTime.inline.hpp:    cb->at_start(start);
gcTraceTime.inline.hpp:    cb->at_end(end);
gcTraceTime.inline.hpp:    _timer->register_gc_phase_start(_title, start);
gcTraceTime.inline.hpp:    _timer->register_gc_phase_end(end);
gcTraceTime.inline.hpp:    _timer->register_gc_pause_start(_title, start);
gcTraceTime.inline.hpp:    _timer->register_gc_pause_end(end);
gcTraceTime.inline.hpp:                                                       TimeHelper::counter_to_millis(stop_time - _start_time));
gcTraceTime.inline.hpp:// Helper macros to support the usual use-cases.
gcTraceTime.inline.hpp://  GCTraceTime(Info, gc, phase) t("The sub-phase name");
gcTraceTime.inline.hpp://  GCTraceTime(Info, gc, phase) t("The sub-phase name", timer);
gcTraceTime.inline.hpp:// The vanilla GCTraceTime macro doesn't cater to all use-cases.
gcTraceTime.inline.hpp://  GCTraceTimePauseTimer       timer(_message, g1h->concurrent_mark()->gc_timer_cm());
gcUtil.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcUtil.cpp:// Catch-all file for utility classes
gcUtil.cpp:  float new_dev = compute_adaptive_average(fabsd(new_sample - new_avg),
gcUtil.cpp:    // We only create a new deviation if the sample is non-zero
gcUtil.cpp:    float new_dev = compute_adaptive_average(fabsd(new_sample - new_avg),
gcUtil.cpp:    slope_denominator = (_mean_x.count() * _sum_x_squared - _sum_x * _sum_x);
gcUtil.cpp:      slope_numerator = (_mean_x.count() * _sum_xy - _sum_x * _sum_y);
gcUtil.cpp:      // _intercept = _mean_y.average() - _slope * _mean_x.average();
gcUtil.cpp:      _intercept = (_sum_y - _slope * _sum_x) / ((double) _mean_x.count());
gcUtil.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcUtil.hpp:// Catch-all file for utility classes
gcUtil.hpp:    return (100.0F - weight) * avg / 100.0F + weight * sample / 100.0F;
gcUtil.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcVMOperations.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcVMOperations.cpp:  ch->soft_ref_policy()->set_all_soft_refs_clear(false);
gcVMOperations.cpp:  bool skip = (_gc_count_before != Universe::heap()->total_collections());
gcVMOperations.cpp:    skip = (_full_gc_count_before != Universe::heap()->total_full_collections());
gcVMOperations.cpp:    skip = Universe::heap()->is_maximal_no_gc();
gcVMOperations.cpp:  Heap_lock->lock();
gcVMOperations.cpp:    Heap_lock->unlock();
gcVMOperations.cpp:    Heap_lock->notify_all();
gcVMOperations.cpp:  Heap_lock->unlock();
gcVMOperations.cpp:  Universe::heap()->collect_as_vm_thread(GCCause::_heap_inspection);
gcVMOperations.cpp:  Universe::heap()->ensure_parsability(false); // must happen, even if collection does
gcVMOperations.cpp:      log_warning(gc)("GC locker is held; pre-dump GC was skipped");
gcVMOperations.cpp:  _result = gch->satisfy_failed_allocation(_word_size, _tlab);
gcVMOperations.cpp:  assert(_result == NULL || gch->is_in_reserved(_result), "result not in heap");
gcVMOperations.cpp:  gch->do_full_collection(gch->must_clear_all_soft_refs(), _max_generation);
gcVMOperations.cpp:    g1h->policy()->collector_state()->set_initiate_conc_mark_if_possible(true);
gcVMOperations.cpp:    bool should_start = g1h->policy()->force_concurrent_start_if_outside_cycle(_gc_cause);
gcVMOperations.cpp:      double pause_target = g1h->policy()->max_pause_time_ms();
gcVMOperations.cpp:      g1h->do_collection_pause_at_safepoint(pause_target);
gcVMOperations.cpp:    _result = _loader_data->metaspace_non_null()->allocate(_size, _mdtype);
gcVMOperations.cpp:    _result = _loader_data->metaspace_non_null()->expand_and_allocate(_size, _mdtype);
gcVMOperations.cpp:  heap->collect_as_vm_thread(GCCause::_metadata_GC_threshold);
gcVMOperations.cpp:  _result = _loader_data->metaspace_non_null()->allocate(_size, _mdtype);
gcVMOperations.cpp:  _result = _loader_data->metaspace_non_null()->expand_and_allocate(_size, _mdtype);
gcVMOperations.cpp:  heap->collect_as_vm_thread(GCCause::_metadata_GC_clear_soft_refs);
gcVMOperations.cpp:  _result = _loader_data->metaspace_non_null()->allocate(_size, _mdtype);
gcVMOperations.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
gcVMOperations.hpp://   - implements methods common to all classes in the hierarchy:
gcVMOperations.hpp://   - prints class histogram on SIGBREAK if PrintClassHistogram
gcVMOperations.hpp://   - this operation is invoked when allocation is failed;
gcVMOperations.hpp://   - these operations preform full collection of heaps of
gcWhen.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
genArguments.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
genArguments.cpp:  size_t max_minus = maximum_size - alignment;
genArguments.cpp:      size_t smaller_max_new_size = MaxHeapSize - GenAlignment;
genArguments.cpp:      FLAG_SET_ERGO(OldSize, MaxHeapSize - NewSize);
genArguments.cpp:      size_t new_size = InitialHeapSize - OldSize;
genArguments.cpp:// also a command line specification of -Xms.  Issue a warning
genArguments.cpp:  MaxOldSize = MAX2(MaxHeapSize - max_young_size, GenAlignment);
genArguments.cpp:    initial_old_size = clamp(InitialHeapSize - initial_young_size, MinOldSize, MaxOldSize);
genArguments.cpp:                            ", -XX:OldSize flag is being ignored",
genArguments.cpp:    MinOldSize = MIN2(initial_old_size, MinHeapSize - MinNewSize);
genArguments.cpp:    size_t desired_young_size = InitialHeapSize - initial_old_size;
genArguments.cpp:      initial_old_size = InitialHeapSize - MinNewSize;
genArguments.cpp:      initial_old_size = InitialHeapSize - max_young_size;
genArguments.cpp:      initial_old_size = InitialHeapSize - MinNewSize;
genArguments.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
genCollectedHeap.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
genCollectedHeap.cpp:  // system which believe this to be true (e.g. oop->object_size in some
genCollectedHeap.cpp:  _rem_set->initialize();
genCollectedHeap.cpp:  bs->initialize();
genCollectedHeap.cpp:  ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec->max_size());
genCollectedHeap.cpp:  _young_gen = _young_gen_spec->init(young_rs, rem_set());
genCollectedHeap.cpp:  ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec->max_size());
genCollectedHeap.cpp:  old_rs = old_rs.first_part(_old_gen_spec->max_size());
genCollectedHeap.cpp:  _old_gen = _old_gen_spec->init(old_rs, rem_set());
genCollectedHeap.cpp:  size_t total_reserved = _young_gen_spec->max_size() + _old_gen_spec->max_size();
genCollectedHeap.cpp:  if (total_reserved < _young_gen_spec->max_size()) {
genCollectedHeap.cpp:    return GenCollectedHeap::heap()->is_in_young(obj);
genCollectedHeap.cpp:  initialize_size_policy(def_new_gen->eden()->capacity(),
genCollectedHeap.cpp:                         _old_gen->capacity(),
genCollectedHeap.cpp:                         def_new_gen->from()->capacity());
genCollectedHeap.cpp:  _young_gen->ref_processor_init();
genCollectedHeap.cpp:  _old_gen->ref_processor_init();
genCollectedHeap.cpp:  return PreGenGCValues(def_new_gen->used(),
genCollectedHeap.cpp:                        def_new_gen->capacity(),
genCollectedHeap.cpp:                        def_new_gen->eden()->used(),
genCollectedHeap.cpp:                        def_new_gen->eden()->capacity(),
genCollectedHeap.cpp:                        def_new_gen->from()->used(),
genCollectedHeap.cpp:                        def_new_gen->from()->capacity(),
genCollectedHeap.cpp:                        old_gen()->used(),
genCollectedHeap.cpp:                        old_gen()->capacity());
genCollectedHeap.cpp:  return _young_gen->capacity() + _old_gen->capacity();
genCollectedHeap.cpp:  return _young_gen->used() + _old_gen->used();
genCollectedHeap.cpp:  _old_gen->save_used_region();
genCollectedHeap.cpp:  _young_gen->save_used_region();
genCollectedHeap.cpp:  return _young_gen->max_capacity() + _old_gen->max_capacity();
genCollectedHeap.cpp:// at the end of a stop-world full GC.
genCollectedHeap.cpp:// . heap memory is tight -- the most recent previous collection
genCollectedHeap.cpp:  size_t young_capacity = _young_gen->capacity_before_gc();
genCollectedHeap.cpp:  if (_old_gen->should_allocate(size, is_tlab)) {
genCollectedHeap.cpp:    result = _old_gen->expand_and_allocate(size, is_tlab);
genCollectedHeap.cpp:    if (_young_gen->should_allocate(size, is_tlab)) {
genCollectedHeap.cpp:      result = _young_gen->expand_and_allocate(size, is_tlab);
genCollectedHeap.cpp:    // First allocation attempt is lock-free.
genCollectedHeap.cpp:    assert(young->supports_inline_contig_alloc(),
genCollectedHeap.cpp:    if (young->should_allocate(size, is_tlab)) {
genCollectedHeap.cpp:      result = young->par_allocate(size, is_tlab);
genCollectedHeap.cpp:        if (!jthr->in_critical()) {
genCollectedHeap.cpp:      // this time, return NULL so that an out-of-memory
genCollectedHeap.cpp:      const bool limit_exceeded = size_policy()->gc_overhead_limit_exceeded();
genCollectedHeap.cpp:      const bool softrefs_clear = soft_ref_policy()->all_soft_refs_clear();
genCollectedHeap.cpp:        size_policy()->set_gc_overhead_limit_exceeded(false);
genCollectedHeap.cpp:  if (_young_gen->should_allocate(size, is_tlab)) {
genCollectedHeap.cpp:    res = _young_gen->allocate(size, is_tlab);
genCollectedHeap.cpp:  if (_old_gen->should_allocate(size, is_tlab)) {
genCollectedHeap.cpp:    res = _old_gen->allocate(size, is_tlab);
genCollectedHeap.cpp:  FormatBuffer<> title("Collect gen: %s", gen->short_name());
genCollectedHeap.cpp:  TraceCollectorStats tcs(gen->counters());
genCollectedHeap.cpp:  TraceMemoryManagerStats tmms(gen->gc_manager(), gc_cause());
genCollectedHeap.cpp:  gen->stat_record()->invocations++;
genCollectedHeap.cpp:  gen->stat_record()->accumulated_time.start();
genCollectedHeap.cpp:  log_trace(gc)("%s invoke=%d size=" SIZE_FORMAT, heap()->is_young_gen(gen) ? "Young" : "Old", gen->stat_record()->invocations, size * HeapWordSize);
genCollectedHeap.cpp:    // scavenge-only collections where it's unnecessary
genCollectedHeap.cpp:    ReferenceProcessor* rp = gen->ref_processor();
genCollectedHeap.cpp:    if (rp->discovery_is_atomic()) {
genCollectedHeap.cpp:      rp->enable_discovery();
genCollectedHeap.cpp:      rp->setup_policy(clear_soft_refs);
genCollectedHeap.cpp:    gen->collect(full, clear_soft_refs, size, is_tlab);
genCollectedHeap.cpp:    if (!rp->enqueuing_is_done()) {
genCollectedHeap.cpp:      rp->disable_discovery();
genCollectedHeap.cpp:      rp->set_enqueuing_is_done(false);
genCollectedHeap.cpp:    rp->verify_no_references_recorded();
genCollectedHeap.cpp:  gen->stat_record()->accumulated_time.stop();
genCollectedHeap.cpp:  assert(my_thread->is_VM_thread() ||
genCollectedHeap.cpp:         my_thread->is_ConcurrentGC_thread(),
genCollectedHeap.cpp:  assert(Heap_lock->is_locked(),
genCollectedHeap.cpp:                          soft_ref_policy()->should_clear_all_soft_refs();
genCollectedHeap.cpp:  bool do_young_collection = !old_collects_young && _young_gen->should_collect(full, size, is_tlab);
genCollectedHeap.cpp:    if (size > 0 && (!is_tlab || _young_gen->supports_tlab_allocation()) &&
genCollectedHeap.cpp:        size * HeapWordSize <= _young_gen->unsafe_max_alloc_nogc()) {
genCollectedHeap.cpp:      _young_gen->compute_new_size();
genCollectedHeap.cpp:    _old_gen->compute_new_size();
genCollectedHeap.cpp:    _young_gen->compute_new_size();
genCollectedHeap.cpp:  return max_gen == OldGen && _old_gen->should_collect(full, size, is_tlab);
genCollectedHeap.cpp:    // with from-space allocation criteria modified and
genCollectedHeap.cpp:  assert(!soft_ref_policy()->should_clear_all_soft_refs(),
genCollectedHeap.cpp:    assert(!GenCollectedHeap::heap()->is_in_partial_collection(*p),
genCollectedHeap.cpp:  if (_process_strong_tasks->try_claim_task(GCH_PS_ClassLoaderDataGraph_oops_do)) {
genCollectedHeap.cpp:  bool is_par = scope->n_threads() > 1;
genCollectedHeap.cpp:  if (_process_strong_tasks->try_claim_task(GCH_PS_ObjectSynchronizer_oops_do)) {
genCollectedHeap.cpp:  if (UseAOT && _process_strong_tasks->try_claim_task(GCH_PS_aot_oops_do)) {
genCollectedHeap.cpp:  if (_process_strong_tasks->try_claim_task(GCH_PS_OopStorageSet_oops_do)) {
genCollectedHeap.cpp:  if (_process_strong_tasks->try_claim_task(GCH_PS_CodeCache_oops_do)) {
genCollectedHeap.cpp:      // CMSCollector uses this to do intermediate-strength collections.
genCollectedHeap.cpp:  _process_strong_tasks->all_tasks_completed(scope->n_threads());
genCollectedHeap.cpp:  _young_gen->ref_processor()->weak_oops_do(root_closure);
genCollectedHeap.cpp:  _old_gen->ref_processor()->weak_oops_do(root_closure);
genCollectedHeap.cpp:  return _young_gen->no_allocs_since_save_marks() &&
genCollectedHeap.cpp:         _old_gen->no_allocs_since_save_marks();
genCollectedHeap.cpp:  return _young_gen->supports_inline_contig_alloc();
genCollectedHeap.cpp:  return _young_gen->top_addr();
genCollectedHeap.cpp:  return _young_gen->end_addr();
genCollectedHeap.cpp:    // Stop-the-world full collection.
genCollectedHeap.cpp:    // Stop-the-world full collection.
genCollectedHeap.cpp:  assert(!Heap_lock->owned_by_self(), "this thread should not own the Heap_lock");
genCollectedHeap.cpp:  assert(Heap_lock->owned_by_self(), "this thread should own the Heap_lock");
genCollectedHeap.cpp:  bool result = cast_from_oop<HeapWord*>(p) < _old_gen->reserved().start();
genCollectedHeap.cpp:  assert(result == _young_gen->is_in_reserved(p),
genCollectedHeap.cpp:         "incorrect test - result=%d, p=" INTPTR_FORMAT, result, p2i((void*)p));
genCollectedHeap.cpp:  return _young_gen->is_in(p) || _old_gen->is_in(p);
genCollectedHeap.cpp:    "Does not work if address is non-null and outside of the heap");
genCollectedHeap.cpp:  return p < _young_gen->reserved().end() && p != NULL;
genCollectedHeap.cpp:  _young_gen->oop_iterate(cl);
genCollectedHeap.cpp:  _old_gen->oop_iterate(cl);
genCollectedHeap.cpp:  _young_gen->object_iterate(cl);
genCollectedHeap.cpp:  _old_gen->object_iterate(cl);
genCollectedHeap.cpp:  Space* res = _young_gen->space_containing(addr);
genCollectedHeap.cpp:  res = _old_gen->space_containing(addr);
genCollectedHeap.cpp:  if (_young_gen->is_in_reserved(addr)) {
genCollectedHeap.cpp:    assert(_young_gen->is_in(addr), "addr should be in allocated part of generation");
genCollectedHeap.cpp:    return _young_gen->block_start(addr);
genCollectedHeap.cpp:  assert(_old_gen->is_in_reserved(addr), "Some generation should contain the address");
genCollectedHeap.cpp:  assert(_old_gen->is_in(addr), "addr should be in allocated part of generation");
genCollectedHeap.cpp:  return _old_gen->block_start(addr);
genCollectedHeap.cpp:  if (_young_gen->is_in_reserved(addr)) {
genCollectedHeap.cpp:    return _young_gen->block_is_obj(addr);
genCollectedHeap.cpp:  assert(_old_gen->is_in_reserved(addr), "Some generation should contain the address");
genCollectedHeap.cpp:  return _old_gen->block_is_obj(addr);
genCollectedHeap.cpp:  assert(!_old_gen->supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
genCollectedHeap.cpp:  return _young_gen->supports_tlab_allocation();
genCollectedHeap.cpp:  assert(!_old_gen->supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
genCollectedHeap.cpp:  if (_young_gen->supports_tlab_allocation()) {
genCollectedHeap.cpp:    return _young_gen->tlab_capacity();
genCollectedHeap.cpp:  assert(!_old_gen->supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
genCollectedHeap.cpp:  if (_young_gen->supports_tlab_allocation()) {
genCollectedHeap.cpp:    return _young_gen->tlab_used();
genCollectedHeap.cpp:  assert(!_old_gen->supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
genCollectedHeap.cpp:  if (_young_gen->supports_tlab_allocation()) {
genCollectedHeap.cpp:    return _young_gen->unsafe_max_tlab_alloc();
genCollectedHeap.cpp:// Requires "*prev_ptr" to be non-NULL.  Deletes and a block of minimal size
genCollectedHeap.cpp:    if (first || cur->num_words < min_size) {
genCollectedHeap.cpp:      min_size     = smallest->num_words;
genCollectedHeap.cpp:    prev_ptr = &cur->next;
genCollectedHeap.cpp:    cur     =  cur->next;
genCollectedHeap.cpp:  *smallest_ptr = smallest->next;
genCollectedHeap.cpp:    smallest->next  = sorted;
genCollectedHeap.cpp:  _young_gen->contribute_scratch(res, requestor, max_alloc_words);
genCollectedHeap.cpp:  _old_gen->contribute_scratch(res, requestor, max_alloc_words);
genCollectedHeap.cpp:  _young_gen->reset_scratch();
genCollectedHeap.cpp:  _old_gen->reset_scratch();
genCollectedHeap.cpp:    gen->prepare_for_verify();
genCollectedHeap.cpp:    cl->do_generation(_old_gen);
genCollectedHeap.cpp:    cl->do_generation(_young_gen);
genCollectedHeap.cpp:    cl->do_generation(_young_gen);
genCollectedHeap.cpp:    cl->do_generation(_old_gen);
genCollectedHeap.cpp:  return _young_gen->is_maximal_no_gc() && _old_gen->is_maximal_no_gc();
genCollectedHeap.cpp:  _young_gen->save_marks();
genCollectedHeap.cpp:  _old_gen->save_marks();
genCollectedHeap.cpp:  _old_gen->prepare_for_compaction(&cp);
genCollectedHeap.cpp:  _young_gen->prepare_for_compaction(&cp);
genCollectedHeap.cpp:  log_debug(gc, verify)("%s", _old_gen->name());
genCollectedHeap.cpp:  _old_gen->verify();
genCollectedHeap.cpp:  log_debug(gc, verify)("%s", _old_gen->name());
genCollectedHeap.cpp:  _young_gen->verify();
genCollectedHeap.cpp:  rem_set()->verify();
genCollectedHeap.cpp:    _young_gen->print_on(st);
genCollectedHeap.cpp:    _old_gen->print_on(st);
genCollectedHeap.cpp:    _young_gen->print_summary_info_on(&lsh);
genCollectedHeap.cpp:    _old_gen->print_summary_info_on(&lsh);
genCollectedHeap.cpp:                     HEAP_CHANGE_FORMAT_ARGS(def_new_gen->short_name(),
genCollectedHeap.cpp:                                             def_new_gen->used(),
genCollectedHeap.cpp:                                             def_new_gen->capacity()),
genCollectedHeap.cpp:                                             def_new_gen->eden()->used(),
genCollectedHeap.cpp:                                             def_new_gen->eden()->capacity()),
genCollectedHeap.cpp:                                             def_new_gen->from()->used(),
genCollectedHeap.cpp:                                             def_new_gen->from()->capacity()));
genCollectedHeap.cpp:                     HEAP_CHANGE_FORMAT_ARGS(old_gen()->short_name(),
genCollectedHeap.cpp:                                             old_gen()->used(),
genCollectedHeap.cpp:                                             old_gen()->capacity()));
genCollectedHeap.cpp:    gen->gc_prologue(_full);
genCollectedHeap.cpp:  generation_iterate(&blk, false);  // not old-to-young.
genCollectedHeap.cpp:    gen->gc_epilogue(_full);
genCollectedHeap.cpp:  size_t actual_gap = pointer_delta((HeapWord*) (max_uintx-3), *(end_addr()));
genCollectedHeap.cpp:  generation_iterate(&blk, false);  // not old-to-young.
genCollectedHeap.cpp:    gen->record_spaces_top();
genCollectedHeap.cpp:    generation_iterate(&blk, false);  // not old-to-young.
genCollectedHeap.cpp:    gen->ensure_parsability();
genCollectedHeap.cpp:  assert(obj_size == (size_t)obj->size(), "bad obj_size passed in");
genCollectedHeap.cpp:  result = old_gen->expand_and_allocate(obj_size, false);
genCollectedHeap.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
genCollectedHeap.hpp:  // (gen-specific) roots processing.
genCollectedHeap.hpp:  // free-list-based heap (or subheap), as long as the two kinds are
genCollectedHeap.hpp:  // non-object.
genCollectedHeap.hpp:  // the block is an object. Assumes (and verifies in non-product
genCollectedHeap.hpp:  // Update above counter, as appropriate, at the end of a stop-world GC cycle
genCollectedHeap.hpp:    _old_gen->update_gc_stats(current_generation, full);
genCollectedHeap.hpp:           (consult_young && !_young_gen->collection_attempt_is_safe());
genCollectedHeap.hpp:  // Otherwise, try expand-and-allocate for obj in both the young and old
genCollectedHeap.hpp:  // For use by mark-sweep.  As implemented, mark-sweep-compact is global
generationCounters.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
generationCounters.cpp:             min_capacity, max_capacity, v->committed_size());
generationCounters.cpp:  _current_size->set_value(_virtual_space->committed_size());
generationCounters.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
generation.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
generation.cpp:  if (gch->is_young_gen(this)) {
generation.cpp:    return gch->young_gen_spec()->init_size();
generation.cpp:  return gch->old_gen_spec()->init_size();
generation.cpp:// generations needing multi-threaded refs processing or discovery override this method.
generation.cpp:  st->print(" %-20s", name());
generation.cpp:  st->print(" total " SIZE_FORMAT "K, used " SIZE_FORMAT "K",
generation.cpp:  st->print_cr(" [" INTPTR_FORMAT ", " INTPTR_FORMAT ", " INTPTR_FORMAT ")",
generation.cpp:  double time = sr->accumulated_time.seconds();
generation.cpp:  st->print_cr("Accumulated %s generation GC time %3.7f secs, "
generation.cpp:               GenCollectedHeap::heap()->is_young_gen(this) ? "young" : "old" ,
generation.cpp:               sr->invocations,
generation.cpp:               sr->invocations > 0 ? time / sr->invocations : 0.0);
generation.cpp:      if (s->is_in_reserved(_p)) sp = s;
generation.cpp:      if (s->is_in(_p)) sp = s;
generation.cpp:  ((Generation*)this)->space_iterate(&blk);
generation.cpp:  if (GenCollectedHeap::heap()->is_young_gen(this)) {
generation.cpp:    old_avail = GenCollectedHeap::heap()->old_gen()->contiguous_available();
generation.cpp:  assert(obj_size == (size_t)obj->size(), "bad obj_size passed in");
generation.cpp:  if (GenCollectedHeap::heap()->promotion_should_fail()) {
generation.cpp:    return gch->handle_failed_promotion(this, obj, obj_size);
generation.cpp:  ((Generation*)this)->space_iterate(&blk);
generation.cpp:    if (_start == NULL && s->is_in_reserved(_p)) {
generation.cpp:      _start = s->block_start(_p);
generation.cpp:  ((Generation*)this)->space_iterate(&blk);
generation.cpp:    if (size == 0 && s->is_in_reserved(_p)) {
generation.cpp:      size = s->block_size(_p);
generation.cpp:  ((Generation*)this)->space_iterate(&blk);
generation.cpp:    if (!is_obj && s->is_in_reserved(_p)) {
generation.cpp:      is_obj |= s->block_is_obj(_p);
generation.cpp:  ((Generation*)this)->space_iterate(&blk);
generation.cpp:    s->oop_iterate(_cl);
generation.cpp:    s->object_iterate(_cl);
generation.cpp:    space->prepare_for_compaction(cp);
generation.cpp:    space = space->next_compaction_space();
generation.cpp:    sp->adjust_pointers();
generation.cpp:    sp->compact();
generation.cpp:    sp = sp->next_compaction_space();
generation.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
generation.hpp:// A Generation models a heap area for similarly-aged objects.
generation.hpp:// Generation                      - abstract base class
generation.hpp:// - DefNewGeneration              - allocation area (copy collected)
generation.hpp:// - CardGeneration                 - abstract class adding offset array behavior
generation.hpp://   - TenuredGeneration             - tenured (old object) space (markSweepCompact)
generation.hpp:  HeapWord scratch_space[1];  // Actually, of size "num_words-2" (assuming
generation.hpp:                              // first two fields are word-sized.)
generation.hpp:    // Generations are GenGrain-aligned and have size that are multiples of
generation.hpp:    // (we expect its low byte to be zero - see implementation of post_barrier)
generation.hpp:  // For a non-young generation, this interface can be used to inform a
generation.hpp:  // Typically used to enable diagnostic output for post-mortem analysis,
generation.hpp:  // Iteration - do not use for time critical operations
generation.hpp:  // often, they would greatly increase the frequency of young-gen
generation.hpp:    size_t overflow_limit = (size_t)1 << (BitsPerSize_t - LogHeapWordSize);
generation.hpp:  // A generation that supports this allocation style must use lock-free
generation.hpp:  // Thread-local allocation buffers
generation.hpp:  // The "obj_size" argument is just obj->size(), passed along so the caller can
generation.hpp:  // slow path (the only fast-path place to allocate is DefNew, which
generation.hpp:  // Some generations may need to be "fixed-up" after some allocation
generation.hpp:  // no data recording is expected by the provider. The data-recorder is
generation.hpp:  // expected to be GC worker thread-local, with the worker index
generation.hpp:  // beginning allocation point post-collection, which might allow some later
generation.hpp:  // Iterate over all the ref-containing fields of all objects in the
generation.hpp:  // non-object.
generationSpec.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
generationSpec.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
generationSpec.hpp:// some generation-specific behavior.  This is done here rather than as a
genMemoryPools.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
genMemoryPools.cpp:  CollectedMemoryPool(name, space->capacity(), max_size,
genMemoryPools.cpp:  return space()->used();
genMemoryPools.cpp:  size_t committed = _space->capacity();
genMemoryPools.cpp:  CollectedMemoryPool(name, young_gen->from()->capacity(), max_size,
genMemoryPools.cpp:  return _young_gen->from()->used();
genMemoryPools.cpp:  return _young_gen->from()->capacity();
genMemoryPools.cpp:  CollectedMemoryPool(name, gen->capacity(), gen->max_capacity(),
genMemoryPools.cpp:  return _gen->used();
genMemoryPools.cpp:  size_t committed = _gen->capacity();
genMemoryPools.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
genOopClosures.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
genOopClosures.hpp:// - Derived: The derived type provides necessary barrier
genOopClosures.hpp:    OopIterateClosure(cl->ref_discoverer()), _boundary(boundary),
genOopClosures.hpp:  virtual bool do_metadata()            { assert(!_cl->do_metadata(), "assumption broken, must change to 'return _cl->do_metadata()'"); return false; }
genOopClosures.hpp://  -- weak references are processed all at once,
genOopClosures.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
genOopClosures.inline.hpp:    BasicOopIterateClosure(g->ref_processor()),
genOopClosures.inline.hpp:    _young_gen_end(g->reserved().end()) {}
genOopClosures.inline.hpp:      assert(!_young_gen->to()->is_in_reserved(obj), "Scanning field twice?");
genOopClosures.inline.hpp:      oop new_obj = obj->is_forwarded() ? obj->forwardee()
genOopClosures.inline.hpp:                                        : _young_gen->copy_to_survivor_space(obj);
genOopClosures.inline.hpp:      static_cast<Derived*>(this)->barrier(p);
genOopClosures.inline.hpp:    _old_gen_start(old_gen->reserved().start()),
genOopClosures.inline.hpp:    _rs(GenCollectedHeap::heap()->rem_set()) {}
genOopClosures.inline.hpp:  assert(_old_gen->is_in_reserved(p), "expected ref in generation");
genOopClosures.inline.hpp:  assert(!CompressedOops::is_null(heap_oop), "expected non-null oop");
genOopClosures.inline.hpp:    _rs->inline_write_ref_field_gc(p, obj);
genOopClosures.inline.hpp:  if (_scanned_cld != NULL && !_scanned_cld->has_modified_oops()) {
genOopClosures.inline.hpp:    _scanned_cld->record_modified_oops();
genOopClosures.inline.hpp:      _cl->do_oop(p);
genOopClosures.inline.hpp:  // that to-space doesn't already contain this object
genOopClosures.inline.hpp:  if (cast_from_oop<HeapWord*>(obj) < _boundary && !_g->to()->is_in_reserved(obj)) {
genOopClosures.inline.hpp:    oop new_obj = obj->is_forwarded() ? obj->forwardee()
genOopClosures.inline.hpp:                                      : _g->copy_to_survivor_space(obj);
hSpaceCounters.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
hSpaceCounters.cpp:  _capacity->set_value(v);
hSpaceCounters.cpp:  _used->set_value(v);
hSpaceCounters.cpp:    return _used->get_value();
hSpaceCounters.cpp:    return _used->get_value();
hSpaceCounters.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
isGCActiveMark.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
isGCActiveMark.cpp:  assert(!heap->is_gc_active(), "Not reentrant");
isGCActiveMark.cpp:  heap->_is_gc_active = true;
isGCActiveMark.cpp:  assert(heap->is_gc_active(), "Sanity");
isGCActiveMark.cpp:  heap->_is_gc_active = false;
isGCActiveMark.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
jvmFlagConstraintsGC.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
jvmFlagConstraintsGC.cpp:  size_t aligned_max = ((max_uintx - alignment) & ~(alignment-1));
jvmFlagConstraintsGC.cpp:  if (UseCompressedOops && FLAG_IS_ERGO(MaxHeapSize) && (value > (max_uintx - MaxHeapSize))) {
jvmFlagConstraintsGC.cpp:    size_t refill_waste_limit = Thread::current()->tlab().refill_waste_limit();
jvmFlagConstraintsGC.cpp:    if (refill_waste_limit > (max_uintx - value)) {
jvmFlagConstraintsGC.cpp:                          value, (max_uintx - refill_waste_limit));
jvmFlagConstraintsGC.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
locationPrinter.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
locationPrinter.cpp:  if (!Universe::heap()->is_in(obj)) {
locationPrinter.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
locationPrinter.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
locationPrinter.inline.hpp:  HeapWord* p = CollectedHeapT::heap()->block_start(addr);
locationPrinter.inline.hpp:  if (p != NULL && CollectedHeapT::heap()->block_is_obj(p)) {
locationPrinter.inline.hpp:  if (CollectedHeapT::heap()->is_in(addr)) {
locationPrinter.inline.hpp:        st->print(INTPTR_FORMAT " is an oop: ", p2i(addr));
locationPrinter.inline.hpp:        st->print(INTPTR_FORMAT " is pointing into object: " , p2i(addr));
locationPrinter.inline.hpp:      o->print_on(st);
locationPrinter.inline.hpp:  } else if (CollectedHeapT::heap()->is_in_reserved(addr)) {
locationPrinter.inline.hpp:    st->print_cr(INTPTR_FORMAT " is an unallocated location in the heap", p2i(addr));
locationPrinter.inline.hpp:      st->print(UINT32_FORMAT " is a compressed pointer to object: ", narrow_oop);
locationPrinter.inline.hpp:      o->print_on(st);
markBitMap.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
markBitMap.cpp:  assert(Universe::heap()->is_in(addr),
markBitMap.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
markBitMap.hpp:  // "addr", and before "limit", if "limit" is non-NULL.  If there is no
markBitMap.hpp:  // such bit, returns "limit" if that is non-NULL, or else "endWord()".
markBitMap.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
memAllocator.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
memAllocator.cpp:  if (!THREAD->in_retryable_allocation()) {
memAllocator.cpp:    // -XX:+HeapDumpOnOutOfMemoryError and -XX:OnOutOfMemoryError support
memAllocator.cpp:  assert(!Universe::heap()->is_gc_active(), "Allocation during gc not allowed");
memAllocator.cpp:             "Found badHeapWordValue in post-allocation check");
memAllocator.cpp:  assert(!_thread->has_pending_exception(),
memAllocator.cpp:  _thread->check_for_valid_safepoint_state();
memAllocator.cpp:  // support for JVMTI VMObjectAlloc event (no-op if not enabled)
memAllocator.cpp:    // Sample if it's a non-TLAB allocation, or a TLAB allocation that either refills the TLAB
memAllocator.cpp:    ThreadLocalAllocBuffer& tlab = _thread->tlab();
memAllocator.cpp:    _thread->heap_sampler().check_for_sampling(obj_h(), size_in_bytes, bytes_since_last);
memAllocator.cpp:    _thread->tlab().set_sample_end(bytes_since_last != 0);
memAllocator.cpp:  // support low memory notifications (no-op if not enabled)
memAllocator.cpp:    AllocTracer::send_allocation_outside_tlab(obj()->klass(), mem, size_in_bytes, _thread);
memAllocator.cpp:    AllocTracer::send_allocation_in_new_tlab(obj()->klass(), mem, _allocated_tlab_size * HeapWordSize,
memAllocator.cpp:    // support for Dtrace object alloc event (no-op most of the time)
memAllocator.cpp:    Klass* klass = obj()->klass();
memAllocator.cpp:    if (klass != NULL && klass->name() != NULL) {
memAllocator.cpp:  HeapWord* mem = Universe::heap()->mem_allocate(_word_size, &allocation._overhead_limit_exceeded);
memAllocator.cpp:  NOT_PRODUCT(Universe::heap()->check_for_non_bad_heap_word_value(mem, _word_size));
memAllocator.cpp:  _thread->incr_allocated_bytes(size_in_bytes);
memAllocator.cpp:  HeapWord* mem = _thread->tlab().allocate(_word_size);
memAllocator.cpp:  ThreadLocalAllocBuffer& tlab = _thread->tlab();
memAllocator.cpp:  mem = Universe::heap()->allocate_new_tlab(min_tlab_size, new_tlab_size, &allocation._allocated_tlab_size);
memAllocator.cpp:    Copy::fill_to_words(mem + hdr_size, allocation._allocated_tlab_size - hdr_size, badHeapWordVal);
memAllocator.cpp:  Copy::fill_to_aligned_words(mem + hs, _word_size - hs);
memAllocator.cpp:    oopDesc::set_mark_raw(mem, _klass->prototype_header());
memAllocator.cpp:  // object zeroing are visible before setting the klass non-NULL, for
memAllocator.cpp:  const size_t hs = arrayOopDesc::header_size(array_klass->element_type());
memAllocator.cpp:  return MemRegion(cast_from_oop<HeapWord*>(obj) + hs, _word_size - hs);
memAllocator.cpp:  // non-NULL klass field indicates that the object is parsable by
memAllocator.cpp:  assert(_length >= 0, "length should be non-negative");
memAllocator.cpp:  // non-NULL _klass field indicates that the object is parsable by
memAllocator.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
memAllocator.hpp:  // Allocate from the current thread's TLAB, with broken-out slow path.
memset_with_concurrent_readers.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
modRefBarrierSetAssembler.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
modRefBarrierSet.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
modRefBarrierSet.hpp:  // at the address "start", which may not necessarily be HeapWord-aligned
modRefBarrierSet.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
modRefBarrierSet.inline.hpp:  // logging barrier of narrow oop granularity, like the pre-barrier for G1
modRefBarrierSet.inline.hpp:  bs->template write_ref_field_pre<decorators>(addr);
modRefBarrierSet.inline.hpp:  bs->template write_ref_field_post<decorators>(addr, value);
modRefBarrierSet.inline.hpp:  bs->template write_ref_field_pre<decorators>(addr);
modRefBarrierSet.inline.hpp:    bs->template write_ref_field_post<decorators>(addr, new_value);
modRefBarrierSet.inline.hpp:  bs->template write_ref_field_pre<decorators>(addr);
modRefBarrierSet.inline.hpp:  bs->template write_ref_field_post<decorators>(addr, new_value);
modRefBarrierSet.inline.hpp:    bs->write_ref_array_pre(dst_raw, length,
modRefBarrierSet.inline.hpp:    bs->write_ref_array((HeapWord*)dst_raw, length);
modRefBarrierSet.inline.hpp:    Klass* bound = objArrayOop(dst_obj)->element_klass();
modRefBarrierSet.inline.hpp:        bs->template write_ref_field_pre<decorators>(p);
modRefBarrierSet.inline.hpp:        bs->write_ref_array((HeapWord*)dst_raw, pd);
modRefBarrierSet.inline.hpp:    bs->write_ref_array((HeapWord*)dst_raw, length);
modRefBarrierSet.inline.hpp:  bs->write_region(MemRegion((HeapWord*)(void*)dst, size));
objectCountEventSender.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
objectCountEventSender.cpp:  Klass* klass = entry->klass();
objectCountEventSender.cpp:  jlong count = entry->count();
objectCountEventSender.cpp:  julong total_size = entry->words() * BytesPerWord;
objectCountEventSender.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorage.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorage.cpp:  assert(_head == NULL, "deleting non-empty block list");
oopStorage.cpp:  assert(_tail == NULL, "deleting non-empty block list");
oopStorage.cpp:    old->allocation_list_entry()._prev = &block;
oopStorage.cpp:    old->allocation_list_entry()._next = &block;
oopStorage.cpp:    next_blk->allocation_list_entry()._prev = NULL;
oopStorage.cpp:    prev_blk->allocation_list_entry()._next = NULL;
oopStorage.cpp:    next_blk->allocation_list_entry()._prev = prev_blk;
oopStorage.cpp:    prev_blk->allocation_list_entry()._next = next_blk;
oopStorage.cpp:  ba->~ActiveArray();
oopStorage.cpp:  assert(new_value >= 1, "negative refcount %d", new_value - 1);
oopStorage.cpp:    block->set_active_index(index);
oopStorage.cpp:  size_t index = block->active_index();
oopStorage.cpp:  size_t last_index = _block_count - 1;
oopStorage.cpp:  last_block->set_active_index(index);
oopStorage.cpp:  size_t count = from->_block_count;
oopStorage.cpp:  Block* const* from_ptr = from->block_ptr(0);
oopStorage.cpp:    assert(block->active_index() == i, "invariant");
oopStorage.cpp:  // might help catch bugs.  Volatile to prevent dead-store elimination.
oopStorage.cpp:  return sizeof(Block) + block_alignment - sizeof(void*);
oopStorage.cpp:  STATIC_ASSERT(sizeof(intptr_t) == sizeof(block->_active_index));
oopStorage.cpp:  return SafeFetchN((intptr_t*)&block->_active_index, 0);
oopStorage.cpp:  return static_cast<unsigned>(ptr - get_pointer(0));
oopStorage.cpp:  // Const-ness of ptr is not related to const-ness of containing block.
oopStorage.cpp:  // Blocks are allocated section-aligned, so get the containing section.
oopStorage.cpp:  // so the block starts section_count-1 sections earlier.
oopStorage.cpp:  oop* section = section_start - (section_size * (section_count - 1));
oopStorage.cpp:    if (SafeFetchN(&candidate->_owner_address, 0) == owner_addr) {
oopStorage.cpp:// blocks owned by a storage object.  This is a doubly-linked list, linked
oopStorage.cpp:// release() is performed lock-free. (Note: This means it can't notify the
oopStorage.cpp:// service thread of pending cleanup work.  It must be lock-free because
oopStorage.cpp:// locking the _allocation_mutex.  To keep the release() operation lock-free,
oopStorage.cpp:// lock-free push of the block onto the _deferred_updates list.  Entries on
oopStorage.cpp:  assert(!block->is_full(), "invariant");
oopStorage.cpp:  if (block->is_empty()) {
oopStorage.cpp:  oop* result = block->allocate();
oopStorage.cpp:  assert(!block->is_empty(), "postcondition");
oopStorage.cpp:  if (block->is_full()) {
oopStorage.cpp:  if (!_active_array->push(block)) {
oopStorage.cpp:      guarantee(_active_array->push(block), "push failed after expansion");
oopStorage.cpp:  // allocate from non-empty blocks, to allow empty blocks to be
oopStorage.cpp:  size_t new_size = 2 * old_array->size();
oopStorage.cpp:  new_array->copy_from(old_array);
oopStorage.cpp:  new_array->increment_refcount();
oopStorage.cpp:  result->increment_refcount();
oopStorage.cpp:  if (array->decrement_refcount()) {
oopStorage.cpp:    _active_array(storage->obtain_active_array())
oopStorage.cpp:    _storage->relinquish_block_array(_active_array);
oopStorage.cpp:      ls.print_cr("%s: block not full " PTR_FORMAT, owner->name(), p2i(block));
oopStorage.cpp:      ls.print_cr("%s: block empty " PTR_FORMAT, owner->name(), p2i(block));
oopStorage.cpp:    // list, by setting the link to non-NULL by self-looping.  If this fails,
oopStorage.cpp:      // Successfully claimed.  Push, with self-loop for end-of-list.
oopStorage.cpp:      Block* head = owner->_deferred_updates;
oopStorage.cpp:        Block* fetched = Atomic::cmpxchg(&owner->_deferred_updates, head, this);
oopStorage.cpp:      // Only request cleanup for to-empty transitions, not for from-full.
oopStorage.cpp:      // There isn't any rush to process from-full transitions.  Allocation
oopStorage.cpp:      // if there are any pending to-empty transitions.
oopStorage.cpp:        owner->record_needs_cleanup();
oopStorage.cpp:                                    owner->name(), p2i(this));
oopStorage.cpp:    Block* tail = block->deferred_updates_next();
oopStorage.cpp:    if (block == tail) tail = NULL; // Handle self-loop end marker.
oopStorage.cpp:  block->set_deferred_updates_next(NULL); // Clear tail after updating head.
oopStorage.cpp:  uintx allocated = block->allocated_bitmask();
oopStorage.cpp:  block->release_entries(block->bitmask_for_entry(ptr), this);
oopStorage.cpp:      if (!block->contains(entry)) break;
oopStorage.cpp:      uintx entry_bitmask = block->bitmask_for_entry(entry);
oopStorage.cpp:    block->release_entries(releasing, this);
oopStorage.cpp:  _active_mutex(make_oopstorage_mutex(name, "active", Mutex::oopstorage - 1)),
oopStorage.cpp:  _active_array->increment_refcount();
oopStorage.cpp:  assert(_active_mutex->rank() < _allocation_mutex->rank(),
oopStorage.cpp:  assert(Service_lock->rank() < _active_mutex->rank(),
oopStorage.cpp:  assert(_active_mutex->_safepoint_check_required == Mutex::_safepoint_check_never,
oopStorage.cpp:  assert(_allocation_mutex->_safepoint_check_required == Mutex::_safepoint_check_never,
oopStorage.cpp:  assert(block.is_empty(), "discarding non-empty block");
oopStorage.cpp:    _deferred_updates = block->deferred_updates_next();
oopStorage.cpp:    block->set_deferred_updates_next(NULL);
oopStorage.cpp:  bool unreferenced = _active_array->decrement_refcount();
oopStorage.cpp:  for (size_t i = _active_array->block_count(); 0 < i; ) {
oopStorage.cpp:    block = _active_array->at(--i);
oopStorage.cpp:      // Be safepoint-polite while looping.
oopStorage.cpp:      if ((block == NULL) || !block->is_empty()) {
oopStorage.cpp:      } else if (!block->is_safe_to_delete()) {
oopStorage.cpp:        // but don't re-notify, to avoid useless spinning of the
oopStorage.cpp:        _active_array->remove(block);
oopStorage.cpp:      // Be safepoint-polite while deleting and looping.
oopStorage.cpp:    if ((index < _active_array->block_count()) &&
oopStorage.cpp:        (block == _active_array->at(index)) &&
oopStorage.cpp:        block->contains(ptr)) {
oopStorage.cpp:      if ((block->allocated_bitmask() & block->bitmask_for_entry(ptr)) != 0) {
oopStorage.cpp:  _active_array(_storage->obtain_active_array()),
oopStorage.cpp:  _block_count = _active_array->block_count_acquire();
oopStorage.cpp:  _storage->relinquish_block_array(_active_array);
oopStorage.cpp:  update_concurrent_iteration_count(-1);
oopStorage.cpp:    const_cast<OopStorage*>(_storage)->record_needs_cleanup();
oopStorage.cpp:    MutexLocker ml(_storage->_active_mutex, Mutex::_no_safepoint_check_flag);
oopStorage.cpp:    _storage->_concurrent_iteration_count += value;
oopStorage.cpp:    assert(_storage->_concurrent_iteration_count >= 0, "invariant");
oopStorage.cpp:  data->_processed += data->_segment_end - data->_segment_start;
oopStorage.cpp:  size_t remaining = _block_count - start;
oopStorage.cpp:  start = end - step;
oopStorage.cpp:    data->_segment_start = start;
oopStorage.cpp:    data->_segment_end = end;
oopStorage.cpp:           _storage->name(), _block_count, data->_processed,
oopStorage.cpp:           percent_of(data->_processed, _block_count));
oopStorage.cpp:  _storage->report_num_dead(Atomic::load(&_num_dead));
oopStorage.cpp:  size_t blocks = _active_array->block_count();
oopStorage.cpp:  st->print("%s: " SIZE_FORMAT " entries in " SIZE_FORMAT " blocks (%.F%%), " SIZE_FORMAT " bytes",
oopStorage.cpp:    st->print(", concurrent iteration active");
oopStorage.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorage.hpp:// OopStorage supports management of off-heap references to objects allocated
oopStorage.hpp:// (3) iteration by other, non-GC, tools (only at safepoints).
oopStorage.hpp:// A goal of OopStorage is to make these interactions thread-safe, while
oopStorage.hpp:  // on whether the associated storage is const or non-const, respectively.
oopStorage.hpp:  // an adaptation layer allowing the use of existing is-alive closures and
oopStorage.hpp:  // the associated storage is const or non-const, respectively.  Then
oopStorage.hpp:  // - closure->do_oop(p) must be a valid expression whose value is ignored.
oopStorage.hpp:  // - is_alive->do_object_b(*p) must be a valid expression whose value is
oopStorage.hpp:  // invoked for p.  If is_alive->do_object_b(*p) is false, then closure will
oopStorage.hpp:  // blocks.  Stops deleting if there is an in-progress concurrent
oopStorage.hpp:  // That class is defined as part of the unit-test. It "exports" the needed
oopStorage.hpp:  class Block;                  // Fixed-size array of oops, plus bookkeeping.
oopStorage.hpp:  // Doubly-linked list of Blocks.
oopStorage.hpp:  // Wrapper for OopClosure-style function, so it can be used with
oopStorage.hpp:  // iterate.  Assume p is of type oop*.  Then cl->do_oop(p) must be a
oopStorage.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorage.inline.hpp:// Array of all active blocks.  Refcounted for lock-free reclaim of
oopStorage.inline.hpp:// Fixed-sized array of oops, plus bookkeeping data.
oopStorage.inline.hpp:// Non-full blocks are in the storage's _allocation_list, linked through the
oopStorage.inline.hpp:  // _data must be the first non-static data member, for alignment.
oopStorage.inline.hpp:    _cl->do_oop(ptr);
oopStorage.inline.hpp:      if (_is_alive->do_object_b(v)) {
oopStorage.inline.hpp:// Provide const or non-const iteration, depending on whether BlockPtr
oopStorage.inline.hpp:  uintx bitmask = block->allocated_bitmask();
oopStorage.inline.hpp:    bitmask ^= block->bitmask_for_index(index);
oopStorage.inline.hpp:    if (!f(block->get_pointer(index))) {
oopStorage.inline.hpp:// Provide const or non-const iteration, depending on whether Storage is
oopStorage.inline.hpp:  // Propagate const/non-const iteration to the block layer, by using
oopStorage.inline.hpp:  // const or non-const blocks as corresponding to Storage.
oopStorage.inline.hpp:  ActiveArray* blocks = storage->_active_array;
oopStorage.inline.hpp:  size_t limit = blocks->block_count();
oopStorage.inline.hpp:    BlockPtr block = blocks->at(i);
oopStorage.inline.hpp:    if (!block->iterate(f)) {
oopStorageParState.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorageParState.hpp:// _concurrent_iteration_count is non-zero.  The counter check and the dependent
oopStorageParState.hpp://   cl->do_oop(p) must be a valid expression whose value is ignored.
oopStorageParState.hpp:// provided when concurrent because any pre-filtering behavior by the
oopStorageParState.hpp:// pre-filtering being applied (successfully or not) to objects that
oopStorageParState.hpp://   the use of is-alive closures and OopClosures for iteration.
oopStorageParState.hpp://   - cl->do_oop(p) must be a valid expression whose value is ignored.
oopStorageParState.hpp://   - is_alive->do_object_b(*p) must be a valid expression whose value
oopStorageParState.hpp://   If is_alive->do_object_b(*p) is false, then cl will not be
oopStorageParState.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorageParState.inline.hpp:      BlockPtr block = _active_array->at(i);
oopStorageParState.inline.hpp:      block->iterate(atf_f);
oopStorageParState.inline.hpp:  this->iterate(oop_fn(cl));
oopStorageParState.inline.hpp:  this->iterate(oop_fn(cl));
oopStorageParState.inline.hpp:  this->iterate(skip_null_fn(oop_fn(cl)));
oopStorageParState.inline.hpp:  this->iterate(if_alive_fn(is_alive, oop_fn(cl)));
oopStorageSet.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorageSet.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorageSet.hpp:  OopStorage* operator->() const {
oopStorageSet.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorageSet.inline.hpp:    (*it)->oops_do(cl);
oopStorageSetParState.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorageSetParState.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
oopStorageSetParState.inline.hpp:    _par_states.at(i)->oops_do(cl);
oopStorageSetParState.inline.hpp:    _cl->do_oop(p);
oopStorageSetParState.inline.hpp:    if (state->storage()->should_report_num_dead()) {
oopStorageSetParState.inline.hpp:      state->oops_do(&counting_cl);
oopStorageSetParState.inline.hpp:      state->increment_num_dead(counting_cl.num_dead());
oopStorageSetParState.inline.hpp:      state->oops_do(cl);
oopStorageSetParState.inline.hpp:    state->storage()->report_num_dead(state->num_dead());
parallelCleaning.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
parallelCleaning.cpp:    _first_nmethod->do_unloading(_unloading_occurred);
parallelCleaning.cpp:      claimed_nmethods[i]->do_unloading(_unloading_occurred);
parallelCleaning.cpp:  } while (klass != NULL && !klass->is_instance_klass());
parallelCleaning.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
parallelCleaning.hpp:    ik->clean_weak_instanceklass_links();
plab.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
plab.cpp:  AlignmentReserve = Universe::heap()->tlab_alloc_reserve();
plab.cpp:  stats->add_allocated(_allocated);
plab.cpp:  stats->add_wasted(_wasted);
plab.cpp:  stats->add_undo_wasted(_undo_wasted);
plab.cpp:  stats->add_unused(unused);
plab.cpp:    Universe::heap()->fill_with_dummy_object(_top, _hard_end, true);
plab.cpp:  Universe::heap()->fill_with_dummy_object(obj, obj + word_sz, true);
plab.cpp:    assert(contains(obj + word_sz - 1),
plab.cpp:  size_t used = allocated - _wasted - _unused;
plab.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
plab.hpp:// A per-thread allocation buffer used during GC.
plab.hpp:    _end      = _hard_end - AlignmentReserve;
plab.hpp:// PLAB book-keeping.
plab.hpp:  size_t used() const { return allocated() - (wasted() + unused()); }
plab.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
preGCValues.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
preservedMarks.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
preservedMarks.cpp:    oop obj = elem->get_oop();
preservedMarks.cpp:    if (obj->is_forwarded()) {
preservedMarks.cpp:      elem->set_oop(obj->forwardee());
preservedMarks.cpp:  if (obj->is_forwarded()) {
preservedMarks.cpp:  assert(_stacks == NULL && _num == 0, "do not re-initialize");
preservedMarks.cpp:  assert(num > 0, "pre-condition");
preservedMarks.cpp:      _preserved_marks_set->get(task_id)->restore_and_increment(_total_size_addr);
preservedMarks.cpp:    _sub_tasks.set_n_tasks(preserved_marks_set->num());
preservedMarks.cpp:    total_size_before += get(i)->size();
preservedMarks.cpp:      total_size += get(i)->size();
preservedMarks.cpp:      get(i)->restore();
preservedMarks.cpp:    ParRestoreTask task(workers->active_workers(), this, &total_size);
preservedMarks.cpp:    workers->run_task(&task);
preservedMarks.cpp:    // the array was resource-allocated, so nothing to do
preservedMarks.cpp:    get(i)->assert_empty();
preservedMarks.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
preservedMarks.hpp:  // true -> _stacks will be allocated in the C heap
preservedMarks.hpp:  // false -> _stacks will be allocated in the resource arena
preservedMarks.hpp:    assert(i < _num, "pre-condition");
preservedMarks.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
preservedMarks.inline.hpp:  return obj->mark_must_be_preserved_for_promotion_failure(m);
preservedMarks.inline.hpp:  assert(should_preserve_mark(obj, m), "pre-condition");
preservedMarks.inline.hpp:  obj->init_mark_raw();
preservedMarks.inline.hpp:  _o->set_mark_raw(_m);
ptrQueue.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
ptrQueue.cpp:  _capacity_in_bytes(index_to_byte_index(qset->buffer_size())),
ptrQueue.cpp:      qset()->deallocate_buffer(node);
ptrQueue.cpp:      qset()->enqueue_completed_buffer(node);
ptrQueue.cpp:  _index -= _element_size;
ptrQueue.cpp:  _buf = qset()->allocate_buffer();
ptrQueue.cpp:  qset()->enqueue_completed_buffer(node);
ptrQueue.cpp:  node->~BufferNode();
ptrQueue.cpp:  strncpy(_name, name, sizeof(_name) - 1);
ptrQueue.cpp:  _name[sizeof(_name) - 1] = '\0';
ptrQueue.cpp:    BufferNode* next = list->next();
ptrQueue.cpp:    DEBUG_ONLY(list->set_next(NULL);)
ptrQueue.cpp:// To solve the ABA problem for lock-free stack pop, allocate does the
ptrQueue.cpp:  assert(node->next() == NULL, "precondition");
ptrQueue.cpp:// synchronization delay for any in-progress pops from the _free_list,
ptrQueue.cpp:// in-progress transfer.
ptrQueue.cpp:    for (BufferNode* next = first->next(); next != NULL; next = next->next()) {
ptrQueue.cpp:    // Wait for any in-progress pops, to avoid ABA for them.
ptrQueue.cpp:  BufferNode* node = _allocator->allocate();
ptrQueue.cpp:  _allocator->release(node);
ptrQueue.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
ptrQueue.hpp:// the addresses of modified old-generation objects.  This type supports
ptrQueue.hpp:  // Value is always pointer-size aligned.
ptrQueue.hpp:  // Value is always pointer-size aligned.
ptrQueue.hpp:  // Return the size of the in-use region.
ptrQueue.hpp:      result = byte_index_to_index(capacity_in_bytes() - _index);
ptrQueue.hpp:  // inactive thread is a no-op.  Setting a queue to inactive resets its
ptrQueue.hpp:        reinterpret_cast<char*>(buffer) - buffer_offset());
ptrQueue.hpp:    node->set_index(index);
ptrQueue.hpp:  class Allocator;              // Free-list based allocator.
ptrQueue.hpp:// Allocation is based on a lock-free free list of nodes, linked through
ptrQueue.hpp:// This is documented behavior so that other parts of the node life-cycle
ptrQueue.hpp:  char _name[DEFAULT_CACHE_LINE_SIZE - sizeof(size_t)]; // Use name as padding.
ptrQueue.hpp:    return _allocator->buffer_size();
referenceDiscoverer.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
referencePolicy.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
referencePolicy.cpp:// Capture state (of-the-VM) information needed to evaluate the policy
referencePolicy.cpp:  jlong interval = timestamp_clock - java_lang_ref_SoftReference::timestamp(p);
referencePolicy.cpp:// Capture state (of-the-VM) information needed to evaluate the policy
referencePolicy.cpp:  max_heap -= Universe::get_heap_used_at_last_gc();
referencePolicy.cpp:  jlong interval = timestamp_clock - java_lang_ref_SoftReference::timestamp(p);
referencePolicy.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
referencePolicy.hpp:  // Capture state (of-the-VM) information needed to evaluate the policy
referencePolicy.hpp:  // Capture state (of-the-VM) information needed to evaluate the policy
referencePolicy.hpp:  // Capture state (of-the-VM) information needed to evaluate the policy
referenceProcessor.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
referenceProcessor.cpp:  // We need a monotonically non-decreasing time in ms but
referenceProcessor.cpp:              "Found non-empty discovered list at %u", i);
referenceProcessor.cpp:      f->do_oop((narrowOop*)_discovered_refs[i].adr_head());
referenceProcessor.cpp:      f->do_oop((oop*)_discovered_refs[i].adr_head());
referenceProcessor.cpp:  // We need a monotonically non-decreasing time in ms but
referenceProcessor.cpp:  // non-decreasing provided the underlying platform provides such
referenceProcessor.cpp:  // In product mode, however, protect ourselves from non-monotonicity.
referenceProcessor.cpp:    task_executor->set_single_threaded_mode();
referenceProcessor.cpp:  phase_times->set_total_time_ms((os::elapsedTime() - start_time) * 1000);
referenceProcessor.cpp:  assert(Universe::heap()->is_in_or_null(_referent),
referenceProcessor.cpp:  // pre-barrier here because we know the Reference has already been found/marked,
referenceProcessor.cpp:                               iter.obj()->klass()->internal_name());
referenceProcessor.cpp:                               reason, p2i(iter.obj()), iter.obj()->klass()->internal_name());
referenceProcessor.cpp:  assert(policy != NULL, "Must have a non-NULL policy");
referenceProcessor.cpp:        !policy->should_clear_reference(iter.obj(), _soft_ref_timestamp_clock)) {
referenceProcessor.cpp:  complete_gc->do_void();
referenceProcessor.cpp:    // Self-loop next, to mark the FinalReference not active.
referenceProcessor.cpp:  complete_gc->do_void();
referenceProcessor.cpp:  complete_gc->do_void();
referenceProcessor.cpp:    _phase_times->add_ref_cleared(REF_SOFT, removed);
referenceProcessor.cpp:    _phase_times->add_ref_cleared(ref_type, removed);
referenceProcessor.cpp:    RefProcWorkerTimeTracker t(_phase_times->phase2_worker_time_sec(), worker_id);
referenceProcessor.cpp:    _phase_times->add_ref_cleared(REF_PHANTOM, removed);
referenceProcessor.cpp:  assert(_processing_is_mt, "why balance non-mt processing?");
referenceProcessor.cpp:  // _num_queues will be processed, so any non-empty lists beyond
referenceProcessor.cpp:    // Configuration says don't balance, but if there are non-empty
referenceProcessor.cpp:// Move entries from all queues[0, 1, ..., _max_num_q-1] to
referenceProcessor.cpp:// queues[0, 1, ..., _num_q-1] because only the first _num_q
referenceProcessor.cpp:                              avg_refs - ref_lists[to_idx].length());
referenceProcessor.cpp:          refs_to_move = MIN2(ref_lists[from_idx].length() - avg_refs,
referenceProcessor.cpp:                              avg_refs - ref_lists[to_idx].length());
referenceProcessor.cpp:  phase_times->set_ref_discovered(REF_SOFT, num_soft_refs);
referenceProcessor.cpp:  phase_times->set_processing_is_mt(_processing_is_mt);
referenceProcessor.cpp:    task_executor->execute(phase1, num_queues());
referenceProcessor.cpp:    phase_times->add_ref_cleared(REF_SOFT, removed);
referenceProcessor.cpp:  phase_times->set_ref_discovered(REF_WEAK, num_weak_refs);
referenceProcessor.cpp:  phase_times->set_ref_discovered(REF_FINAL, num_final_refs);
referenceProcessor.cpp:  phase_times->set_processing_is_mt(_processing_is_mt);
referenceProcessor.cpp:    task_executor->execute(phase2, num_queues());
referenceProcessor.cpp:    RefProcWorkerTimeTracker t(phase_times->phase2_worker_time_sec(), 0);
referenceProcessor.cpp:      phase_times->add_ref_cleared(REF_SOFT, removed);
referenceProcessor.cpp:      phase_times->add_ref_cleared(REF_WEAK, removed);
referenceProcessor.cpp:      phase_times->add_ref_cleared(REF_FINAL, removed);
referenceProcessor.cpp:    complete_gc->do_void();
referenceProcessor.cpp:  phase_times->set_processing_is_mt(_processing_is_mt);
referenceProcessor.cpp:    task_executor->execute(phase3, num_queues());
referenceProcessor.cpp:  phase_times->set_ref_discovered(REF_PHANTOM, num_phantom_refs);
referenceProcessor.cpp:  phase_times->set_processing_is_mt(_processing_is_mt);
referenceProcessor.cpp:    task_executor->execute(phase4, num_queues());
referenceProcessor.cpp:    phase_times->add_ref_cleared(REF_PHANTOM, removed);
referenceProcessor.cpp:    // During a multi-threaded discovery phase,
referenceProcessor.cpp:    id = thr->as_Worker_thread()->id();
referenceProcessor.cpp:    // single-threaded discovery, we save in round-robin
referenceProcessor.cpp:                               p2i(obj), obj->klass()->internal_name());
referenceProcessor.cpp:                               p2i(obj), obj->klass()->internal_name());
referenceProcessor.cpp:// Non-atomic (i.e. concurrent) discovery might allow us
referenceProcessor.cpp:         p2i(referent), p2i(obj), da ? "" : "non-");
referenceProcessor.cpp:  return _is_subject_to_discovery->do_object_b(obj);
referenceProcessor.cpp:    // Don't rediscover non-active FinalReferences.
referenceProcessor.cpp:    if (is_alive_non_header()->do_object_b(java_lang_ref_Reference::referent(obj))) {
referenceProcessor.cpp:    // to the reference-processing phase. Since all current
referenceProcessor.cpp:    // time-stamp policies advance the soft-ref clock only
referenceProcessor.cpp:    if (!_current_soft_ref_policy->should_clear_reference(obj, _soft_ref_timestamp_clock)) {
referenceProcessor.cpp:                               p2i(obj), obj->klass()->internal_name());
referenceProcessor.cpp:    oop current_head = list->head();
referenceProcessor.cpp:    list->set_head(obj);
referenceProcessor.cpp:    list->inc_length(1);
referenceProcessor.cpp:    log_develop_trace(gc, ref)("Discovered reference (" INTPTR_FORMAT ": %s)", p2i(obj), obj->klass()->internal_name());
referenceProcessor.cpp:      if (yield->should_return()) {
referenceProcessor.cpp:      if (yield->should_return()) {
referenceProcessor.cpp:      if (yield->should_return()) {
referenceProcessor.cpp:      if (yield->should_return()) {
referenceProcessor.cpp:// are not active (have a non-NULL next field). NOTE: When we are
referenceProcessor.cpp:// thus precleaning the ref lists (which happens single-threaded today),
referenceProcessor.cpp:    if (yield->should_return_fine_grain()) {
referenceProcessor.cpp:                                 p2i(iter.obj()), iter.obj()->klass()->internal_name());
referenceProcessor.cpp:  complete_gc->do_void();
referenceProcessor.cpp:    _saved_mt_processing(_rp->processing_is_mt()),
referenceProcessor.cpp:    _saved_num_queues(_rp->num_queues()) {
referenceProcessor.cpp:  if (!_rp->processing_is_mt() || !_rp->adjust_no_of_processing_threads() || (ReferencesPerThread == 0)) {
referenceProcessor.cpp:  uint workers = ergo_proc_thread_count(ref_count, _rp->num_queues(), phase);
referenceProcessor.cpp:  _rp->set_mt_processing(workers > 1);
referenceProcessor.cpp:  _rp->set_active_mt_degree(workers);
referenceProcessor.cpp:  _rp->set_mt_processing(_saved_mt_processing);
referenceProcessor.cpp:  _rp->set_active_mt_degree(_saved_num_queues);
referenceProcessor.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
referenceProcessor.hpp:  void   dec_length(size_t dec) { _len -= dec; }
referenceProcessor.hpp:    return _is_alive->do_object_b(_referent);
referenceProcessor.hpp:      _keep_alive->do_oop((narrowOop*)_referent_addr);
referenceProcessor.hpp:      _keep_alive->do_oop((oop*)_referent_addr);
referenceProcessor.hpp:// The ReferenceProcessor class encapsulates the per-"collector" processing
referenceProcessor.hpp:// generations that are being independently collected -- possibly
referenceProcessor.hpp:// straightforward manner in a general, non-generational, non-contiguous generation
referenceProcessor.hpp:  // Names of sub-phases of reference processing. Indicates the type of the reference
referenceProcessor.hpp:  uint        _next_id;                 // round-robin mod _num_queues counter in
referenceProcessor.hpp:  // Phase 1: Re-evaluate soft ref policy.
referenceProcessor.hpp:  // and enqueue non-Final references.
referenceProcessor.hpp:  static int number_of_subclasses_of_ref() { return (REF_PHANTOM - REF_OTHER); }
referenceProcessor.hpp:    _current_soft_ref_policy->setup();   // snapshot the policy threshold
referenceProcessor.hpp:  // round-robin mod _num_queues (not: _not_ mod _max_num_queues)
referenceProcessor.hpp:  // whether discovery is done by multiple threads same-old-timeously
referenceProcessor.hpp:// A subject-to-discovery closure that uses a single memory span to determine the area that
referenceProcessor.hpp:    _was_discovering_refs = _rp->discovery_enabled();
referenceProcessor.hpp:      _rp->disable_discovery();
referenceProcessor.hpp:      _rp->enable_discovery(false /*check_no_refs*/);
referenceProcessor.hpp:    _saved_cl = _rp->is_subject_to_discovery_closure();
referenceProcessor.hpp:    _rp->set_is_subject_to_discovery_closure(cl);
referenceProcessor.hpp:    _rp->set_is_subject_to_discovery_closure(_saved_cl);
referenceProcessor.hpp:    _old_discoverer(rp->is_subject_to_discovery_closure()) {
referenceProcessor.hpp:    rp->set_is_subject_to_discovery_closure(&_discoverer);
referenceProcessor.hpp:    _rp->set_is_subject_to_discovery_closure(_old_discoverer);
referenceProcessor.hpp:    _saved_mt = _rp->discovery_is_mt();
referenceProcessor.hpp:    _rp->set_mt_discovery(mt);
referenceProcessor.hpp:    _rp->set_mt_discovery(_saved_mt);
referenceProcessor.hpp:    _saved_cl = _rp->is_alive_non_header();
referenceProcessor.hpp:    _rp->set_is_alive_non_header(cl);
referenceProcessor.hpp:    _rp->set_is_alive_non_header(_saved_cl);
referenceProcessor.hpp:    _saved_atomic_discovery = _rp->discovery_is_atomic();
referenceProcessor.hpp:    _rp->set_atomic_discovery(atomic);
referenceProcessor.hpp:    _rp->set_atomic_discovery(_saved_atomic_discovery);
referenceProcessor.hpp:    _saved_mt = _rp->processing_is_mt();
referenceProcessor.hpp:    _rp->set_mt_processing(mt);
referenceProcessor.hpp:    _rp->set_mt_processing(_saved_mt);
referenceProcessor.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
referenceProcessorPhaseTimes.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
referenceProcessorPhaseTimes.cpp:  double result = os::elapsedTime() - _start_time;
referenceProcessorPhaseTimes.cpp:  _worker_time->set(_worker_id, result);
referenceProcessorPhaseTimes.cpp:  RefProcWorkerTimeTracker(phase_times->sub_phase_worker_time_sec(phase), worker_id) {
referenceProcessorPhaseTimes.cpp:  if (_phase_times->gc_timer() != NULL) {
referenceProcessorPhaseTimes.cpp:    _phase_times->gc_timer()->register_gc_phase_start(title, _start_ticks);
referenceProcessorPhaseTimes.cpp:  // If ASSERT is defined, the default value of Ticks will be -2.
referenceProcessorPhaseTimes.cpp:  return TimeHelper::counter_to_millis(end_value - _start_ticks.value());
referenceProcessorPhaseTimes.cpp:  if (_phase_times->gc_timer() != NULL) {
referenceProcessorPhaseTimes.cpp:    _phase_times->gc_timer()->register_gc_phase_end(ticks);
referenceProcessorPhaseTimes.cpp:  phase_times()->set_balance_queues_time_ms(_phase_number, elapsed);
referenceProcessorPhaseTimes.cpp:  phase_times()->set_phase_time_ms(_phase_number, elapsed);
referenceProcessorPhaseTimes.cpp:  phase_times()->set_phase_time_ms(_phase_number, elapsed);
referenceProcessorPhaseTimes.cpp:  return ref_type - REF_SOFT;
referenceProcessorPhaseTimes.cpp:    _sub_phases_worker_time_sec[i]->reset();
referenceProcessorPhaseTimes.cpp:  _phase2_worker_time_sec->reset();
referenceProcessorPhaseTimes.cpp:    ls->print_cr("%s%s " TIME_FORMAT, Indents[indent], "Balance queues:", balance_time);
referenceProcessorPhaseTimes.cpp:  ls->print("%s", Indents[indent]);
referenceProcessorPhaseTimes.cpp:    worker_time->print_summary_on(ls, true);
referenceProcessorPhaseTimes.cpp:      worker_time->print_details_on(&ls2);
referenceProcessorPhaseTimes.cpp:    if (worker_time->get(0) != uninitialized()) {
referenceProcessorPhaseTimes.cpp:      ls->print_cr("%s " TIME_FORMAT,
referenceProcessorPhaseTimes.cpp:                   worker_time->get(0) * MILLIUNITS);
referenceProcessorPhaseTimes.cpp:      ls->print_cr("%s skipped", ser_title);
referenceProcessorPhaseTimes.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
referenceProcessorPhaseTimes.hpp:  static const int number_of_subclasses_of_ref = REF_PHANTOM - REF_OTHER; // 5 - 1 = 4
referenceProcessorPhaseTimes.hpp:  static double uninitialized() { return -1.0; }
referenceProcessorStats.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
satbMarkQueue.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
satbMarkQueue.cpp:  // field to true. This must be done in the collector-specific
satbMarkQueue.cpp:// use the buffer as-is, instead of enqueueing and replacing it.
satbMarkQueue.cpp:  // This method should only be called if there is a non-NULL buffer
satbMarkQueue.cpp:  assert(index() == 0, "pre-condition");
satbMarkQueue.cpp:  assert(_buf != NULL, "pre-condition");
satbMarkQueue.cpp:  size_t threshold = satb_qset()->buffer_enqueue_threshold();
satbMarkQueue.cpp:    cl->do_buffer(&_buf[index()], size());
satbMarkQueue.cpp:  tty->print_cr("  SATB BUFFER [%s] buf: " PTR_FORMAT " index: " SIZE_FORMAT
satbMarkQueue.cpp:    value -= 2;
satbMarkQueue.cpp:  _buffer_enqueue_threshold = MAX2(size - enqueue_qty, (size_t)1);
satbMarkQueue.cpp:      SATBMarkQueue& queue = _qset->satb_queue_for_thread(t);
satbMarkQueue.cpp:                            t->name(),
satbMarkQueue.cpp:      if (_qset->satb_queue_for_thread(t).is_active() != _expected_active) {
satbMarkQueue.cpp:        _qset->dump_active_states(_expected_active);
satbMarkQueue.cpp:      _qset->satb_queue_for_thread(t).set_active(_active);
satbMarkQueue.cpp:    size_t index = nd->index();
satbMarkQueue.cpp:    cl->do_buffer(buf + index, size - index);
satbMarkQueue.cpp:// SATB buffer life-cycle - Per-thread queues obtain buffers from the
satbMarkQueue.cpp:// them, and returns them to the buffer allocator for re-use.  Both
satbMarkQueue.cpp:// the allocator and the qset use lock-free stacks.  The ABA problem
satbMarkQueue.cpp:  tty->cr();
satbMarkQueue.cpp:  tty->print_cr("SATB BUFFERS [%s]", msg);
satbMarkQueue.cpp:    print_satb_buffer(buffer, buf, nd->index(), buffer_size());
satbMarkQueue.cpp:    nd = nd->next();
satbMarkQueue.cpp:      os::snprintf(_buffer, SATB_PRINTER_BUFFER_SIZE, "Thread: %s", t->name());
satbMarkQueue.cpp:      _qset->satb_queue_for_thread(t).print(_buffer);
satbMarkQueue.cpp:  tty->cr();
satbMarkQueue.cpp:    buffers_to_delete = bn->next();
satbMarkQueue.cpp:    bn->set_next(NULL);
satbMarkQueue.cpp:      _qset->satb_queue_for_thread(t).reset();
satbMarkQueue.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
satbMarkQueue.hpp:    queue->apply_filter(filter);
satbMarkQueue.hpp:  satb_qset()->filter(this);
satbMarkQueue.hpp:  void** buf = this->_buf;
satbMarkQueue.hpp:  // Two-fingered compaction toward the end.
satbMarkQueue.hpp:  void** src = &buf[this->index()];
satbMarkQueue.hpp:  void** dst = &buf[this->capacity()];
satbMarkQueue.hpp:      while (src < --dst) {
satbMarkQueue.hpp:  this->set_index(dst - buf);
scavengableNMethods.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
scavengableNMethods.cpp:    if (*p == NULL || !_is_scavengable->do_object_b(*p)) {
scavengableNMethods.cpp:      _nm->print_nmethod(true);
scavengableNMethods.cpp:    tty->print_cr("*** scavengable oop " PTR_FORMAT " found at " PTR_FORMAT " (offset %d)",
scavengableNMethods.cpp:                  p2i(*p), p2i(p), (int)((intptr_t)p - (intptr_t)_nm));
scavengableNMethods.cpp:    (*p)->print();
scavengableNMethods.cpp:    nm->oops_do(&cl);
scavengableNMethods.cpp:    if (!_found && *p != NULL && _is_scavengable->do_object_b(*p)) {
scavengableNMethods.cpp:  nm->oops_do(&cl);
scavengableNMethods.cpp:    assert(cur->is_alive(), "Must be");
scavengableNMethods.cpp:      cl->do_code_blob(cur);
scavengableNMethods.cpp:      cl->do_code_blob(nm);
scavengableNMethodsData.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
scavengableNMethodsData.hpp:  static const uintptr_t state_mask = (1 << state_bits) - 1;
scavengableNMethodsData.hpp:  uintptr_t data() const                    { return reinterpret_cast<uintptr_t>(_nm->gc_data<void>()); }
scavengableNMethodsData.hpp:  void set_data(uintptr_t data) const       { _nm->set_gc_data(reinterpret_cast<void*>(data)); }
scavengableNMethods.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
softRefGenPolicy.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
softRefGenPolicy.cpp:  AdaptiveSizePolicy* size_policy = GenCollectedHeap::heap()->size_policy();
softRefGenPolicy.cpp:    set_should_clear_all_soft_refs(size_policy->gc_overhead_limit_near());
softRefGenPolicy.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
softRefPolicy.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
softRefPolicy.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
softRefPolicy.hpp:  // Set to true by the GC if the just-completed gc cleared all
softRefPolicy.hpp:      _soft_ref_policy->cleared_all_soft_refs();
space.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
space.cpp:    if (_sp->block_is_obj(top_obj)) {
space.cpp:        if (oop(top_obj)->is_objArray() || oop(top_obj)->is_typeArray()) {
space.cpp:          // An arrayOop is starting on the dirty card - since we do exact
space.cpp:          top = top_obj + oop(top_obj)->size();
space.cpp:    assert(top == _sp->end(), "only case where top_obj == NULL");
space.cpp:  // We can and should try to optimize by calling the non-MemRegion
space.cpp:  // oop_iterate()) To be done post-beta XXX
space.cpp:  for (; bottom < top; bottom += _sp->block_size(bottom)) {
space.cpp:    if (_sp->block_is_obj(bottom) &&
space.cpp:        !_sp->obj_allocated_since_save_marks(oop(bottom))) {
space.cpp:      oop(bottom)->oop_iterate(_cl, mr);
space.cpp:// trim its right-end back some so we do not scan what
space.cpp:  bottom_obj = _sp->block_start(bottom);
space.cpp:  top_obj    = _sp->block_start(last);
space.cpp:  // -- something that could happen with a concurrent sweeper.
space.cpp:  if (top_obj != NULL && top_obj < (_sp->toContiguousSpace())->top()) {
space.cpp:      if (oop(top_obj)->is_objArray() || oop(top_obj)->is_typeArray()) {
space.cpp:        // An arrayOop is starting on the dirty card - since we do exact
space.cpp:        assert(_sp->block_size(top_obj) == (size_t) oop(top_obj)->size(),
space.cpp:        top = top_obj + oop(top_obj)->size();
space.cpp:    top = (_sp->toContiguousSpace())->top();
space.cpp:  // they were dirtied and before the stop-the-world GC that is
space.cpp:  bottom += oop(bottom)->oop_iterate_size(cl, mr);                      \
space.cpp:    HeapWord* next_obj = bottom + oop(bottom)->size();                  \
space.cpp:      /* non-memRegion version of oop_iterate below. */                 \
space.cpp:      oop(bottom)->oop_iterate(cl);                                     \
space.cpp:      next_obj = bottom + oop(bottom)->size();                          \
space.cpp:    oop(bottom)->oop_iterate(cl, mr);                                   \
space.cpp:  mangler()->set_top_for_allocations(v);
space.cpp:  mangler()->set_top_for_allocations(top());
space.cpp:  mangler()->check_mangled_unused_area(limit);
space.cpp:  mangler()->check_mangled_unused_area_complete();
space.cpp:  mangler()->mangle_unused_area();
space.cpp:  mangler()->mangle_unused_area_complete();
space.cpp:  assert(this == cp->space, "'this' should be current compaction space.");
space.cpp:    cp->space->set_compaction_top(compact_top);
space.cpp:    cp->space = cp->space->next_compaction_space();
space.cpp:    if (cp->space == NULL) {
space.cpp:      cp->gen = GenCollectedHeap::heap()->young_gen();
space.cpp:      assert(cp->gen != NULL, "compaction must succeed");
space.cpp:      cp->space = cp->gen->first_compaction_space();
space.cpp:      assert(cp->space != NULL, "generation must have a first compaction space");
space.cpp:    compact_top = cp->space->bottom();
space.cpp:    cp->space->set_compaction_top(compact_top);
space.cpp:    cp->threshold = cp->space->initialize_threshold();
space.cpp:    compaction_max_size = pointer_delta(cp->space->end(), compact_top);
space.cpp:    q->forward_to(oop(compact_top));
space.cpp:    assert(q->is_gc_marked(), "encoding the pointer should preserve the mark");
space.cpp:    q->init_mark_raw();
space.cpp:    assert(q->forwardee() == NULL, "should be forwarded to NULL");
space.cpp:  if (compact_top > cp->threshold)
space.cpp:    cp->threshold =
space.cpp:      cp->space->cross_threshold(compact_top - size, compact_top);
space.cpp:  st->print(" space " SIZE_FORMAT "K, %3d%% used", capacity() / K,
space.cpp:  st->print_cr(" [" INTPTR_FORMAT ", " INTPTR_FORMAT ")",
space.cpp:  st->print_cr(" [" INTPTR_FORMAT ", " INTPTR_FORMAT ", " INTPTR_FORMAT ")",
space.cpp:  st->print_cr(" [" INTPTR_FORMAT ", " INTPTR_FORMAT ", "
space.cpp:    p += oop(p)->size();
space.cpp:    guarantee(top() == block_start_const(end()-1) &&
space.cpp:    obj_addr += oop(obj_addr)->oop_iterate_size(blk);
space.cpp:    blk->do_object(oop(mark));
space.cpp:    mark += oop(mark)->size();
space.cpp:      cur += oop(cur)->size();
space.cpp:         "p > current top - p: " PTR_FORMAT ", current top: " PTR_FORMAT,
space.cpp:         "p (" PTR_FORMAT ") is not a block start - "
space.cpp:    return oop(p)->size();
space.cpp:  assert(Heap_lock->owned_by_self() ||
space.cpp:         (SafepointSynchronize::is_at_safepoint() && Thread::current()->is_VM_thread()),
space.cpp:// This version is lock-free.
space.cpp:  assert(Heap_lock->owned_by_self() || (SafepointSynchronize::is_at_safepoint() && Thread::current()->is_VM_thread()), "not locked");
space.cpp:// Lock-free.
space.cpp:    size -= size/factor;
space.cpp:    size_t length = (size - array_header_size) * (HeapWordSize / sizeof(jint));
space.cpp:    t->set_mark_raw(markWord::prototype());
space.cpp:    t->set_klass(Universe::intArrayKlassObj());
space.cpp:    t->set_length((int)length);
space.cpp:    obj->set_mark_raw(markWord::prototype());
space.cpp:    obj->set_klass_gap(0);
space.cpp:    obj->set_klass(SystemDictionary::Object_klass());
space.cpp:    size_t size = oop(p)->size();
spaceDecorator.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
spaceDecorator.cpp:// Catch-all file for utility classes
spaceDecorator.cpp:// This can be the case when to-space is being used for
spaceDecorator.cpp:// scratch space during a mark-sweep-compact.  See
spaceDecorator.cpp:           (is_mangled(end() - 1)), "End not properly mangled");
spaceDecorator.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
spaceDecorator.hpp:// are not necessarily up-to-date when this mangling occurs
spaceDecorator.hpp:// to-space is used as scratch space for a mark-sweep-compact
spaceDecorator.hpp:  // Mangle the MemRegion.  This is a non-space specific mangler.  It
spaceDecorator.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
spaceDecorator.inline.hpp:inline HeapWord* GenSpaceMangler::top() const { return _sp->top(); }
spaceDecorator.inline.hpp:inline HeapWord* GenSpaceMangler::end() const { return _sp->end(); }
spaceDecorator.inline.hpp:inline HeapWord* MutableSpaceMangler::top() const { return _sp->top(); }
spaceDecorator.inline.hpp:inline HeapWord* MutableSpaceMangler::end() const { return _sp->end(); }
space.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
space.hpp:  // Test whether p is double-aligned
space.hpp:  // Iterate over all the ref-containing fields of all objects in the
space.hpp:  // object or a non-object.  If "p" is not in the space, return NULL.
space.hpp:  // The non-const version may have benevolent side effects on the data
space.hpp:  // Mark-sweep-compact support: all spaces can update pointers to objects
space.hpp:// OopClosure to (the addresses of) all the ref-containing fields that could
space.hpp:// in non-contiguous spaces with no boundaries, and should be sub-classed
space.hpp:// to support other space types. See ContiguousDCTOC for a sub-class
space.hpp:  HeapWord* _boundary;          // If non-NULL, process only non-NULL oops
space.hpp:  // blocks, where a block may or may not be an object. Sub-
space.hpp:// free-list-based space whose normal collection is a mark-sweep without
space.hpp:// The following are, non-virtual, auxiliary functions used by these function templates:
space.hpp:// - scan_limit()
space.hpp:// - scanned_block_is_obj()
space.hpp:// - scanned_block_size()
space.hpp:// - adjust_obj_size()
space.hpp:// - obj_size()
space.hpp:// and must be defined for all (non-abstract) subclasses of CompactibleSpace.
space.hpp://  - scan_limit
space.hpp://  - scanned_block_is_obj
space.hpp://  - scanned_block_size
space.hpp://  - adjust_obj_size  and adjust_pointers()
space.hpp://  - obj_size         and compact().
space.hpp:  // post-compaction addresses, and insert forwarding pointers.  The fields
space.hpp:  // "cp->gen" and "cp->compaction_space" are the generation and space into
space.hpp:  // "cp->compaction_space" up-to-date.  Offset tables may be updated in
space.hpp:  // this phase as if the final copy had occurred; if so, "cp->threshold"
space.hpp:  // also equal "cp->space").  "compact_top" is where in "this" the
space.hpp:  // be one, since compaction must succeed -- we go to the first space of
space.hpp:  // If the forwarding crosses "cp->threshold", invokes the "cross_threshold"
space.hpp:  // function of the then-current compaction space, and updates "cp->threshold
space.hpp:  // Apply "blk->do_oop" to the addresses of all reference fields in objects
space.hpp:// (ContiguousSpace and sub-classes).
space.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
space.inline.hpp:  // above.  Perhaps in the future some lock-free manner of keeping the
space.inline.hpp:  return oop(addr)->size();
space.inline.hpp:    size_t ratio = _space->allowed_dead_ratio();
space.inline.hpp:        _allowed_deadspace_words = (space->capacity() * ratio / 100) / HeapWordSize;
space.inline.hpp:      _allowed_deadspace_words -= dead_length;
space.inline.hpp:      obj->set_mark_raw(obj->mark_raw().set_marked());
space.inline.hpp:      assert(dead_length == (size_t)obj->size(), "bad filler object size");
space.inline.hpp:  space->set_compaction_top(space->bottom());
space.inline.hpp:  if (cp->space == NULL) {
space.inline.hpp:    assert(cp->gen != NULL, "need a generation");
space.inline.hpp:    assert(cp->threshold == NULL, "just checking");
space.inline.hpp:    assert(cp->gen->first_compaction_space() == space, "just checking");
space.inline.hpp:    cp->space = cp->gen->first_compaction_space();
space.inline.hpp:    cp->threshold = cp->space->initialize_threshold();
space.inline.hpp:    cp->space->set_compaction_top(cp->space->bottom());
space.inline.hpp:  HeapWord* compact_top = cp->space->compaction_top(); // This is where we are currently compacting to.
space.inline.hpp:  HeapWord*  end_of_live = space->bottom();  // One byte beyond the last byte of the last live object.
space.inline.hpp:  HeapWord* cur_obj = space->bottom();
space.inline.hpp:  HeapWord* scan_limit = space->scan_limit();
space.inline.hpp:    assert(!space->scanned_block_is_obj(cur_obj) ||
space.inline.hpp:           oop(cur_obj)->mark_raw().is_marked() || oop(cur_obj)->mark_raw().is_unlocked() ||
space.inline.hpp:           oop(cur_obj)->mark_raw().has_bias_pattern(),
space.inline.hpp:    if (space->scanned_block_is_obj(cur_obj) && oop(cur_obj)->is_gc_marked()) {
space.inline.hpp:      size_t size = space->scanned_block_size(cur_obj);
space.inline.hpp:      compact_top = cp->space->forward(oop(cur_obj), size, cp, compact_top);
space.inline.hpp:        end += space->scanned_block_size(end);
space.inline.hpp:      } while (end < scan_limit && (!space->scanned_block_is_obj(end) || !oop(end)->is_gc_marked()));
space.inline.hpp:        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);
space.inline.hpp:  space->_end_of_live = end_of_live;
space.inline.hpp:    space->_first_dead = first_dead;
space.inline.hpp:    space->_first_dead = end_of_live;
space.inline.hpp:  cp->space->set_compaction_top(compact_top);
space.inline.hpp:  HeapWord* cur_obj = space->bottom();
space.inline.hpp:  HeapWord* const end_of_live = space->_end_of_live;  // Established by "scan_and_forward".
space.inline.hpp:  HeapWord* const first_dead = space->_first_dead;    // Established by "scan_and_forward".
space.inline.hpp:    if (cur_obj < first_dead || oop(cur_obj)->is_gc_marked()) {
space.inline.hpp:      size = space->adjust_obj_size(size);
space.inline.hpp:  HeapWord* cur_obj = space->bottom();
space.inline.hpp:  if (cur_obj < space->_end_of_live && space->_first_dead > cur_obj && !oop(cur_obj)->is_gc_marked()) {
space.inline.hpp:     while (cur_obj < space->_first_dead) {
space.inline.hpp:       size_t size = space->obj_size(cur_obj);
space.inline.hpp:       assert(!oop(cur_obj)->is_gc_marked(), "should be unmarked (special dense prefix handling)");
space.inline.hpp:  bool was_empty = space->used_region().is_empty();
space.inline.hpp:  space->reset_after_compaction();
space.inline.hpp:  // re-initialized.  Also mangles unused area for debugging.
space.inline.hpp:  if (space->used_region().is_empty()) {
space.inline.hpp:    if (!was_empty) space->clear(SpaceDecorator::Mangle);
space.inline.hpp:    if (ZapUnusedHeapArea) space->mangle_unused_area();
space.inline.hpp:  HeapWord* const bottom = space->bottom();
space.inline.hpp:  HeapWord* const end_of_live = space->_end_of_live;
space.inline.hpp:  assert(space->_first_dead <= end_of_live, "Invariant. _first_dead: " PTR_FORMAT " <= end_of_live: " PTR_FORMAT, p2i(space->_first_dead), p2i(end_of_live));
space.inline.hpp:  if (space->_first_dead == end_of_live && (bottom == end_of_live || !oop(bottom)->is_gc_marked())) {
space.inline.hpp:  if (space->_first_dead > cur_obj && !oop(cur_obj)->is_gc_marked()) {
space.inline.hpp:    cur_obj = *(HeapWord**)(space->_first_dead);
space.inline.hpp:    if (!oop(cur_obj)->is_gc_marked()) {
space.inline.hpp:      size_t size = space->obj_size(cur_obj);
space.inline.hpp:      HeapWord* compaction_top = cast_from_oop<HeapWord*>(oop(cur_obj)->forwardee());
space.inline.hpp:      oop(compaction_top)->init_mark_raw();
space.inline.hpp:      assert(oop(compaction_top)->klass() != NULL, "should have a class");
space.inline.hpp:  return oop(addr)->size();
space.inline.hpp:      p += m->oop_iterate_size(blk);
stringdedup/stringDedupTable.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupTable.cpp:    entry->set_next(_list);
stringdedup/stringDedupTable.cpp:      _list = entry->next();
stringdedup/stringDedupTable.cpp:      _length--;
stringdedup/stringDedupTable.cpp:// is important because freeing of table entries is done during stop-the-world
stringdedup/stringDedupTable.cpp:// The cache allows for single-threaded allocations and multi-threaded frees.
stringdedup/stringDedupTable.cpp:  assert(entry->obj() != NULL, "Double free");
stringdedup/stringDedupTable.cpp:  entry->set_obj(NULL);
stringdedup/stringDedupTable.cpp:  entry->set_hash(0);
stringdedup/stringDedupTable.cpp:      StringDedupEntry* next = entry->next();
stringdedup/stringDedupTable.cpp:                             count, STRDEDUP_TIME_PARAM_MS(end - start));
stringdedup/stringDedupTable.cpp:  StringDedupEntry* entry = _entry_cache->alloc();
stringdedup/stringDedupTable.cpp:  entry->set_obj(value);
stringdedup/stringDedupTable.cpp:  entry->set_hash(hash);
stringdedup/stringDedupTable.cpp:  entry->set_latin1(latin1);
stringdedup/stringDedupTable.cpp:  entry->set_next(*list);
stringdedup/stringDedupTable.cpp:  *pentry = entry->next();
stringdedup/stringDedupTable.cpp:  _entry_cache->free(entry, worker_id);
stringdedup/stringDedupTable.cpp:  *pentry = entry->next();
stringdedup/stringDedupTable.cpp:  unsigned int hash = entry->hash();
stringdedup/stringDedupTable.cpp:  size_t index = dest->hash_to_index(hash);
stringdedup/stringDedupTable.cpp:  StringDedupEntry** list = dest->bucket(index);
stringdedup/stringDedupTable.cpp:  entry->set_next(*list);
stringdedup/stringDedupTable.cpp:  for (StringDedupEntry* entry = *list; entry != NULL; entry = entry->next()) {
stringdedup/stringDedupTable.cpp:    if (entry->hash() == hash && entry->latin1() == latin1) {
stringdedup/stringDedupTable.cpp:      oop* obj_addr = (oop*)entry->obj_addr();
stringdedup/stringDedupTable.cpp:  int length = value->length();
stringdedup/stringDedupTable.cpp:    const jbyte* data = (jbyte*)value->base(T_BYTE);
stringdedup/stringDedupTable.cpp:      hash = AltHashing::murmur3_32(_table->_hash_seed, data, length);
stringdedup/stringDedupTable.cpp:    const jchar* data = (jchar*)value->base(T_CHAR);
stringdedup/stringDedupTable.cpp:      hash = AltHashing::murmur3_32(_table->_hash_seed, data, length);
stringdedup/stringDedupTable.cpp:  stat->inc_inspected();
stringdedup/stringDedupTable.cpp:    stat->inc_skipped();
stringdedup/stringDedupTable.cpp:      stat->inc_hashed();
stringdedup/stringDedupTable.cpp:    stat->inc_hashed();
stringdedup/stringDedupTable.cpp:    stat->inc_known();
stringdedup/stringDedupTable.cpp:  uintx size_in_bytes = value->size() * HeapWordSize;
stringdedup/stringDedupTable.cpp:  stat->inc_new(size_in_bytes);
stringdedup/stringDedupTable.cpp:    stat->deduped(value, size_in_bytes);
stringdedup/stringDedupTable.cpp:  size_t size = _table->_size;
stringdedup/stringDedupTable.cpp:  if (_table->_entries > _table->_grow_threshold) {
stringdedup/stringDedupTable.cpp:  } else if (_table->_entries < _table->_shrink_threshold) {
stringdedup/stringDedupTable.cpp:  _entry_cache->set_max_size(size * _max_cache_factor);
stringdedup/stringDedupTable.cpp:  return new StringDedupTable(size, _table->_hash_seed);
stringdedup/stringDedupTable.cpp:  resized_table->_entries = _table->_entries;
stringdedup/stringDedupTable.cpp:  // The table is divided into partitions to allow lock-less parallel processing by
stringdedup/stringDedupTable.cpp:  size_t table_half = _table->_size / 2;
stringdedup/stringDedupTable.cpp:    Atomic::sub(&_table->_entries, removed);
stringdedup/stringDedupTable.cpp:    StringDedupEntry** entry = _table->bucket(bucket);
stringdedup/stringDedupTable.cpp:      oop* p = (oop*)(*entry)->obj_addr();
stringdedup/stringDedupTable.cpp:      if (cl->is_alive(*p)) {
stringdedup/stringDedupTable.cpp:        cl->keep_alive(p);
stringdedup/stringDedupTable.cpp:          _table->transfer(entry, _resized_table);
stringdedup/stringDedupTable.cpp:            bool latin1 = (*entry)->latin1();
stringdedup/stringDedupTable.cpp:            (*entry)->set_hash(hash);
stringdedup/stringDedupTable.cpp:          entry = (*entry)->next_addr();
stringdedup/stringDedupTable.cpp:        _table->remove(entry, worker_id);
stringdedup/stringDedupTable.cpp:  assert(_claimed_index >= _table->_size / 2 || _claimed_index == 0, "All or nothing");
stringdedup/stringDedupTable.cpp:  if (!_table->_rehash_needed && !StringDeduplicationRehashALot) {
stringdedup/stringDedupTable.cpp:  _table->_hash_seed = AltHashing::compute_seed();
stringdedup/stringDedupTable.cpp:  return new StringDedupTable(_table->_size, _table->_hash_seed);
stringdedup/stringDedupTable.cpp:  for (size_t bucket = 0; bucket < _table->_size; bucket++) {
stringdedup/stringDedupTable.cpp:    StringDedupEntry** entry = _table->bucket(bucket);
stringdedup/stringDedupTable.cpp:      _table->transfer(entry, rehashed_table);
stringdedup/stringDedupTable.cpp:  rehashed_table->_entries = _table->_entries;
stringdedup/stringDedupTable.cpp:  for (size_t bucket = 0; bucket < _table->_size; bucket++) {
stringdedup/stringDedupTable.cpp:    StringDedupEntry** entry = _table->bucket(bucket);
stringdedup/stringDedupTable.cpp:      typeArrayOop value = (*entry)->obj();
stringdedup/stringDedupTable.cpp:      guarantee(Universe::heap()->is_in(value), "Object must be on the heap");
stringdedup/stringDedupTable.cpp:      guarantee(!value->is_forwarded(), "Object must not be forwarded");
stringdedup/stringDedupTable.cpp:      guarantee(value->is_typeArray(), "Object must be a typeArrayOop");
stringdedup/stringDedupTable.cpp:      bool latin1 = (*entry)->latin1();
stringdedup/stringDedupTable.cpp:      guarantee((*entry)->hash() == hash, "Table entry has inorrect hash");
stringdedup/stringDedupTable.cpp:      guarantee(_table->hash_to_index(hash) == bucket, "Table entry has incorrect index");
stringdedup/stringDedupTable.cpp:      entry = (*entry)->next_addr();
stringdedup/stringDedupTable.cpp:    StringDedupEntry** entry1 = _table->bucket(bucket);
stringdedup/stringDedupTable.cpp:      typeArrayOop value1 = (*entry1)->obj();
stringdedup/stringDedupTable.cpp:      bool latin1_1 = (*entry1)->latin1();
stringdedup/stringDedupTable.cpp:      StringDedupEntry** entry2 = (*entry1)->next_addr();
stringdedup/stringDedupTable.cpp:        typeArrayOop value2 = (*entry2)->obj();
stringdedup/stringDedupTable.cpp:        bool latin1_2 = (*entry2)->latin1();
stringdedup/stringDedupTable.cpp:        entry2 = (*entry2)->next_addr();
stringdedup/stringDedupTable.cpp:      entry1 = (*entry1)->next_addr();
stringdedup/stringDedupTable.cpp:  _entry_cache->delete_overflowed();
stringdedup/stringDedupTable.cpp:            STRDEDUP_BYTES_PARAM(_table->_size * sizeof(StringDedupEntry*) + (_table->_entries + _entry_cache->size()) * sizeof(StringDedupEntry)));
stringdedup/stringDedupTable.cpp:  log.debug("    Size: " SIZE_FORMAT ", Min: " SIZE_FORMAT ", Max: " SIZE_FORMAT, _table->_size, _min_size, _max_size);
stringdedup/stringDedupTable.cpp:            _table->_entries, percent_of((size_t)_table->_entries, _table->_size), _entry_cache->size(), _entries_added, _entries_removed);
stringdedup/stringDedupTable.cpp:            _resize_count, _table->_shrink_threshold, _shrink_load_factor * 100.0, _table->_grow_threshold, _grow_load_factor * 100.0);
stringdedup/stringDedupTable.cpp:  log.debug("    Rehash Count: " UINTX_FORMAT ", Rehash Threshold: " UINTX_FORMAT ", Hash Seed: 0x%x", _rehash_count, _rehash_threshold, _table->_hash_seed);
stringdedup/stringDedup.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedup.cpp:  StringDedupThread::thread()->stop();
stringdedup/stringDedup.cpp:  tc->do_thread(StringDedupThread::thread());
stringdedup/stringDedupThread.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupThread.inline.hpp:    // Wait for the queue to become non-empty
stringdedup/stringDedupThread.inline.hpp:    if (this->should_terminate()) {
stringdedup/stringDedupQueue.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupQueue.hpp:// queues, one queue per GC worker thread, to allow lock-free and cache-friendly enqueue
stringdedup/stringDedupQueue.hpp:  // Blocks and waits for the queue to become non-empty.
stringdedup/stringDedupQueue.hpp:  // Wakes up any thread blocked waiting for the queue to become non-empty.
stringdedup/stringDedupQueue.hpp:  // Blocks and waits for the queue to become non-empty.
stringdedup/stringDedupQueue.hpp:  // Wakes up any thread blocked waiting for the queue to become non-empty.
stringdedup/stringDedupStat.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupStat.hpp:    _idle_elapsed = now - _start_phase;
stringdedup/stringDedupStat.hpp:    _exec_elapsed += now - _start_phase;
stringdedup/stringDedupStat.hpp:    _block_elapsed += now - _start_phase;
stringdedup/stringDedupStat.hpp:    _exec_elapsed += now - _start_phase;
stringdedup/stringDedupThread.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupThread.hpp:  void run_service() { this->do_deduplication(); }
stringdedup/stringDedup.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedup.hpp:// String deduplication aims to reduce the heap live-set by deduplicating identical
stringdedup/stringDedup.hpp:// starts right after the stop-the-wold marking/evacuation phase. This phase is
stringdedup/stringDedup.hpp:// strings is usually dwarfed by the number of normal (non-interned) strings.
stringdedup/stringDedup.hpp:  bool is_alive(oop o) { return _is_alive->do_object_b(o); }
stringdedup/stringDedup.hpp:  void keep_alive(oop* p) { _keep_alive->do_oop(p); }
stringdedup/stringDedupStat.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupStat.cpp:  _inspected           += stat->_inspected;
stringdedup/stringDedupStat.cpp:  _skipped             += stat->_skipped;
stringdedup/stringDedupStat.cpp:  _hashed              += stat->_hashed;
stringdedup/stringDedupStat.cpp:  _known               += stat->_known;
stringdedup/stringDedupStat.cpp:  _new                 += stat->_new;
stringdedup/stringDedupStat.cpp:  _new_bytes           += stat->_new_bytes;
stringdedup/stringDedupStat.cpp:  _deduped             += stat->_deduped;
stringdedup/stringDedupStat.cpp:  _deduped_bytes       += stat->_deduped_bytes;
stringdedup/stringDedupStat.cpp:  _idle                += stat->_idle;
stringdedup/stringDedupStat.cpp:  _exec                += stat->_exec;
stringdedup/stringDedupStat.cpp:  _block               += stat->_block;
stringdedup/stringDedupStat.cpp:  _idle_elapsed        += stat->_idle_elapsed;
stringdedup/stringDedupStat.cpp:  _exec_elapsed        += stat->_exec_elapsed;
stringdedup/stringDedupStat.cpp:  _block_elapsed       += stat->_block_elapsed;
stringdedup/stringDedupStat.cpp:     STRDEDUP_TIME_PARAM(last_stat->_start_concurrent));
stringdedup/stringDedupStat.cpp:  if (total_stat->_new_bytes > 0) {
stringdedup/stringDedupStat.cpp:    total_deduped_bytes_percent = percent_of(total_stat->_deduped_bytes, total_stat->_new_bytes);
stringdedup/stringDedupStat.cpp:    STRDEDUP_BYTES_FORMAT_NS "->" STRDEDUP_BYTES_FORMAT_NS "(" STRDEDUP_BYTES_FORMAT_NS ") "
stringdedup/stringDedupStat.cpp:    STRDEDUP_BYTES_PARAM(last_stat->_new_bytes),
stringdedup/stringDedupStat.cpp:    STRDEDUP_BYTES_PARAM(last_stat->_new_bytes - last_stat->_deduped_bytes),
stringdedup/stringDedupStat.cpp:    STRDEDUP_BYTES_PARAM(last_stat->_deduped_bytes),
stringdedup/stringDedupStat.cpp:    STRDEDUP_TIME_PARAM(last_stat->_start_concurrent),
stringdedup/stringDedupStat.cpp:    STRDEDUP_TIME_PARAM(last_stat->_end_concurrent),
stringdedup/stringDedupStat.cpp:    STRDEDUP_TIME_PARAM_MS(last_stat->_exec_elapsed));
stringdedup/stringDedupTable.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupTable.hpp:  // function (which doesn't use a seed), and a non-zero hash
stringdedup/stringDedupTable.hpp:    return (size_t)hash & (_size - 1);
stringdedup/stringDedupTable.hpp:    return _table->lookup_or_add_inner(value, latin1, hash);
stringdedup/stringDedupTable.hpp:    return _table->_hash_seed == 0;
stringdedup/stringDedupQueue.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupQueue.cpp:  while (claimed_queue < queue()->num_queues()) {
stringdedup/stringDedupQueue.cpp:    queue()->unlink_or_oops_do_impl(cl, claimed_queue);
stringdedup/stringDedupQueue.cpp:  queue()->print_statistics_impl();
stringdedup/stringDedupQueue.cpp:  queue()->verify_impl();
stringdedup/stringDedupQueue.cpp:  assert(_claimed_index >= queue()->num_queues() || _claimed_index == 0, "All or nothing");
stringdedup/stringDedupQueue.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupQueue.inline.hpp:  queue()->wait_impl();
stringdedup/stringDedupQueue.inline.hpp:  queue()->cancel_wait_impl();
stringdedup/stringDedupQueue.inline.hpp:  queue()->push_impl(worker_id, java_string);
stringdedup/stringDedupQueue.inline.hpp:  return queue()->pop_impl();
stringdedup/stringDedup.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupThread.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
stringdedup/stringDedupThread.cpp:    last_stat->print_statistics(false);
stringdedup/stringDedupThread.cpp:    total_stat->print_statistics(true);
strongRootsScope.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
strongRootsScope.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
suspendibleThreadSet.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
suspendibleThreadSet.cpp:  assert(!Thread::current()->is_suspendible_thread(), "Thread already joined");
suspendibleThreadSet.cpp:  DEBUG_ONLY(Thread::current()->set_suspendible_thread();)
suspendibleThreadSet.cpp:  assert(Thread::current()->is_suspendible_thread(), "Thread not joined");
suspendibleThreadSet.cpp:  DEBUG_ONLY(Thread::current()->clear_suspendible_thread();)
suspendibleThreadSet.cpp:  _nthreads--;
suspendibleThreadSet.cpp:    _synchronize_wakeup->signal();
suspendibleThreadSet.cpp:  assert(Thread::current()->is_suspendible_thread(), "Must have joined");
suspendibleThreadSet.cpp:        guarantee((now - _suspend_all_start) * 1000.0 < (double)ConcGCYieldTimeout, "Long delay");
suspendibleThreadSet.cpp:      _synchronize_wakeup->signal();
suspendibleThreadSet.cpp:    _nthreads_stopped--;
suspendibleThreadSet.cpp:  assert(Thread::current()->is_VM_thread(), "Must be the VM thread");
suspendibleThreadSet.cpp:  // synchronize call.  Hence, there is no need to re-check for
suspendibleThreadSet.cpp:  _synchronize_wakeup->wait();
suspendibleThreadSet.cpp:  assert(Thread::current()->is_VM_thread(), "Must be the VM thread");
suspendibleThreadSet.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
taskqueue.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
taskqueue.cpp:  "qpush", "qpop", "qpop-s", "qattempt", "qsteal", "opush", "omax"
taskqueue.cpp:    const unsigned int hdr_width = w * last_stat_id + last_stat_id - 1;
taskqueue.cpp:    stream->print("%*s", hdr_width, " ");
taskqueue.cpp:    stream->print("%*s", w, _names[0]);
taskqueue.cpp:      stream->print(" %*s", w, _names[i]);
taskqueue.cpp:    memset(dashes, '-', w);
taskqueue.cpp:    stream->print("%s", dashes);
taskqueue.cpp:      stream->print(" %s", dashes);
taskqueue.cpp:  stream->print(FMT, width, _stats[0]);
taskqueue.cpp:    stream->print(" " FMT, width, _stats[i]);
taskqueue.cpp:  return _obj != NULL && _obj->is_objArray() && _index >= 0 &&
taskqueue.cpp:      _index < objArrayOop(_obj)->length();
taskqueue.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
taskqueue.hpp:    pop_slow,         // subset of taskqueue pops that were done slow-path
taskqueue.hpp:  STATIC_ASSERT((N >= 2) && ((N & (N - 1)) == 0));
taskqueue.hpp:  static const uint MOD_N_MASK = N - 1;
taskqueue.hpp:    return (ind - 1) & MOD_N_MASK;
taskqueue.hpp:  // Returns a number in the range [0..N).  If the result is "N-1", it should be
taskqueue.hpp:    return (bot - top) & MOD_N_MASK;
taskqueue.hpp:    // join the competitors.  The "sz == -1" / "sz == N-1" state will not be
taskqueue.hpp:    return (sz == N - 1) ? 0 : sz;
taskqueue.hpp:    assert(dirty_size != N - 1, "invariant");
taskqueue.hpp:  // (_bottom - top()) mod N == N-1; the latter indicates underflow
taskqueue.hpp:  uint max_elems() const { return N - 2; }
taskqueue.hpp:// GenericTaskQueue implements an ABP, Aurora-Blumofe-Plaxton, double-
taskqueue.hpp:// ended-queue (deque), intended for use in work stealing. Queue operations
taskqueue.hpp:// are non-blocking.
taskqueue.hpp:// implementation allows wrap-around at the end of its allocated
taskqueue.hpp:// Theory of Computing Systems 34, 2 (2001), 115-144.
taskqueue.hpp:// implementation for weakly ordered memory models including (pseudo-)
taskqueue.hpp:// code containing memory barriers for a Chase-Lev deque. Chase-Lev is
taskqueue.hpp:// Correct and efficient work-stealing for weak memory models
taskqueue.hpp:// practice of parallel programming (PPoPP 2013), 69-80
taskqueue.hpp:  static const uint InvalidQueueId = uint(-1);
taskqueue.hpp:// push() - push onto the task queue or, if that fails, onto the overflow stack
taskqueue.hpp:// is_empty() - return true if both the TaskQueue and overflow stack are empty
taskqueue.hpp:// Note that size() is not hidden--it returns the number of elements in the
taskqueue.hpp:    _queues[j]->assert_empty();
taskqueue.hpp:    n += _queues[j]->size();
taskqueue.hpp:  static const uintptr_t TagMask = TagAlignment - 1;
taskqueue.hpp:    return static_cast<char*>(_p) - tag;
taskqueue.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
taskqueue.inline.hpp:  // A dirty_size of N-1 cannot happen in push.  Considering only push:
taskqueue.inline.hpp:  // (2) push adds an element iff dirty_n_elems < max_elems(), which is N - 2.
taskqueue.inline.hpp:  // => dirty_n_elems <= N - 2, by induction
taskqueue.inline.hpp:  // => dirty_n_elems < N - 1, invariant
taskqueue.inline.hpp:  // dirty_size == N-1.  pop_global only removes an element if dirty_elems > 0,
taskqueue.inline.hpp:  // so can't underflow to -1 (== N-1) with push.
taskqueue.inline.hpp:    overflow_stack()->push(t);
taskqueue.inline.hpp:    TASKQUEUE_STATS_ONLY(stats.record_overflow(overflow_stack()->size()));
taskqueue.inline.hpp:  // This value cannot be N-1.  That can only occur as a result of
taskqueue.inline.hpp:    // one is the age() below, and the other is age_top() above the if-stmt.
taskqueue.inline.hpp:  t = overflow_stack()->pop();
taskqueue.inline.hpp:// captured (localBottom - oldAge.top) == 1.
taskqueue.inline.hpp:  // Architectures with non-multi-copy-atomic memory model require a
taskqueue.inline.hpp:  int test = a * lo - r * hi;
taskqueue.inline.hpp:    if (local_queue->is_last_stolen_queue_id_valid()) {
taskqueue.inline.hpp:      k1 = local_queue->last_stolen_queue_id();
taskqueue.inline.hpp:        k1 = local_queue->next_random_queue_id() % _n;
taskqueue.inline.hpp:      k2 = local_queue->next_random_queue_id() % _n;
taskqueue.inline.hpp:    uint sz1 = _queues[k1]->size();
taskqueue.inline.hpp:    uint sz2 = _queues[k2]->size();
taskqueue.inline.hpp:      suc = _queues[k2]->pop_global(t);
taskqueue.inline.hpp:      suc = _queues[k1]->pop_global(t);
taskqueue.inline.hpp:      local_queue->set_last_stolen_queue_id(sel_k);
taskqueue.inline.hpp:      local_queue->invalidate_last_stolen_queue_id();
taskqueue.inline.hpp:    return _queues[k]->pop_global(t);
taskqueue.inline.hpp:    TASKQUEUE_STATS_ONLY(queue(queue_num)->stats.record_steal_attempt());
taskqueue.inline.hpp:      TASKQUEUE_STATS_ONLY(queue(queue_num)->stats.record_steal());
taskTerminator.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
taskTerminator.cpp:  _queue_set->assert_empty();
taskTerminator.cpp:  return tasks > 0 || (terminator != NULL && terminator->should_exit_termination());
taskTerminator.cpp:  return _queue_set->tasks();
taskTerminator.cpp:  if (tasks >= _offered_termination - 1) {
taskTerminator.cpp:    for (; tasks > 1; tasks--) {
taskTerminator.cpp:        // Immediately check exit conditions after re-acquiring the lock.
taskTerminator.cpp:          _offered_termination--;
taskTerminator.cpp:    // Immediately check exit conditions after re-acquiring the lock.
taskTerminator.cpp:      _offered_termination--;
taskTerminator.cpp:        _offered_termination--;
taskTerminator.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
taskTerminator.hpp: * New York, NY, USA, 46-54. DOI: https://doi.org/10.1145/2926697.2926706"
taskTerminator.hpp: * Instead of a dedicated spin-master, our implementation will let spin-master
taskTerminator.hpp: * The intention of above enhancement is to reduce spin-master's latency on
taskTerminator.hpp:  // Perform one iteration of spin-master work.
taskTerminator.hpp:  // in an MT-safe manner, once the previous round of use of
threadLocalAllocBuffer.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
threadLocalAllocBuffer.cpp:  size_t capacity = Universe::heap()->tlab_capacity(thr);
threadLocalAllocBuffer.cpp:  size_t used     = Universe::heap()->tlab_used(thr);
threadLocalAllocBuffer.cpp:  size_t total_allocated = thr->allocated_bytes();
threadLocalAllocBuffer.cpp:  size_t allocated_since_last_gc = total_allocated - _allocated_before_last_gc;
threadLocalAllocBuffer.cpp:    stats->update_fast_allocations(_number_of_refills,
threadLocalAllocBuffer.cpp:  stats->update_slow_allocations(_slow_allocations);
threadLocalAllocBuffer.cpp:    Universe::heap()->fill_with_dummy_object(top(), hard_end(), true);
threadLocalAllocBuffer.cpp:    thread()->incr_allocated_bytes(used_bytes());
threadLocalAllocBuffer.cpp:                          (Universe::heap()->tlab_capacity(thread()) / HeapWordSize));
threadLocalAllocBuffer.cpp:                      " refills %d  alloc: %8.6f desired_size: " SIZE_FORMAT " -> " SIZE_FORMAT,
threadLocalAllocBuffer.cpp:                      p2i(thread()), thread()->osthread()->thread_id(),
threadLocalAllocBuffer.cpp:  assert(top <= start + new_size - alignment_reserve(), "size too small");
threadLocalAllocBuffer.cpp:  initialize(start, top, start + new_size - alignment_reserve());
threadLocalAllocBuffer.cpp:  size_t capacity = Universe::heap()->tlab_capacity(thread()) / HeapWordSize;
threadLocalAllocBuffer.cpp:  // Only SPARC-specific BIS instructions are known to fault. (Those
threadLocalAllocBuffer.cpp:  guarantee(Thread::current()->is_Java_thread(), "tlab initialization thread not Java thread");
threadLocalAllocBuffer.cpp:  Thread::current()->tlab().initialize();
threadLocalAllocBuffer.cpp:                               min_size(), Thread::current()->tlab().initial_desired_size(), max_size());
threadLocalAllocBuffer.cpp:    init_sz  = (Universe::heap()->tlab_capacity(thread()) / HeapWordSize) /
threadLocalAllocBuffer.cpp:  size_t tlab_used  = Universe::heap()->tlab_used(thrd);
threadLocalAllocBuffer.cpp:            tag, p2i(thrd), thrd->osthread()->thread_id(),
threadLocalAllocBuffer.cpp:  size_t bytes_until_sample = thread()->heap_sampler().bytes_until_sample();
threadLocalAllocBuffer.cpp:  return (Thread*)(((char*)this) + in_bytes(start_offset()) - in_bytes(Thread::tlab_start_offset()));
threadLocalAllocBuffer.cpp:    _perf_allocating_threads      ->set_value(_allocating_threads);
threadLocalAllocBuffer.cpp:    _perf_total_refills           ->set_value(_total_refills);
threadLocalAllocBuffer.cpp:    _perf_max_refills             ->set_value(_max_refills);
threadLocalAllocBuffer.cpp:    _perf_total_allocations       ->set_value(_total_allocations);
threadLocalAllocBuffer.cpp:    _perf_total_gc_waste          ->set_value(_total_gc_waste);
threadLocalAllocBuffer.cpp:    _perf_max_gc_waste            ->set_value(_max_gc_waste);
threadLocalAllocBuffer.cpp:    _perf_total_slow_refill_waste ->set_value(_total_slow_refill_waste);
threadLocalAllocBuffer.cpp:    _perf_max_slow_refill_waste   ->set_value(_max_slow_refill_waste);
threadLocalAllocBuffer.cpp:    _perf_total_fast_refill_waste ->set_value(_total_fast_refill_waste);
threadLocalAllocBuffer.cpp:    _perf_max_fast_refill_waste   ->set_value(_max_fast_refill_waste);
threadLocalAllocBuffer.cpp:    _perf_total_slow_allocations  ->set_value(_total_slow_allocations);
threadLocalAllocBuffer.cpp:    _perf_max_slow_allocations    ->set_value(_max_slow_allocations);
threadLocalAllocBuffer.cpp:  size_t reserve_size = Universe::heap()->tlab_alloc_reserve();
threadLocalAllocBuffer.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
threadLocalAllocBuffer.hpp:// ThreadLocalAllocBuffer: a descriptor for thread-local storage used by
threadLocalAllocBuffer.hpp://            It is thread-private at any time, but maybe multiplexed over
threadLocalAllocBuffer.hpp:  // Make an in-use tlab parsable.
threadLocalAllocBuffer.hpp:  // Retire an in-use tlab and optionally collect statistics.
threadLocalAllocBuffer.hpp:  // Retire in-use tlab before allocation of a new tlab
threadLocalAllocBuffer.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
threadLocalAllocBuffer.inline.hpp:    // successful thread-local allocation
threadLocalAllocBuffer.inline.hpp:    Copy::fill_to_words(obj + hdr_size, size - hdr_size, badHeapWordVal);
threadLocalAllocBuffer.inline.hpp:  const size_t available_size = Universe::heap()->unsafe_max_tlab_alloc(thread()) / HeapWordSize;
threadLocalAllocBuffer.inline.hpp:                              "slow", p2i(thread()), thread()->osthread()->thread_id(),
verifyOption.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
vmStructs_gc.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
vmStructs_gc.hpp:  /* (needed for run-time type information) */                            \
weakProcessor.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
weakProcessor.cpp:    if (it->should_report_num_dead()) {
weakProcessor.cpp:      it->oops_do(&cl);
weakProcessor.cpp:      it->report_num_dead(cl.num_skipped() + cl.num_dead());
weakProcessor.cpp:      it->weak_oops_do(is_alive, keep_alive);
weakProcessor.cpp:    it->weak_oops_do(closure);
weakProcessor.cpp:    ref_count += it->allocation_count();
weakProcessor.cpp:  assert(_phase_times == NULL || _nworkers <= _phase_times->max_threads(),
weakProcessor.cpp:         _nworkers, _phase_times->max_threads());
weakProcessor.cpp:    _phase_times->set_active_workers(_nworkers);
weakProcessor.cpp:    state->report_num_dead();
weakProcessor.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
weakProcessor.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
weakProcessor.inline.hpp:    bool result = _inner->do_object_b(obj);
weakProcessor.inline.hpp:      _keep_alive->do_oop(p);
weakProcessor.inline.hpp:        _phase_times->record_phase_items(phase, cl.num_dead(), cl.num_total());
weakProcessor.inline.hpp:    cur_state->oops_do(&cl);
weakProcessor.inline.hpp:    cur_state->increment_num_dead(cl.num_skipped() + cl.num_dead());
weakProcessor.inline.hpp:      _phase_times->record_worker_items(worker_id, phase, cl.num_dead(), cl.num_total());
weakProcessor.inline.hpp:    task->_task.work(worker_id,
weakProcessor.inline.hpp:                     static_cast<IsAlive*>(task->_is_alive),
weakProcessor.inline.hpp:                     static_cast<KeepAlive*>(task->_keep_alive));
weakProcessor.inline.hpp:  uint nworkers = ergo_workers(MIN2(workers->active_workers(),
weakProcessor.inline.hpp:                                    phase_times->max_threads()));
weakProcessor.inline.hpp:  workers->run_task(&task, nworkers);
weakProcessor.inline.hpp:  uint nworkers = ergo_workers(workers->active_workers());
weakProcessorPhases.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
weakProcessorPhases.cpp:  return raw_phase_index(phase) - serial_phase_start;
weakProcessorPhases.cpp:  return raw_phase_index(phase) - oopstorage_phase_start;
weakProcessorPhases.cpp:  return (raw_phase_index(phase) - start) < count;
weakProcessorPhases.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
weakProcessorPhases.hpp:  // Phase doesn't have members, so no operator->().
weakProcessorPhaseTimes.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
weakProcessorPhaseTimes.cpp:const double uninitialized_time = -1.0;
weakProcessorPhaseTimes.cpp:    assert(size_t(wpt - _worker_data) < ARRAY_SIZE(_worker_data), "invariant");
weakProcessorPhaseTimes.cpp:    const char* description = it->name();
weakProcessorPhaseTimes.cpp:    (*wpt)->create_thread_work_items("Dead", DeadItems);
weakProcessorPhaseTimes.cpp:    (*wpt)->create_thread_work_items("Total", TotalItems);
weakProcessorPhaseTimes.cpp:  assert(size_t(wpt - _worker_data) == ARRAY_SIZE(_worker_data), "invariant");
weakProcessorPhaseTimes.cpp:  assert(n > 0, "active workers must be non-zero");
weakProcessorPhaseTimes.cpp:    _worker_data[i]->reset();
weakProcessorPhaseTimes.cpp:  return worker_data(phase)->get(worker_id);
weakProcessorPhaseTimes.cpp:  worker_data(phase)->set(worker_id, time_sec);
weakProcessorPhaseTimes.cpp:  phase_data->set_or_add_thread_work_item(worker_id, num_dead, DeadItems);
weakProcessorPhaseTimes.cpp:  phase_data->set_or_add_thread_work_item(worker_id, num_total, TotalItems);
weakProcessorPhaseTimes.cpp:  return (end_time - start_time).seconds();
weakProcessorPhaseTimes.cpp:    _times->record_total_time_sec(elapsed_time_sec(_start_time, end_time));
weakProcessorPhaseTimes.cpp:  assert(_times == NULL || worker_id < _times->active_workers(),
weakProcessorPhaseTimes.cpp:      _times->record_phase_time_sec(_phase, time_sec);
weakProcessorPhaseTimes.cpp:      _times->record_worker_time_sec(_worker_id, _phase, time_sec);
weakProcessorPhaseTimes.cpp:const size_t max_indents_index = ARRAY_SIZE(indents) - 1;
weakProcessorPhaseTimes.cpp:  worker_data(phase)->print_summary_on(&ls, true);
weakProcessorPhaseTimes.cpp:  for (uint i = 0; i < worker_data(phase)->MaxThreadWorkItems; i++) {
weakProcessorPhaseTimes.cpp:    WorkerDataArray<size_t>* work_items = worker_data(phase)->thread_work_items(i);
weakProcessorPhaseTimes.cpp:      work_items->print_summary_on(&ls, true);
weakProcessorPhaseTimes.cpp:    data->print_details_on(&ls);
weakProcessorPhaseTimes.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
weakProcessorPhaseTimes.hpp:  // Per-worker times and linked items.
weakProcessorPhaseTimes.hpp:  // Precondition: worker_id < times->max_threads().
workerDataArray.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
workerDataArray.cpp:  return (size_t)-1;
workerDataArray.cpp:  return -1.0;
workerDataArray.cpp:  out->print_cr(" %.1lfms", time * MILLIUNITS);
workerDataArray.cpp:  out->print_cr(" " SIZE_FORMAT, value);
workerDataArray.cpp:  out->print(" Min: %4.1lf, Avg: %4.1lf, Max: %4.1lf, Diff: %4.1lf", min * MILLIUNITS, avg * MILLIUNITS, max * MILLIUNITS, diff* MILLIUNITS);
workerDataArray.cpp:    out->print(", Sum: %4.1lf", sum * MILLIUNITS);
workerDataArray.cpp:  out->print(" Min: " SIZE_FORMAT ", Avg: %4.1lf, Max: " SIZE_FORMAT ", Diff: " SIZE_FORMAT, min, avg, max, diff);
workerDataArray.cpp:    out->print(", Sum: " SIZE_FORMAT, sum);
workerDataArray.cpp:  out->print("%-25s", "");
workerDataArray.cpp:  for (uint i = 0; i < phase->_length; ++i) {
workerDataArray.cpp:    double value = phase->get(i);
workerDataArray.cpp:    if (value != phase->uninitialized()) {
workerDataArray.cpp:      out->print(" %4.1lf", phase->get(i) * 1000.0);
workerDataArray.cpp:      out->print(" -");
workerDataArray.cpp:  out->cr();
workerDataArray.cpp:  out->print("%-25s", "");
workerDataArray.cpp:  for (uint i = 0; i < phase->_length; ++i) {
workerDataArray.cpp:    size_t value = phase->get(i);
workerDataArray.cpp:    if (value != phase->uninitialized()) {
workerDataArray.cpp:      out->print("  " SIZE_FORMAT, phase->get(i));
workerDataArray.cpp:      out->print(" -");
workerDataArray.cpp:  out->cr();
workerDataArray.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
workerDataArray.hpp:  // Create an integer sub-item at the given index to this WorkerDataArray. If length_override
workerDataArray.inline.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
workerDataArray.inline.hpp:  _thread_work_items[index]->set(worker_i, value);
workerDataArray.inline.hpp:  _thread_work_items[index]->add(worker_i, value);
workerDataArray.inline.hpp:  if (_thread_work_items[index]->get(worker_i) == _thread_work_items[index]->uninitialized()) {
workerDataArray.inline.hpp:    _thread_work_items[index]->set(worker_i, value);
workerDataArray.inline.hpp:    _thread_work_items[index]->add(worker_i, value);
workerDataArray.inline.hpp:  return _thread_work_items[index]->get(worker_i);
workerDataArray.inline.hpp:    out->print("%s:", title());
workerDataArray.inline.hpp:    out->print("%-25s", title());
workerDataArray.inline.hpp:      T diff = max - min;
workerDataArray.inline.hpp:      out->print_cr(", Workers: %d", contributing_threads);
workerDataArray.inline.hpp:    out->print_cr(" skipped");
workerDataArray.inline.hpp:      _thread_work_items[i]->reset();
workerManager.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
workerManager.hpp:  //   active_workers - number of workers being requested for an upcoming
workerManager.hpp:  //   total_workers - total number of workers.  This is the maximum
workerManager.hpp:  //   created_workers - number of workers already created.  This maybe
workerManager.hpp:  //   worker_type - type of thread.
workerManager.hpp:  //   initializing - true if this is called to get the initial number of
workerManager.hpp:      new_worker = holder->install_worker(worker_id);
workerManager.hpp:                        initializing_msg, holder->group_name(), previous_created_workers, active_workers, created_workers);
workerPolicy.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
workerPolicy.cpp:    // use 8 + (72 - 8) * (5/8) == 48 worker threads.
workerPolicy.cpp:              (switch_pt + ((ncpus - switch_pt) * num) / den);
workerPolicy.cpp:    // On 32-bit binaries the virtual address space available to the JVM
workerPolicy.cpp:    // is usually limited to 2-3 GB (depends on the platform).
workerPolicy.cpp:    // Do not use up address space with too many threads (stacks and per-thread
workerPolicy.cpp:    // consequences of over-provisioning threads are higher on 32-bit JVMS,
workerPolicy.cpp:    MAX2((size_t) 2U, Universe::heap()->capacity() / HeapSizePerGCThread);
workerPolicy.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
workerPolicy.hpp:  // be CPU-architecture-specific.
workgroup.cpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
workgroup.cpp:    tc->do_thread(worker(i));
workgroup.cpp:      Thread::current()->name(), task->name());
workgroup.cpp:    task->work(num_workers);
workgroup.cpp:      "thread: " PTR_FORMAT, Thread::current()->name(), task->name(), p2i(Thread::current()));
workgroup.cpp:// Semaphores don't require the worker threads to re-claim the lock when they wake up.
workgroup.cpp:    _start_semaphore->signal(num_workers);
workgroup.cpp:    _end_semaphore->wait();
workgroup.cpp:    _start_semaphore->wait();
workgroup.cpp:    // Subtract one to get a zero-indexed worker id.
workgroup.cpp:    uint worker_id = num_started - 1;
workgroup.cpp:      _end_semaphore->signal();
workgroup.cpp:            task->name(), num_workers, total_workers());
workgroup.cpp:  guarantee(num_workers > 0, "Trying to execute task %s with zero workers", task->name());
workgroup.cpp:  _dispatcher->coordinator_execute_on_workers(task, num_workers, add_foreground_work);
workgroup.cpp:  set_name("%s#%d", gang->name(), id);
workgroup.cpp:  log_develop_trace(gc, workgang)("Running gang worker for gang %s id %u", gang()->name(), id());
workgroup.cpp:  assert(!Thread::current()->is_VM_thread(), "VM thread should not be part"
workgroup.cpp:  return gang()->are_GC_task_threads();
workgroup.cpp:  return gang()->are_ConcurrentGC_threads();
workgroup.cpp:  st->print("\"%s\" ", name());
workgroup.cpp:  st->cr();
workgroup.cpp:  return gang()->dispatcher()->worker_wait_for_task();
workgroup.cpp:  gang()->dispatcher()->worker_done_with_task();
workgroup.cpp:  GCIdMark gc_id_mark(data._task->gc_id());
workgroup.cpp:  log_develop_trace(gc, workgang)("Running work gang: %s task: %s worker: %u", name(), data._task->name(), data._worker_id);
workgroup.cpp:  data._task->work(data._worker_id);
workgroup.cpp:                                  name(), data._task->name(), data._worker_id, p2i(Thread::current()));
workgroup.cpp:  monitor()->notify_all();
workgroup.hpp: * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
workgroup.hpp:      _old_active_workers(gang->active_workers()) {
workgroup.hpp:    uint capped_num_workers = MIN2(requested_num_workers, gang->total_workers());
workgroup.hpp:    gang->update_active_workers(capped_num_workers);
workgroup.hpp:    _gang->update_active_workers(_old_active_workers);
workgroup.hpp:  // n_threads - Number of threads executing the sub-tasks.
workgroup.hpp:// sub-tasks from a set (possibly an enumeration), claim sub-tasks
workgroup.hpp:// a stack object - is there any reason for it not to be?
